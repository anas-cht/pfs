[
  {
    "Title": "Lecture Notes: Optimization for Machine Learning",
    "Authors": "Elad Hazan",
    "Published": "2019-09-08T21:49:42Z",
    "Summary": "Lecture notes on optimization for machine learning, derived from a course at Princeton University and tutorials given in MLSS, Buenos Aires, as well as Simons Foundation, Berkeley.",
    "Link": "http://arxiv.org/abs/1909.03550v1",
    "PDF Link": "http://arxiv.org/pdf/1909.03550v1"
  },
  {
    "Title": "An Optimal Control View of Adversarial Machine Learning",
    "Authors": "Xiaojin Zhu",
    "Published": "2018-11-11T14:28:34Z",
    "Summary": "I describe an optimal control view of adversarial machine learning, where the dynamical system is the machine learner, the input are adversarial actions, and the control costs are defined by the adversary's goals to do harm and be hard to detect. This view encompasses many types of adversarial machine learning, including test-item attacks, training-data poisoning, and adversarial reward shaping. The view encourages adversarial machine learning researcher to utilize advances in control theory and reinforcement learning.",
    "Link": "http://arxiv.org/abs/1811.04422v1",
    "PDF Link": "http://arxiv.org/pdf/1811.04422v1"
  },
  {
    "Title": "Minimax deviation strategies for machine learning and recognition with\n  short learning samples",
    "Authors": "Michail Schlesinger, Evgeniy Vodolazskiy",
    "Published": "2017-07-16T09:15:08Z",
    "Summary": "The article is devoted to the problem of small learning samples in machine learning. The flaws of maximum likelihood learning and minimax learning are looked into and the concept of minimax deviation learning is introduced that is free of those flaws.",
    "Link": "http://arxiv.org/abs/1707.04849v1",
    "PDF Link": "http://arxiv.org/pdf/1707.04849v1"
  },
  {
    "Title": "Machine Learning for Clinical Predictive Analytics",
    "Authors": "Wei-Hung Weng",
    "Published": "2019-09-19T22:02:00Z",
    "Summary": "In this chapter, we provide a brief overview of applying machine learning techniques for clinical prediction tasks. We begin with a quick introduction to the concepts of machine learning and outline some of the most common machine learning algorithms. Next, we demonstrate how to apply the algorithms with appropriate toolkits to conduct machine learning experiments for clinical prediction tasks. The objectives of this chapter are to (1) understand the basics of machine learning techniques and the reasons behind why they are useful for solving clinical prediction problems, (2) understand the intuition behind some machine learning models, including regression, decision trees, and support vector machines, and (3) understand how to apply these models to clinical prediction problems using publicly available datasets via case studies.",
    "Link": "http://arxiv.org/abs/1909.09246v1",
    "PDF Link": "http://arxiv.org/pdf/1909.09246v1"
  },
  {
    "Title": "Towards Modular Machine Learning Solution Development: Benefits and\n  Trade-offs",
    "Authors": "Samiyuru Menik, Lakshmish Ramaswamy",
    "Published": "2023-01-23T22:54:34Z",
    "Summary": "Machine learning technologies have demonstrated immense capabilities in various domains. They play a key role in the success of modern businesses. However, adoption of machine learning technologies has a lot of untouched potential. Cost of developing custom machine learning solutions that solve unique business problems is a major inhibitor to far-reaching adoption of machine learning technologies. We recognize that the monolithic nature prevalent in today's machine learning applications stands in the way of efficient and cost effective customized machine learning solution development. In this work we explore the benefits of modular machine learning solutions and discuss how modular machine learning solutions can overcome some of the major solution engineering limitations of monolithic machine learning solutions. We analyze the trade-offs between modular and monolithic machine learning solutions through three deep learning problems; one text based and the two image based. Our experimental results show that modular machine learning solutions have a promising potential to reap the solution engineering advantages of modularity while gaining performance and data advantages in a way the monolithic machine learning solutions do not permit.",
    "Link": "http://arxiv.org/abs/2301.09753v1",
    "PDF Link": "http://arxiv.org/pdf/2301.09753v1"
  },
  {
    "Title": "Introduction to Machine Learning: Class Notes 67577",
    "Authors": "Amnon Shashua",
    "Published": "2009-04-23T11:40:57Z",
    "Summary": "Introduction to Machine learning covering Statistical Inference (Bayes, EM, ML/MaxEnt duality), algebraic and spectral methods (PCA, LDA, CCA, Clustering), and PAC learning (the Formal model, VC dimension, Double Sampling theorem).",
    "Link": "http://arxiv.org/abs/0904.3664v1",
    "PDF Link": "http://arxiv.org/pdf/0904.3664v1"
  },
  {
    "Title": "The Tribes of Machine Learning and the Realm of Computer Architecture",
    "Authors": "Ayaz Akram, Jason Lowe-Power",
    "Published": "2020-12-07T23:10:51Z",
    "Summary": "Machine learning techniques have influenced the field of computer architecture like many other fields. This paper studies how the fundamental machine learning techniques can be applied towards computer architecture problems. We also provide a detailed survey of computer architecture research that employs different machine learning methods. Finally, we present some future opportunities and the outstanding challenges that need to be overcome to exploit full potential of machine learning for computer architecture.",
    "Link": "http://arxiv.org/abs/2012.04105v1",
    "PDF Link": "http://arxiv.org/pdf/2012.04105v1"
  },
  {
    "Title": "A Machine Learning Tutorial for Operational Meteorology, Part I:\n  Traditional Machine Learning",
    "Authors": "Randy J. Chase, David R. Harrison, Amanda Burke, Gary M. Lackmann, Amy McGovern",
    "Published": "2022-04-15T14:48:04Z",
    "Summary": "Recently, the use of machine learning in meteorology has increased greatly. While many machine learning methods are not new, university classes on machine learning are largely unavailable to meteorology students and are not required to become a meteorologist. The lack of formal instruction has contributed to perception that machine learning methods are 'black boxes' and thus end-users are hesitant to apply the machine learning methods in their every day workflow. To reduce the opaqueness of machine learning methods and lower hesitancy towards machine learning in meteorology, this paper provides a survey of some of the most common machine learning methods. A familiar meteorological example is used to contextualize the machine learning methods while also discussing machine learning topics using plain language. The following machine learning methods are demonstrated: linear regression; logistic regression; decision trees; random forest; gradient boosted decision trees; naive Bayes; and support vector machines. Beyond discussing the different methods, the paper also contains discussions on the general machine learning process as well as best practices to enable readers to apply machine learning to their own datasets. Furthermore, all code (in the form of Jupyter notebooks and Google Colaboratory notebooks) used to make the examples in the paper is provided in an effort to catalyse the use of machine learning in meteorology.",
    "Link": "http://arxiv.org/abs/2204.07492v2",
    "PDF Link": "http://arxiv.org/pdf/2204.07492v2"
  },
  {
    "Title": "Position Paper: Towards Transparent Machine Learning",
    "Authors": "Dustin Juliano",
    "Published": "2019-11-12T10:49:55Z",
    "Summary": "Transparent machine learning is introduced as an alternative form of machine learning, where both the model and the learning system are represented in source code form. The goal of this project is to enable direct human understanding of machine learning models, giving us the ability to learn, verify, and refine them as programs. If solved, this technology could represent a best-case scenario for the safety and security of AI systems going forward.",
    "Link": "http://arxiv.org/abs/1911.06612v1",
    "PDF Link": "http://arxiv.org/pdf/1911.06612v1"
  },
  {
    "Title": "Understanding Bias in Machine Learning",
    "Authors": "Jindong Gu, Daniela Oelke",
    "Published": "2019-09-02T20:36:19Z",
    "Summary": "Bias is known to be an impediment to fair decisions in many domains such as human resources, the public sector, health care etc. Recently, hope has been expressed that the use of machine learning methods for taking such decisions would diminish or even resolve the problem. At the same time, machine learning experts warn that machine learning models can be biased as well. In this article, our goal is to explain the issue of bias in machine learning from a technical perspective and to illustrate the impact that biased data can have on a machine learning model. To reach such a goal, we develop interactive plots to visualizing the bias learned from synthetic data.",
    "Link": "http://arxiv.org/abs/1909.01866v1",
    "PDF Link": "http://arxiv.org/pdf/1909.01866v1"
  },
  {
    "Title": "A Unified Analytical Framework for Trustable Machine Learning and\n  Automation Running with Blockchain",
    "Authors": "Tao Wang",
    "Published": "2019-03-21T02:17:08Z",
    "Summary": "Traditional machine learning algorithms use data from databases that are mutable, and therefore the data cannot be fully trusted. Also, the machine learning process is difficult to automate. This paper proposes building a trustable machine learning system by using blockchain technology, which can store data in a permanent and immutable way. In addition, smart contracts are used to automate the machine learning process. This paper makes three contributions. First, it establishes a link between machine learning technology and blockchain technology. Previously, machine learning and blockchain have been considered two independent technologies without an obvious link. Second, it proposes a unified analytical framework for trustable machine learning by using blockchain technology. This unified framework solves both the trustability and automation issues in machine learning. Third, it enables a computer to translate core machine learning implementation from a single thread on a single machine to multiple threads on multiple machines running with blockchain by using a unified approach. The paper uses association rule mining as an example to demonstrate how trustable machine learning can be implemented with blockchain, and it shows how this approach can be used to analyze opioid prescriptions to help combat the opioid crisis.",
    "Link": "http://arxiv.org/abs/1903.08801v1",
    "PDF Link": "http://arxiv.org/pdf/1903.08801v1"
  },
  {
    "Title": "MLBench: How Good Are Machine Learning Clouds for Binary Classification\n  Tasks on Structured Data?",
    "Authors": "Yu Liu, Hantian Zhang, Luyuan Zeng, Wentao Wu, Ce Zhang",
    "Published": "2017-07-29T21:59:18Z",
    "Summary": "We conduct an empirical study of machine learning functionalities provided by major cloud service providers, which we call machine learning clouds. Machine learning clouds hold the promise of hiding all the sophistication of running large-scale machine learning: Instead of specifying how to run a machine learning task, users only specify what machine learning task to run and the cloud figures out the rest. Raising the level of abstraction, however, rarely comes free - a performance penalty is possible. How good, then, are current machine learning clouds on real-world machine learning workloads?   We study this question with a focus on binary classication problems. We present mlbench, a novel benchmark constructed by harvesting datasets from Kaggle competitions. We then compare the performance of the top winning code available from Kaggle with that of running machine learning clouds from both Azure and Amazon on mlbench. Our comparative study reveals the strength and weakness of existing machine learning clouds and points out potential future directions for improvement.",
    "Link": "http://arxiv.org/abs/1707.09562v3",
    "PDF Link": "http://arxiv.org/pdf/1707.09562v3"
  },
  {
    "Title": "Data Pricing in Machine Learning Pipelines",
    "Authors": "Zicun Cong, Xuan Luo, Pei Jian, Feida Zhu, Yong Zhang",
    "Published": "2021-08-18T00:57:06Z",
    "Summary": "Machine learning is disruptive. At the same time, machine learning can only succeed by collaboration among many parties in multiple steps naturally as pipelines in an eco-system, such as collecting data for possible machine learning applications, collaboratively training models by multiple parties and delivering machine learning services to end users. Data is critical and penetrating in the whole machine learning pipelines. As machine learning pipelines involve many parties and, in order to be successful, have to form a constructive and dynamic eco-system, marketplaces and data pricing are fundamental in connecting and facilitating those many parties. In this article, we survey the principles and the latest research development of data pricing in machine learning pipelines. We start with a brief review of data marketplaces and pricing desiderata. Then, we focus on pricing in three important steps in machine learning pipelines. To understand pricing in the step of training data collection, we review pricing raw data sets and data labels. We also investigate pricing in the step of collaborative training of machine learning models, and overview pricing machine learning models for end users in the step of machine learning deployment. We also discuss a series of possible future directions.",
    "Link": "http://arxiv.org/abs/2108.07915v1",
    "PDF Link": "http://arxiv.org/pdf/2108.07915v1"
  },
  {
    "Title": "Techniques for Automated Machine Learning",
    "Authors": "Yi-Wei Chen, Qingquan Song, Xia Hu",
    "Published": "2019-07-21T04:03:36Z",
    "Summary": "Automated machine learning (AutoML) aims to find optimal machine learning solutions automatically given a machine learning problem. It could release the burden of data scientists from the multifarious manual tuning process and enable the access of domain experts to the off-the-shelf machine learning solutions without extensive experience. In this paper, we review the current developments of AutoML in terms of three categories, automated feature engineering (AutoFE), automated model and hyperparameter learning (AutoMHL), and automated deep learning (AutoDL). State-of-the-art techniques adopted in the three categories are presented, including Bayesian optimization, reinforcement learning, evolutionary algorithm, and gradient-based approaches. We summarize popular AutoML frameworks and conclude with current open challenges of AutoML.",
    "Link": "http://arxiv.org/abs/1907.08908v1",
    "PDF Link": "http://arxiv.org/pdf/1907.08908v1"
  },
  {
    "Title": "The Landscape of Modern Machine Learning: A Review of Machine,\n  Distributed and Federated Learning",
    "Authors": "Omer Subasi, Oceane Bel, Joseph Manzano, Kevin Barker",
    "Published": "2023-12-05T20:40:05Z",
    "Summary": "With the advance of the powerful heterogeneous, parallel and distributed computing systems and ever increasing immense amount of data, machine learning has become an indispensable part of cutting-edge technology, scientific research and consumer products. In this study, we present a review of modern machine and deep learning. We provide a high-level overview for the latest advanced machine learning algorithms, applications, and frameworks. Our discussion encompasses parallel distributed learning, deep learning as well as federated learning. As a result, our work serves as an introductory text to the vast field of modern machine learning.",
    "Link": "http://arxiv.org/abs/2312.03120v1",
    "PDF Link": "http://arxiv.org/pdf/2312.03120v1"
  },
  {
    "Title": "Parallelization of Machine Learning Algorithms Respectively on Single\n  Machine and Spark",
    "Authors": "Jiajun Shen",
    "Published": "2022-05-08T03:47:30Z",
    "Summary": "With the rapid development of big data technologies, how to dig out useful information from massive data becomes an essential problem. However, using machine learning algorithms to analyze large data may be time-consuming and inefficient on the traditional single machine. To solve these problems, this paper has made some research on the parallelization of several classic machine learning algorithms respectively on the single machine and the big data platform Spark. We compare the runtime and efficiency of traditional machine learning algorithms with parallelized machine learning algorithms respectively on the single machine and Spark platform. The research results have shown significant improvement in runtime and efficiency of parallelized machine learning algorithms.",
    "Link": "http://arxiv.org/abs/2206.07090v2",
    "PDF Link": "http://arxiv.org/pdf/2206.07090v2"
  },
  {
    "Title": "AutoCompete: A Framework for Machine Learning Competition",
    "Authors": "Abhishek Thakur, Artus Krohn-Grimberghe",
    "Published": "2015-07-08T15:07:39Z",
    "Summary": "In this paper, we propose AutoCompete, a highly automated machine learning framework for tackling machine learning competitions. This framework has been learned by us, validated and improved over a period of more than two years by participating in online machine learning competitions. It aims at minimizing human interference required to build a first useful predictive model and to assess the practical difficulty of a given machine learning challenge. The proposed system helps in identifying data types, choosing a machine learn- ing model, tuning hyper-parameters, avoiding over-fitting and optimization for a provided evaluation metric. We also observe that the proposed system produces better (or comparable) results with less runtime as compared to other approaches.",
    "Link": "http://arxiv.org/abs/1507.02188v1",
    "PDF Link": "http://arxiv.org/pdf/1507.02188v1"
  },
  {
    "Title": "Joint Training of Deep Boltzmann Machines",
    "Authors": "Ian Goodfellow, Aaron Courville, Yoshua Bengio",
    "Published": "2012-12-12T01:59:27Z",
    "Summary": "We introduce a new method for training deep Boltzmann machines jointly. Prior methods require an initial learning pass that trains the deep Boltzmann machine greedily, one layer at a time, or do not perform well on classifi- cation tasks.",
    "Link": "http://arxiv.org/abs/1212.2686v1",
    "PDF Link": "http://arxiv.org/pdf/1212.2686v1"
  },
  {
    "Title": "Proceedings of the 2016 ICML Workshop on #Data4Good: Machine Learning in\n  Social Good Applications",
    "Authors": "Kush R. Varshney",
    "Published": "2016-07-08T16:55:31Z",
    "Summary": "This is the Proceedings of the ICML Workshop on #Data4Good: Machine Learning in Social Good Applications, which was held on June 24, 2016 in New York.",
    "Link": "http://arxiv.org/abs/1607.02450v2",
    "PDF Link": "http://arxiv.org/pdf/1607.02450v2"
  },
  {
    "Title": "Mathematical Perspective of Machine Learning",
    "Authors": "Yarema Boryshchak",
    "Published": "2020-07-03T05:26:02Z",
    "Summary": "We take a closer look at some theoretical challenges of Machine Learning as a function approximation, gradient descent as the default optimization algorithm, limitations of fixed length and width networks and a different approach to RNNs from a mathematical perspective.",
    "Link": "http://arxiv.org/abs/2007.01503v1",
    "PDF Link": "http://arxiv.org/pdf/2007.01503v1"
  },
  {
    "Title": "Private Machine Learning via Randomised Response",
    "Authors": "David Barber",
    "Published": "2020-01-14T17:56:16Z",
    "Summary": "We introduce a general learning framework for private machine learning based on randomised response. Our assumption is that all actors are potentially adversarial and as such we trust only to release a single noisy version of an individual's datapoint. We discuss a general approach that forms a consistent way to estimate the true underlying machine learning model and demonstrate this in the case of logistic regression.",
    "Link": "http://arxiv.org/abs/2001.04942v2",
    "PDF Link": "http://arxiv.org/pdf/2001.04942v2"
  },
  {
    "Title": "Ten-year Survival Prediction for Breast Cancer Patients",
    "Authors": "Changmao Li, Han He, Yunze Hao, Caleb Ziems",
    "Published": "2019-11-02T19:53:32Z",
    "Summary": "This report assesses different machine learning approaches to 10-year survival prediction of breast cancer patients.",
    "Link": "http://arxiv.org/abs/1911.00776v1",
    "PDF Link": "http://arxiv.org/pdf/1911.00776v1"
  },
  {
    "Title": "A Survey of Optimization Methods from a Machine Learning Perspective",
    "Authors": "Shiliang Sun, Zehui Cao, Han Zhu, Jing Zhao",
    "Published": "2019-06-17T02:54:51Z",
    "Summary": "Machine learning develops rapidly, which has made many theoretical breakthroughs and is widely applied in various fields. Optimization, as an important part of machine learning, has attracted much attention of researchers. With the exponential growth of data amount and the increase of model complexity, optimization methods in machine learning face more and more challenges. A lot of work on solving optimization problems or improving optimization methods in machine learning has been proposed successively. The systematic retrospect and summary of the optimization methods from the perspective of machine learning are of great significance, which can offer guidance for both developments of optimization and machine learning research. In this paper, we first describe the optimization problems in machine learning. Then, we introduce the principles and progresses of commonly used optimization methods. Next, we summarize the applications and developments of optimization methods in some popular machine learning fields. Finally, we explore and give some challenges and open problems for the optimization in machine learning.",
    "Link": "http://arxiv.org/abs/1906.06821v2",
    "PDF Link": "http://arxiv.org/pdf/1906.06821v2"
  },
  {
    "Title": "When Machine Learning Meets Privacy: A Survey and Outlook",
    "Authors": "Bo Liu, Ming Ding, Sina Shaham, Wenny Rahayu, Farhad Farokhi, Zihuai Lin",
    "Published": "2020-11-24T00:52:49Z",
    "Summary": "The newly emerged machine learning (e.g. deep learning) methods have become a strong driving force to revolutionize a wide range of industries, such as smart healthcare, financial technology, and surveillance systems. Meanwhile, privacy has emerged as a big concern in this machine learning-based artificial intelligence era. It is important to note that the problem of privacy preservation in the context of machine learning is quite different from that in traditional data privacy protection, as machine learning can act as both friend and foe. Currently, the work on the preservation of privacy and machine learning (ML) is still in an infancy stage, as most existing solutions only focus on privacy problems during the machine learning process. Therefore, a comprehensive study on the privacy preservation problems and machine learning is required. This paper surveys the state of the art in privacy issues and solutions for machine learning. The survey covers three categories of interactions between privacy and machine learning: (i) private machine learning, (ii) machine learning aided privacy protection, and (iii) machine learning-based privacy attack and corresponding protection schemes. The current research progress in each category is reviewed and the key challenges are identified. Finally, based on our in-depth analysis of the area of privacy and machine learning, we point out future research directions in this field.",
    "Link": "http://arxiv.org/abs/2011.11819v1",
    "PDF Link": "http://arxiv.org/pdf/2011.11819v1"
  },
  {
    "Title": "Augmented Q Imitation Learning (AQIL)",
    "Authors": "Xiao Lei Zhang, Anish Agarwal",
    "Published": "2020-03-31T18:08:23Z",
    "Summary": "The study of unsupervised learning can be generally divided into two categories: imitation learning and reinforcement learning. In imitation learning the machine learns by mimicking the behavior of an expert system whereas in reinforcement learning the machine learns via direct environment feedback. Traditional deep reinforcement learning takes a significant time before the machine starts to converge to an optimal policy. This paper proposes Augmented Q-Imitation-Learning, a method by which deep reinforcement learning convergence can be accelerated by applying Q-imitation-learning as the initial training process in traditional Deep Q-learning.",
    "Link": "http://arxiv.org/abs/2004.00993v2",
    "PDF Link": "http://arxiv.org/pdf/2004.00993v2"
  },
  {
    "Title": "Probabilistic Machine Learning for Healthcare",
    "Authors": "Irene Y. Chen, Shalmali Joshi, Marzyeh Ghassemi, Rajesh Ranganath",
    "Published": "2020-09-23T12:14:05Z",
    "Summary": "Machine learning can be used to make sense of healthcare data. Probabilistic machine learning models help provide a complete picture of observed data in healthcare. In this review, we examine how probabilistic machine learning can advance healthcare. We consider challenges in the predictive model building pipeline where probabilistic models can be beneficial including calibration and missing data. Beyond predictive models, we also investigate the utility of probabilistic machine learning models in phenotyping, in generative models for clinical use cases, and in reinforcement learning.",
    "Link": "http://arxiv.org/abs/2009.11087v1",
    "PDF Link": "http://arxiv.org/pdf/2009.11087v1"
  },
  {
    "Title": "Evaluation Challenges for Geospatial ML",
    "Authors": "Esther Rolf",
    "Published": "2023-03-31T14:24:06Z",
    "Summary": "As geospatial machine learning models and maps derived from their predictions are increasingly used for downstream analyses in science and policy, it is imperative to evaluate their accuracy and applicability. Geospatial machine learning has key distinctions from other learning paradigms, and as such, the correct way to measure performance of spatial machine learning outputs has been a topic of debate. In this paper, I delineate unique challenges of model evaluation for geospatial machine learning with global or remotely sensed datasets, culminating in concrete takeaways to improve evaluations of geospatial model performance.",
    "Link": "http://arxiv.org/abs/2303.18087v1",
    "PDF Link": "http://arxiv.org/pdf/2303.18087v1"
  },
  {
    "Title": "A comprehensive review of Quantum Machine Learning: from NISQ to Fault\n  Tolerance",
    "Authors": "Yunfei Wang, Junyu Liu",
    "Published": "2024-01-21T00:19:16Z",
    "Summary": "Quantum machine learning, which involves running machine learning algorithms on quantum devices, has garnered significant attention in both academic and business circles. In this paper, we offer a comprehensive and unbiased review of the various concepts that have emerged in the field of quantum machine learning. This includes techniques used in Noisy Intermediate-Scale Quantum (NISQ) technologies and approaches for algorithms compatible with fault-tolerant quantum computing hardware. Our review covers fundamental concepts, algorithms, and the statistical learning theory pertinent to quantum machine learning.",
    "Link": "http://arxiv.org/abs/2401.11351v2",
    "PDF Link": "http://arxiv.org/pdf/2401.11351v2"
  },
  {
    "Title": "Towards CRISP-ML(Q): A Machine Learning Process Model with Quality\n  Assurance Methodology",
    "Authors": "Stefan Studer, Thanh Binh Bui, Christian Drescher, Alexander Hanuschkin, Ludwig Winkler, Steven Peters, Klaus-Robert Mueller",
    "Published": "2020-03-11T08:25:49Z",
    "Summary": "Machine learning is an established and frequently used technique in industry and academia but a standard process model to improve success and efficiency of machine learning applications is still missing. Project organizations and machine learning practitioners have a need for guidance throughout the life cycle of a machine learning application to meet business expectations. We therefore propose a process model for the development of machine learning applications, that covers six phases from defining the scope to maintaining the deployed machine learning application. The first phase combines business and data understanding as data availability oftentimes affects the feasibility of the project. The sixth phase covers state-of-the-art approaches for monitoring and maintenance of a machine learning applications, as the risk of model degradation in a changing environment is eminent. With each task of the process, we propose quality assurance methodology that is suitable to adress challenges in machine learning development that we identify in form of risks. The methodology is drawn from practical experience and scientific literature and has proven to be general and stable. The process model expands on CRISP-DM, a data mining process model that enjoys strong industry support but lacks to address machine learning specific tasks. Our work proposes an industry and application neutral process model tailored for machine learning applications with focus on technical tasks for quality assurance.",
    "Link": "http://arxiv.org/abs/2003.05155v2",
    "PDF Link": "http://arxiv.org/pdf/2003.05155v2"
  },
  {
    "Title": "Temporal-related Convolutional-Restricted-Boltzmann-Machine capable of\n  learning relational order via reinforcement learning procedure?",
    "Authors": "Zizhuang Wang",
    "Published": "2017-06-24T20:56:27Z",
    "Summary": "In this article, we extend the conventional framework of convolutional-Restricted-Boltzmann-Machine to learn highly abstract features among abitrary number of time related input maps by constructing a layer of multiplicative units, which capture the relations among inputs. In many cases, more than two maps are strongly related, so it is wise to make multiplicative unit learn relations among more input maps, in other words, to find the optimal relational-order of each unit. In order to enable our machine to learn relational order, we developed a reinforcement-learning method whose optimality is proven to train the network.",
    "Link": "http://arxiv.org/abs/1706.08001v1",
    "PDF Link": "http://arxiv.org/pdf/1706.08001v1"
  },
  {
    "Title": "Spatial Transfer Learning with Simple MLP",
    "Authors": "Hongjian Yang",
    "Published": "2024-05-05T20:39:15Z",
    "Summary": "First step to investigate the potential of transfer learning applied to the field of spatial statistics",
    "Link": "http://arxiv.org/abs/2405.03720v1",
    "PDF Link": "http://arxiv.org/pdf/2405.03720v1"
  },
  {
    "Title": "Proceedings of the 29th International Conference on Machine Learning\n  (ICML-12)",
    "Authors": "John Langford, Joelle Pineau",
    "Published": "2012-07-19T14:08:22Z",
    "Summary": "This is an index to the papers that appear in the Proceedings of the 29th International Conference on Machine Learning (ICML-12). The conference was held in Edinburgh, Scotland, June 27th - July 3rd, 2012.",
    "Link": "http://arxiv.org/abs/1207.4676v2",
    "PDF Link": "http://arxiv.org/pdf/1207.4676v2"
  },
  {
    "Title": "Distributed Multi-Task Learning with Shared Representation",
    "Authors": "Jialei Wang, Mladen Kolar, Nathan Srebro",
    "Published": "2016-03-07T18:11:54Z",
    "Summary": "We study the problem of distributed multi-task learning with shared representation, where each machine aims to learn a separate, but related, task in an unknown shared low-dimensional subspaces, i.e. when the predictor matrix has low rank. We consider a setting where each task is handled by a different machine, with samples for the task available locally on the machine, and study communication-efficient methods for exploiting the shared structure.",
    "Link": "http://arxiv.org/abs/1603.02185v1",
    "PDF Link": "http://arxiv.org/pdf/1603.02185v1"
  },
  {
    "Title": "Components of Machine Learning: Binding Bits and FLOPS",
    "Authors": "Alexander Jung",
    "Published": "2019-10-25T17:33:33Z",
    "Summary": "Many machine learning problems and methods are combinations of three components: data, hypothesis space and loss function. Different machine learning methods are obtained as combinations of different choices for the representation of data, hypothesis space and loss function. After reviewing the mathematical structure of these three components, we discuss intrinsic trade-offs between statistical and computational properties of machine learning methods.",
    "Link": "http://arxiv.org/abs/1910.12387v2",
    "PDF Link": "http://arxiv.org/pdf/1910.12387v2"
  },
  {
    "Title": "Impact of Legal Requirements on Explainability in Machine Learning",
    "Authors": "Adrien Bibal, Michael Lognoul, Alexandre de Streel, Benoît Frénay",
    "Published": "2020-07-10T16:57:18Z",
    "Summary": "The requirements on explainability imposed by European laws and their implications for machine learning (ML) models are not always clear. In that perspective, our research analyzes explanation obligations imposed for private and public decision-making, and how they can be implemented by machine learning techniques.",
    "Link": "http://arxiv.org/abs/2007.05479v1",
    "PDF Link": "http://arxiv.org/pdf/2007.05479v1"
  },
  {
    "Title": "Machine Learning Potential Repository",
    "Authors": "Atsuto Seko",
    "Published": "2020-07-27T14:30:23Z",
    "Summary": "This paper introduces a machine learning potential repository that includes Pareto optimal machine learning potentials. It also shows the systematic development of accurate and fast machine learning potentials for a wide range of elemental systems. As a result, many Pareto optimal machine learning potentials are available in the repository from a website. Therefore, the repository will help many scientists to perform accurate and fast atomistic simulations.",
    "Link": "http://arxiv.org/abs/2007.14206v1",
    "PDF Link": "http://arxiv.org/pdf/2007.14206v1"
  },
  {
    "Title": "Quantum memristors for neuromorphic quantum machine learning",
    "Authors": "Lucas Lamata",
    "Published": "2024-12-25T20:21:24Z",
    "Summary": "Quantum machine learning may permit to realize more efficient machine learning calculations with near-term quantum devices. Among the diverse quantum machine learning paradigms which are currently being considered, quantum memristors are promising as a way of combining, in the same quantum hardware, a unitary evolution with the nonlinearity provided by the measurement and feedforward. Thus, an efficient way of deploying neuromorphic quantum computing for quantum machine learning may be enabled.",
    "Link": "http://arxiv.org/abs/2412.18979v1",
    "PDF Link": "http://arxiv.org/pdf/2412.18979v1"
  },
  {
    "Title": "metric-learn: Metric Learning Algorithms in Python",
    "Authors": "William de Vazelhes, CJ Carey, Yuan Tang, Nathalie Vauquier, Aurélien Bellet",
    "Published": "2019-08-13T15:52:31Z",
    "Summary": "metric-learn is an open source Python package implementing supervised and weakly-supervised distance metric learning algorithms. As part of scikit-learn-contrib, it provides a unified interface compatible with scikit-learn which allows to easily perform cross-validation, model selection, and pipelining with other machine learning estimators. metric-learn is thoroughly tested and available on PyPi under the MIT licence.",
    "Link": "http://arxiv.org/abs/1908.04710v3",
    "PDF Link": "http://arxiv.org/pdf/1908.04710v3"
  },
  {
    "Title": "Theoretical Models of Learning to Learn",
    "Authors": "Jonathan Baxter",
    "Published": "2020-02-27T13:35:26Z",
    "Summary": "A Machine can only learn if it is biased in some way. Typically the bias is supplied by hand, for example through the choice of an appropriate set of features. However, if the learning machine is embedded within an {\\em environment} of related tasks, then it can {\\em learn} its own bias by learning sufficiently many tasks from the environment. In this paper two models of bias learning (or equivalently, learning to learn) are introduced and the main theoretical results presented. The first model is a PAC-type model based on empirical process theory, while the second is a hierarchical Bayes model.",
    "Link": "http://arxiv.org/abs/2002.12364v1",
    "PDF Link": "http://arxiv.org/pdf/2002.12364v1"
  },
  {
    "Title": "On-the-Fly Learning in a Perpetual Learning Machine",
    "Authors": "Andrew J. R. Simpson",
    "Published": "2015-09-03T01:30:29Z",
    "Summary": "Despite the promise of brain-inspired machine learning, deep neural networks (DNN) have frustratingly failed to bridge the deceptively large gap between learning and memory. Here, we introduce a Perpetual Learning Machine; a new type of DNN that is capable of brain-like dynamic 'on the fly' learning because it exists in a self-supervised state of Perpetual Stochastic Gradient Descent. Thus, we provide the means to unify learning and memory within a machine learning framework. We also explore the elegant duality of abstraction and synthesis: the Yin and Yang of deep learning.",
    "Link": "http://arxiv.org/abs/1509.00913v3",
    "PDF Link": "http://arxiv.org/pdf/1509.00913v3"
  },
  {
    "Title": "An Aggregate and Iterative Disaggregate Algorithm with Proven Optimality\n  in Machine Learning",
    "Authors": "Young Woong Park, Diego Klabjan",
    "Published": "2016-07-05T20:04:57Z",
    "Summary": "We propose a clustering-based iterative algorithm to solve certain optimization problems in machine learning, where we start the algorithm by aggregating the original data, solving the problem on aggregated data, and then in subsequent steps gradually disaggregate the aggregated data. We apply the algorithm to common machine learning problems such as the least absolute deviation regression problem, support vector machines, and semi-supervised support vector machines. We derive model-specific data aggregation and disaggregation procedures. We also show optimality, convergence, and the optimality gap of the approximated solution in each iteration. A computational study is provided.",
    "Link": "http://arxiv.org/abs/1607.01400v1",
    "PDF Link": "http://arxiv.org/pdf/1607.01400v1"
  },
  {
    "Title": "Human-in-the-loop Machine Learning: A Macro-Micro Perspective",
    "Authors": "Jiangtao Wang, Bin Guo, Liming Chen",
    "Published": "2022-02-21T22:45:59Z",
    "Summary": "Though technical advance of artificial intelligence and machine learning has enabled many promising intelligent systems, many computing tasks are still not able to be fully accomplished by machine intelligence. Motivated by the complementary nature of human and machine intelligence, an emerging trend is to involve humans in the loop of machine learning and decision-making. In this paper, we provide a macro-micro review of human-in-the-loop machine learning. We first describe major machine learning challenges which can be addressed by human intervention in the loop. Then we examine closely the latest research and findings of introducing humans into each step of the lifecycle of machine learning. Finally, we analyze current research gaps and point out future research directions.",
    "Link": "http://arxiv.org/abs/2202.10564v1",
    "PDF Link": "http://arxiv.org/pdf/2202.10564v1"
  },
  {
    "Title": "Can Machines Learn the True Probabilities?",
    "Authors": "Jinsook Kim",
    "Published": "2024-07-08T00:19:43Z",
    "Summary": "When there exists uncertainty, AI machines are designed to make decisions so as to reach the best expected outcomes. Expectations are based on true facts about the objective environment the machines interact with, and those facts can be encoded into AI models in the form of true objective probability functions. Accordingly, AI models involve probabilistic machine learning in which the probabilities should be objectively interpreted. We prove under some basic assumptions when machines can learn the true objective probabilities, if any, and when machines cannot learn them.",
    "Link": "http://arxiv.org/abs/2407.05526v1",
    "PDF Link": "http://arxiv.org/pdf/2407.05526v1"
  },
  {
    "Title": "Scientific Machine Learning Benchmarks",
    "Authors": "Jeyan Thiyagalingam, Mallikarjun Shankar, Geoffrey Fox, Tony Hey",
    "Published": "2021-10-25T10:05:11Z",
    "Summary": "The breakthrough in Deep Learning neural networks has transformed the use of AI and machine learning technologies for the analysis of very large experimental datasets. These datasets are typically generated by large-scale experimental facilities at national laboratories. In the context of science, scientific machine learning focuses on training machines to identify patterns, trends, and anomalies to extract meaningful scientific insights from such datasets. With a new generation of experimental facilities, the rate of data generation and the scale of data volumes will increasingly require the use of more automated data analysis. At present, identifying the most appropriate machine learning algorithm for the analysis of any given scientific dataset is still a challenge for scientists. This is due to many different machine learning frameworks, computer architectures, and machine learning models. Historically, for modelling and simulation on HPC systems such problems have been addressed through benchmarking computer applications, algorithms, and architectures. Extending such a benchmarking approach and identifying metrics for the application of machine learning methods to scientific datasets is a new challenge for both scientists and computer scientists. In this paper, we describe our approach to the development of scientific machine learning benchmarks and review other approaches to benchmarking scientific machine learning.",
    "Link": "http://arxiv.org/abs/2110.12773v1",
    "PDF Link": "http://arxiv.org/pdf/2110.12773v1"
  },
  {
    "Title": "Some Insights into Lifelong Reinforcement Learning Systems",
    "Authors": "Changjian Li",
    "Published": "2020-01-27T07:26:12Z",
    "Summary": "A lifelong reinforcement learning system is a learning system that has the ability to learn through trail-and-error interaction with the environment over its lifetime. In this paper, I give some arguments to show that the traditional reinforcement learning paradigm fails to model this type of learning system. Some insights into lifelong reinforcement learning are provided, along with a simplistic prototype lifelong reinforcement learning system.",
    "Link": "http://arxiv.org/abs/2001.09608v1",
    "PDF Link": "http://arxiv.org/pdf/2001.09608v1"
  },
  {
    "Title": "Bayesian Optimization for Machine Learning : A Practical Guidebook",
    "Authors": "Ian Dewancker, Michael McCourt, Scott Clark",
    "Published": "2016-12-14T22:04:33Z",
    "Summary": "The engineering of machine learning systems is still a nascent field; relying on a seemingly daunting collection of quickly evolving tools and best practices. It is our hope that this guidebook will serve as a useful resource for machine learning practitioners looking to take advantage of Bayesian optimization techniques. We outline four example machine learning problems that can be solved using open source machine learning libraries, and highlight the benefits of using Bayesian optimization in the context of these common machine learning applications.",
    "Link": "http://arxiv.org/abs/1612.04858v1",
    "PDF Link": "http://arxiv.org/pdf/1612.04858v1"
  },
  {
    "Title": "Solving machine learning optimization problems using quantum computers",
    "Authors": "Venkat R. Dasari, Mee Seong Im, Lubjana Beshaj",
    "Published": "2019-11-17T17:36:41Z",
    "Summary": "Classical optimization algorithms in machine learning often take a long time to compute when applied to a multi-dimensional problem and require a huge amount of CPU and GPU resource. Quantum parallelism has a potential to speed up machine learning algorithms. We describe a generic mathematical model to leverage quantum parallelism to speed-up machine learning algorithms. We also apply quantum machine learning and quantum parallelism applied to a $3$-dimensional image that vary with time.",
    "Link": "http://arxiv.org/abs/1911.08587v1",
    "PDF Link": "http://arxiv.org/pdf/1911.08587v1"
  },
  {
    "Title": "mlr3proba: An R Package for Machine Learning in Survival Analysis",
    "Authors": "Raphael Sonabend, Franz J. Király, Andreas Bender, Bernd Bischl, Michel Lang",
    "Published": "2020-08-18T11:21:24Z",
    "Summary": "As machine learning has become increasingly popular over the last few decades, so too has the number of machine learning interfaces for implementing these models. Whilst many R libraries exist for machine learning, very few offer extended support for survival analysis. This is problematic considering its importance in fields like medicine, bioinformatics, economics, engineering, and more. mlr3proba provides a comprehensive machine learning interface for survival analysis and connects with mlr3's general model tuning and benchmarking facilities to provide a systematic infrastructure for survival modeling and evaluation.",
    "Link": "http://arxiv.org/abs/2008.08080v2",
    "PDF Link": "http://arxiv.org/pdf/2008.08080v2"
  },
  {
    "Title": "Towards A Rigorous Science of Interpretable Machine Learning",
    "Authors": "Finale Doshi-Velez, Been Kim",
    "Published": "2017-02-28T02:19:20Z",
    "Summary": "As machine learning systems become ubiquitous, there has been a surge of interest in interpretable machine learning: systems that provide explanation for their outputs. These explanations are often used to qualitatively assess other criteria such as safety or non-discrimination. However, despite the interest in interpretability, there is very little consensus on what interpretable machine learning is and how it should be measured. In this position paper, we first define interpretability and describe when interpretability is needed (and when it is not). Next, we suggest a taxonomy for rigorous evaluation and expose open questions towards a more rigorous science of interpretable machine learning.",
    "Link": "http://arxiv.org/abs/1702.08608v2",
    "PDF Link": "http://arxiv.org/pdf/1702.08608v2"
  },
  {
    "Title": "Infrastructure for Usable Machine Learning: The Stanford DAWN Project",
    "Authors": "Peter Bailis, Kunle Olukotun, Christopher Re, Matei Zaharia",
    "Published": "2017-05-22T02:28:19Z",
    "Summary": "Despite incredible recent advances in machine learning, building machine learning applications remains prohibitively time-consuming and expensive for all but the best-trained, best-funded engineering organizations. This expense comes not from a need for new and improved statistical models but instead from a lack of systems and tools for supporting end-to-end machine learning application development, from data preparation and labeling to productionization and monitoring. In this document, we outline opportunities for infrastructure supporting usable, end-to-end machine learning applications in the context of the nascent DAWN (Data Analytics for What's Next) project at Stanford.",
    "Link": "http://arxiv.org/abs/1705.07538v2",
    "PDF Link": "http://arxiv.org/pdf/1705.07538v2"
  },
  {
    "Title": "The Governance of Physical Artificial Intelligence",
    "Authors": "Yingbo Li, Anamaria-Beatrice Spulber, Yucong Duan",
    "Published": "2023-04-06T08:26:38Z",
    "Summary": "Physical artificial intelligence can prove to be one of the most important challenges of the artificial intelligence. The governance of physical artificial intelligence would define its responsible intelligent application in the society.",
    "Link": "http://arxiv.org/abs/2304.02924v1",
    "PDF Link": "http://arxiv.org/pdf/2304.02924v1"
  },
  {
    "Title": "Does an artificial intelligence perform market manipulation with its own\n  discretion? -- A genetic algorithm learns in an artificial market simulation",
    "Authors": "Takanobu Mizuta",
    "Published": "2020-05-21T07:00:31Z",
    "Summary": "Who should be charged with responsibility for an artificial intelligence performing market manipulation have been discussed. In this study, I constructed an artificial intelligence using a genetic algorithm that learns in an artificial market simulation, and investigated whether the artificial intelligence discovers market manipulation through learning with an artificial market simulation despite a builder of artificial intelligence has no intention of market manipulation. As a result, the artificial intelligence discovered market manipulation as an optimal investment strategy. This result suggests necessity of regulation, such as obligating builders of artificial intelligence to prevent artificial intelligence from performing market manipulation.",
    "Link": "http://arxiv.org/abs/2005.10488v1",
    "PDF Link": "http://arxiv.org/pdf/2005.10488v1"
  },
  {
    "Title": "Impact of Artificial Intelligence on Economic Theory",
    "Authors": "Tshilidzi Marwala",
    "Published": "2015-07-01T16:26:21Z",
    "Summary": "Artificial intelligence has impacted many aspects of human life. This paper studies the impact of artificial intelligence on economic theory. In particular we study the impact of artificial intelligence on the theory of bounded rationality, efficient market hypothesis and prospect theory.",
    "Link": "http://arxiv.org/abs/1509.01213v1",
    "PDF Link": "http://arxiv.org/pdf/1509.01213v1"
  },
  {
    "Title": "The case for psychometric artificial general intelligence",
    "Authors": "Mark McPherson",
    "Published": "2020-12-27T23:45:03Z",
    "Summary": "A short review of the literature on measurement and detection of artificial general intelligence is made. Proposed benchmarks and tests for artificial general intelligence are critically evaluated against multiple criteria. Based on the findings, the most promising approaches are identified and some useful directions for future work are proposed.",
    "Link": "http://arxiv.org/abs/2101.02179v1",
    "PDF Link": "http://arxiv.org/pdf/2101.02179v1"
  },
  {
    "Title": "Proceedings of the Thirteenth Conference on Uncertainty in Artificial\n  Intelligence (1997)",
    "Authors": "Dan Geiger, Prakash Shenoy",
    "Published": "2013-04-13T20:44:25Z",
    "Summary": "This is the Proceedings of the Thirteenth Conference on Uncertainty in Artificial Intelligence, which was held in Providence, RI, August 1-3, 1997",
    "Link": "http://arxiv.org/abs/1304.3846v1",
    "PDF Link": "http://arxiv.org/pdf/1304.3846v1"
  },
  {
    "Title": "Proceedings of the Ninth Conference on Uncertainty in Artificial\n  Intelligence (1993)",
    "Authors": "David Heckerman, E. Mamdani",
    "Published": "2013-04-13T21:03:12Z",
    "Summary": "This is the Proceedings of the Ninth Conference on Uncertainty in Artificial Intelligence, which was held in Washington, DC, July 9-11, 1993",
    "Link": "http://arxiv.org/abs/1304.3851v1",
    "PDF Link": "http://arxiv.org/pdf/1304.3851v1"
  },
  {
    "Title": "Proceedings of the Second Conference on Uncertainty in Artificial\n  Intelligence (1986)",
    "Authors": "Laveen Kanal, John Lemmer",
    "Published": "2013-04-13T21:37:12Z",
    "Summary": "This is the Proceedings of the Second Conference on Uncertainty in Artificial Intelligence, which was held in Philadelphia, PA, August 8-10, 1986",
    "Link": "http://arxiv.org/abs/1304.3859v1",
    "PDF Link": "http://arxiv.org/pdf/1304.3859v1"
  },
  {
    "Title": "Artificial Intelligence in Humans",
    "Authors": "Michael Swan Laufer",
    "Published": "2013-10-30T14:19:49Z",
    "Summary": "In this paper, I put forward that in many instances, thinking mechanisms are equivalent to artificial intelligence modules programmed into the human mind.",
    "Link": "http://arxiv.org/abs/1311.0716v1",
    "PDF Link": "http://arxiv.org/pdf/1311.0716v1"
  },
  {
    "Title": "AAAI FSS-18: Artificial Intelligence in Government and Public Sector\n  Proceedings",
    "Authors": "Frank Stein, Alun Preece, Mihai Boicu",
    "Published": "2018-10-14T11:40:30Z",
    "Summary": "Proceedings of the AAAI Fall Symposium on Artificial Intelligence in Government and Public Sector, Arlington, Virginia, USA, October 18-20, 2018",
    "Link": "http://arxiv.org/abs/1810.06018v1",
    "PDF Link": "http://arxiv.org/pdf/1810.06018v1"
  },
  {
    "Title": "Watershed of Artificial Intelligence: Human Intelligence, Machine\n  Intelligence, and Biological Intelligence",
    "Authors": "Li Weigang, Liriam Enamoto, Denise Leyi Li, Geraldo Pereira Rocha Filho",
    "Published": "2021-04-27T13:03:25Z",
    "Summary": "This article reviews the \"Once learning\" mechanism that was proposed 23 years ago and the subsequent successes of \"One-shot learning\" in image classification and \"You Only Look Once - YOLO\" in objective detection. Analyzing the current development of Artificial Intelligence (AI), the proposal is that AI should be clearly divided into the following categories: Artificial Human Intelligence (AHI), Artificial Machine Intelligence (AMI), and Artificial Biological Intelligence (ABI), which will also be the main directions of theory and application development for AI. As a watershed for the branches of AI, some classification standards and methods are discussed: 1) Human-oriented, machine-oriented, and biological-oriented AI R&D; 2) Information input processed by Dimensionality-up or Dimensionality-reduction; 3) The use of one/few or large samples for knowledge learning.",
    "Link": "http://arxiv.org/abs/2104.13155v2",
    "PDF Link": "http://arxiv.org/pdf/2104.13155v2"
  },
  {
    "Title": "Perspective: Purposeful Failure in Artificial Life and Artificial\n  Intelligence",
    "Authors": "Lana Sinapayen",
    "Published": "2021-02-24T05:43:44Z",
    "Summary": "Complex systems fail. I argue that failures can be a blueprint characterizing living organisms and biological intelligence, a control mechanism to increase complexity in evolutionary simulations, and an alternative to classical fitness optimization. Imitating biological successes in Artificial Life and Artificial Intelligence can be misleading; imitating failures offers a path towards understanding and emulating life it in artificial systems.",
    "Link": "http://arxiv.org/abs/2102.12076v1",
    "PDF Link": "http://arxiv.org/pdf/2102.12076v1"
  },
  {
    "Title": "Comprehensible Artificial Intelligence on Knowledge Graphs: A survey",
    "Authors": "Simon Schramm, Christoph Wehner, Ute Schmid",
    "Published": "2024-04-04T14:57:32Z",
    "Summary": "Artificial Intelligence applications gradually move outside the safe walls of research labs and invade our daily lives. This is also true for Machine Learning methods on Knowledge Graphs, which has led to a steady increase in their application since the beginning of the 21st century. However, in many applications, users require an explanation of the Artificial Intelligences decision. This led to increased demand for Comprehensible Artificial Intelligence. Knowledge Graphs epitomize fertile soil for Comprehensible Artificial Intelligence, due to their ability to display connected data, i.e. knowledge, in a human- as well as machine-readable way. This survey gives a short history to Comprehensible Artificial Intelligence on Knowledge Graphs. Furthermore, we contribute by arguing that the concept Explainable Artificial Intelligence is overloaded and overlapping with Interpretable Machine Learning. By introducing the parent concept Comprehensible Artificial Intelligence, we provide a clear-cut distinction of both concepts while accounting for their similarities. Thus, we provide in this survey a case for Comprehensible Artificial Intelligence on Knowledge Graphs consisting of Interpretable Machine Learning on Knowledge Graphs and Explainable Artificial Intelligence on Knowledge Graphs. This leads to the introduction of a novel taxonomy for Comprehensible Artificial Intelligence on Knowledge Graphs. In addition, a comprehensive overview of the research on Comprehensible Artificial Intelligence on Knowledge Graphs is presented and put into the context of the taxonomy. Finally, research gaps in the field of Comprehensible Artificial Intelligence on Knowledge Graphs are identified for future research.",
    "Link": "http://arxiv.org/abs/2404.03499v1",
    "PDF Link": "http://arxiv.org/pdf/2404.03499v1"
  },
  {
    "Title": "Human $\\neq$ AGI",
    "Authors": "Roman V. Yampolskiy",
    "Published": "2020-07-11T14:06:13Z",
    "Summary": "Terms Artificial General Intelligence (AGI) and Human-Level Artificial Intelligence (HLAI) have been used interchangeably to refer to the Holy Grail of Artificial Intelligence (AI) research, creation of a machine capable of achieving goals in a wide range of environments. However, widespread implicit assumption of equivalence between capabilities of AGI and HLAI appears to be unjustified, as humans are not general intelligences. In this paper, we will prove this distinction.",
    "Link": "http://arxiv.org/abs/2007.07710v1",
    "PDF Link": "http://arxiv.org/pdf/2007.07710v1"
  },
  {
    "Title": "Artificial Intelligence Technology analysis using Artificial\n  Intelligence patent through Deep Learning model and vector space model",
    "Authors": "Yongmin Yoo, Dongjin Lim, Kyungsun Kim",
    "Published": "2021-11-08T00:10:49Z",
    "Summary": "Thanks to rapid development of artificial intelligence technology in recent years, the current artificial intelligence technology is contributing to many part of society. Education, environment, medical care, military, tourism, economy, politics, etc. are having a very large impact on society as a whole. For example, in the field of education, there is an artificial intelligence tutoring system that automatically assigns tutors based on student's level. In the field of economics, there are quantitative investment methods that automatically analyze large amounts of data to find investment laws to create investment models or predict changes in financial markets. As such, artificial intelligence technology is being used in various fields. So, it is very important to know exactly what factors have an important influence on each field of artificial intelligence technology and how the relationship between each field is connected. Therefore, it is necessary to analyze artificial intelligence technology in each field. In this paper, we analyze patent documents related to artificial intelligence technology. We propose a method for keyword analysis within factors using artificial intelligence patent data sets for artificial intelligence technology analysis. This is a model that relies on feature engineering based on deep learning model named KeyBERT, and using vector space model. A case study of collecting and analyzing artificial intelligence patent data was conducted to show how the proposed model can be applied to real world problems.",
    "Link": "http://arxiv.org/abs/2111.11295v1",
    "PDF Link": "http://arxiv.org/pdf/2111.11295v1"
  },
  {
    "Title": "Three IQs of AI Systems and their Testing Methods",
    "Authors": "Feng Liu, Yong Shi, Ying Liu",
    "Published": "2017-12-14T17:49:04Z",
    "Summary": "The rapid development of artificial intelligence has brought the artificial intelligence threat theory as well as the problem about how to evaluate the intelligence level of intelligent products. Both need to find a quantitative method to evaluate the intelligence level of intelligence systems, including human intelligence. Based on the standard intelligence system and the extended Von Neumann architecture, this paper proposes General IQ, Service IQ and Value IQ evaluation methods for intelligence systems, depending on different evaluation purposes. Among them, the General IQ of intelligence systems is to answer the question of whether the artificial intelligence can surpass the human intelligence, which is reflected in putting the intelligence systems on an equal status and conducting the unified evaluation. The Service IQ and Value IQ of intelligence systems are used to answer the question of how the intelligent products can better serve the human, reflecting the intelligence and required cost of each intelligence system as a product in the process of serving human.",
    "Link": "http://arxiv.org/abs/1712.06440v1",
    "PDF Link": "http://arxiv.org/pdf/1712.06440v1"
  },
  {
    "Title": "Examining correlation between trust and transparency with explainable\n  artificial intelligence",
    "Authors": "Arnav Kartikeya",
    "Published": "2021-08-10T16:24:30Z",
    "Summary": "Trust between humans and artificial intelligence(AI) is an issue which has implications in many fields of human computer interaction. The current issue with artificial intelligence is a lack of transparency into its decision making, and literature shows that increasing transparency increases trust. Explainable artificial intelligence has the ability to increase transparency of AI, which could potentially increase trust for humans. This paper attempts to use the task of predicting yelp review star ratings with assistance from an explainable and non explainable artificial intelligence to see if trust is increased with increased transparency. Results show that for these tasks, explainable artificial intelligence provided significant increase in trust as a measure of influence.",
    "Link": "http://arxiv.org/abs/2108.04770v1",
    "PDF Link": "http://arxiv.org/pdf/2108.04770v1"
  },
  {
    "Title": "The Artificial Scientist: Logicist, Emergentist, and Universalist\n  Approaches to Artificial General Intelligence",
    "Authors": "Michael Timothy Bennett, Yoshihiro Maruyama",
    "Published": "2021-10-05T05:58:23Z",
    "Summary": "We attempt to define what is necessary to construct an Artificial Scientist, explore and evaluate several approaches to artificial general intelligence (AGI) which may facilitate this, conclude that a unified or hybrid approach is necessary and explore two theories that satisfy this requirement to some degree.",
    "Link": "http://arxiv.org/abs/2110.01831v1",
    "PDF Link": "http://arxiv.org/pdf/2110.01831v1"
  },
  {
    "Title": "Proceedings of the Twenty-Seventh Conference on Uncertainty in\n  Artificial Intelligence (2011)",
    "Authors": "Fabio Cozman, Avi Pfeffer",
    "Published": "2012-05-11T18:35:50Z",
    "Summary": "This is the Proceedings of the Twenty-Seventh Conference on Uncertainty in Artificial Intelligence, which was held in Barcelona, Spain, July 14 - 17 2011.",
    "Link": "http://arxiv.org/abs/1205.2596v2",
    "PDF Link": "http://arxiv.org/pdf/1205.2596v2"
  },
  {
    "Title": "Proceedings of the Twenty-Sixth Conference on Uncertainty in Artificial\n  Intelligence (2010)",
    "Authors": "Peter Grunwald, Peter Spirtes",
    "Published": "2012-05-11T18:40:29Z",
    "Summary": "This is the Proceedings of the Twenty-Sixth Conference on Uncertainty in Artificial Intelligence, which was held on Catalina Island, CA, July 8 - 11 2010.",
    "Link": "http://arxiv.org/abs/1205.2597v2",
    "PDF Link": "http://arxiv.org/pdf/1205.2597v2"
  },
  {
    "Title": "Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial\n  Intelligence (2009)",
    "Authors": "Jeff Bilmes, Andrew Ng",
    "Published": "2012-06-13T16:43:44Z",
    "Summary": "This is the Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence, which was held in Montreal, QC, Canada, June 18 - 21 2009.",
    "Link": "http://arxiv.org/abs/1206.3959v2",
    "PDF Link": "http://arxiv.org/pdf/1206.3959v2"
  },
  {
    "Title": "Proceedings of the Twenty-Fourth Conference on Uncertainty in Artificial\n  Intelligence (2008)",
    "Authors": "David McAllester, Petri Myllymaki",
    "Published": "2012-08-25T18:22:17Z",
    "Summary": "This is the Proceedings of the Twenty-Fourth Conference on Uncertainty in Artificial Intelligence, which was held in Helsinki, Finland, July 9 - 12 2008.",
    "Link": "http://arxiv.org/abs/1208.5154v2",
    "PDF Link": "http://arxiv.org/pdf/1208.5154v2"
  },
  {
    "Title": "Proceedings of the Twenty-First Conference on Uncertainty in Artificial\n  Intelligence (2005)",
    "Authors": "Fahiem Bacchus, Tommi Jaakkola",
    "Published": "2012-08-25T18:44:38Z",
    "Summary": "This is the Proceedings of the Twenty-First Conference on Uncertainty in Artificial Intelligence, which was held in Edinburgh, Scotland July 26 - 29 2005.",
    "Link": "http://arxiv.org/abs/1208.5159v2",
    "PDF Link": "http://arxiv.org/pdf/1208.5159v2"
  },
  {
    "Title": "Proceedings of the Twenty-Second Conference on Uncertainty in Artificial\n  Intelligence (2006)",
    "Authors": "Rina Dechter, Thomas S. Richardson",
    "Published": "2012-08-25T18:45:05Z",
    "Summary": "This is the Proceedings of the Twenty-Second Conference on Uncertainty in Artificial Intelligence, which was held in Cambridge, MA, July 13 - 16 2006.",
    "Link": "http://arxiv.org/abs/1208.5160v2",
    "PDF Link": "http://arxiv.org/pdf/1208.5160v2"
  },
  {
    "Title": "Proceedings of the Twentieth Conference on Uncertainty in Artificial\n  Intelligence (2004)",
    "Authors": "Max Chickering, Joseph Halpern",
    "Published": "2012-08-25T18:48:34Z",
    "Summary": "This is the Proceedings of the Twentieth Conference on Uncertainty in Artificial Intelligence, which was held in Banff, Canada, July 7 - 11 2004.",
    "Link": "http://arxiv.org/abs/1208.5161v2",
    "PDF Link": "http://arxiv.org/pdf/1208.5161v2"
  },
  {
    "Title": "Proceedings of the Twenty-Eighth Conference on Uncertainty in Artificial\n  Intelligence (2012)",
    "Authors": "Nando de Freitas, Kevin Murphy",
    "Published": "2013-01-19T22:32:52Z",
    "Summary": "This is the Proceedings of the Twenty-Eighth Conference on Uncertainty in Artificial Intelligence, which was held on Catalina Island, CA August 14-18 2012.",
    "Link": "http://arxiv.org/abs/1301.4604v2",
    "PDF Link": "http://arxiv.org/pdf/1301.4604v2"
  },
  {
    "Title": "Proceedings of the Nineteenth Conference on Uncertainty in Artificial\n  Intelligence (2003)",
    "Authors": "Christopher Meek, Uffe Kjaerulff",
    "Published": "2013-01-19T23:12:33Z",
    "Summary": "This is the Proceedings of the Nineteenth Conference on Uncertainty in Artificial Intelligence, which was held in Acapulco, Mexico, August 7-10 2003",
    "Link": "http://arxiv.org/abs/1301.4606v2",
    "PDF Link": "http://arxiv.org/pdf/1301.4606v2"
  },
  {
    "Title": "Proceedings of the Seventeenth Conference on Uncertainty in Artificial\n  Intelligence (2001)",
    "Authors": "John Breese, Daphne Koller",
    "Published": "2013-01-19T23:16:59Z",
    "Summary": "This is the Proceedings of the Seventeenth Conference on Uncertainty in Artificial Intelligence, which was held in Seattle, WA, August 2-5 2001",
    "Link": "http://arxiv.org/abs/1301.4607v2",
    "PDF Link": "http://arxiv.org/pdf/1301.4607v2"
  },
  {
    "Title": "Proceedings of the Eighteenth Conference on Uncertainty in Artificial\n  Intelligence (2002)",
    "Authors": "Adnan Darwiche, Nir Friedman",
    "Published": "2013-01-19T23:17:26Z",
    "Summary": "This is the Proceedings of the Eighteenth Conference on Uncertainty in Artificial Intelligence, which was held in Alberta, Canada, August 1-4 2002",
    "Link": "http://arxiv.org/abs/1301.4608v2",
    "PDF Link": "http://arxiv.org/pdf/1301.4608v2"
  },
  {
    "Title": "Proceedings of the Sixteenth Conference on Uncertainty in Artificial\n  Intelligence (2000)",
    "Authors": "Craig Boutilier, Moises Goldszmidt",
    "Published": "2013-04-13T20:21:00Z",
    "Summary": "This is the Proceedings of the Sixteenth Conference on Uncertainty in Artificial Intelligence, which was held in San Francisco, CA, June 30 - July 3, 2000",
    "Link": "http://arxiv.org/abs/1304.3842v2",
    "PDF Link": "http://arxiv.org/pdf/1304.3842v2"
  },
  {
    "Title": "Proceedings of the Fifteenth Conference on Uncertainty in Artificial\n  Intelligence (1999)",
    "Authors": "Kathryn Laskey, Henri Prade",
    "Published": "2013-04-13T20:29:18Z",
    "Summary": "This is the Proceedings of the Fifteenth Conference on Uncertainty in Artificial Intelligence, which was held in Stockholm Sweden, July 30 - August 1, 1999",
    "Link": "http://arxiv.org/abs/1304.3843v2",
    "PDF Link": "http://arxiv.org/pdf/1304.3843v2"
  },
  {
    "Title": "Proceedings of the Fourteenth Conference on Uncertainty in Artificial\n  Intelligence (1998)",
    "Authors": "Gregory Cooper, Serafin Moral",
    "Published": "2013-04-13T20:34:30Z",
    "Summary": "This is the Proceedings of the Fourteenth Conference on Uncertainty in Artificial Intelligence, which was held in Madison, WI, July 24-26, 1998",
    "Link": "http://arxiv.org/abs/1304.3844v2",
    "PDF Link": "http://arxiv.org/pdf/1304.3844v2"
  },
  {
    "Title": "Proceedings of the Twelfth Conference on Uncertainty in Artificial\n  Intelligence (1996)",
    "Authors": "Eric Horvitz, Finn Jensen",
    "Published": "2013-04-13T20:49:49Z",
    "Summary": "This is the Proceedings of the Twelfth Conference on Uncertainty in Artificial Intelligence, which was held in Portland, OR, August 1-4, 1996",
    "Link": "http://arxiv.org/abs/1304.3847v2",
    "PDF Link": "http://arxiv.org/pdf/1304.3847v2"
  },
  {
    "Title": "Proceedings of the Eleventh Conference on Uncertainty in Artificial\n  Intelligence (1995)",
    "Authors": "Philippe Besnard, Steve Hanks",
    "Published": "2013-04-13T20:53:46Z",
    "Summary": "This is the Proceedings of the Eleventh Conference on Uncertainty in Artificial Intelligence, which was held in Montreal, QU, August 18-20, 1995",
    "Link": "http://arxiv.org/abs/1304.3848v2",
    "PDF Link": "http://arxiv.org/pdf/1304.3848v2"
  },
  {
    "Title": "Proceedings of the Tenth Conference on Uncertainty in Artificial\n  Intelligence (1994)",
    "Authors": "Ramon Lopez de Mantaras, David Poole",
    "Published": "2013-04-13T20:58:41Z",
    "Summary": "This is the Proceedings of the Tenth Conference on Uncertainty in Artificial Intelligence, which was held in Seattle, WA, July 29-31, 1994",
    "Link": "http://arxiv.org/abs/1304.3849v1",
    "PDF Link": "http://arxiv.org/pdf/1304.3849v1"
  },
  {
    "Title": "Proceedings of the Eighth Conference on Uncertainty in Artificial\n  Intelligence (1992)",
    "Authors": "Bruce D'Ambrosio, Didier Dubois, Philippe Smets, Michael Wellman",
    "Published": "2013-04-13T21:10:50Z",
    "Summary": "This is the Proceedings of the Eighth Conference on Uncertainty in Artificial Intelligence, which was held in Stanford, CA, July 17-19, 1992",
    "Link": "http://arxiv.org/abs/1304.3852v1",
    "PDF Link": "http://arxiv.org/pdf/1304.3852v1"
  },
  {
    "Title": "Proceedings of the Seventh Conference on Uncertainty in Artificial\n  Intelligence (1991)",
    "Authors": "Piero Bonissone, Bruce D'Ambrosio, Philippe Smets",
    "Published": "2013-04-13T21:18:04Z",
    "Summary": "This is the Proceedings of the Seventh Conference on Uncertainty in Artificial Intelligence, which was held in Los Angeles, CA, July 13-15, 1991",
    "Link": "http://arxiv.org/abs/1304.3853v1",
    "PDF Link": "http://arxiv.org/pdf/1304.3853v1"
  },
  {
    "Title": "Proceedings of the Sixth Conference on Uncertainty in Artificial\n  Intelligence (1990)",
    "Authors": "Piero Bonissone, Max Henrion, Laveen Kanal, John Lemmer",
    "Published": "2013-04-13T21:21:17Z",
    "Summary": "This is the Proceedings of the Sixth Conference on Uncertainty in Artificial Intelligence, which was held in Cambridge, MA, Jul 27 - Jul 29, 1990",
    "Link": "http://arxiv.org/abs/1304.3854v2",
    "PDF Link": "http://arxiv.org/pdf/1304.3854v2"
  },
  {
    "Title": "Proceedings of the Fifth Conference on Uncertainty in Artificial\n  Intelligence (1989)",
    "Authors": "Max Henrion, Laveen Kanal, John Lemmer, Ross Shachter",
    "Published": "2013-04-13T21:26:12Z",
    "Summary": "This is the Proceedings of the Fifth Conference on Uncertainty in Artificial Intelligence, which was held in Windsor, ON, August 18-20, 1989",
    "Link": "http://arxiv.org/abs/1304.3855v1",
    "PDF Link": "http://arxiv.org/pdf/1304.3855v1"
  },
  {
    "Title": "Proceedings of the Fourth Conference on Uncertainty in Artificial\n  Intelligence (1988)",
    "Authors": "Laveen Kanal, John Lemmer, Tod Levitt, Ross Shachter",
    "Published": "2013-04-13T21:30:26Z",
    "Summary": "This is the Proceedings of the Fourth Conference on Uncertainty in Artificial Intelligence, which was held in Minneapolis, MN, July 10-12, 1988",
    "Link": "http://arxiv.org/abs/1304.3856v1",
    "PDF Link": "http://arxiv.org/pdf/1304.3856v1"
  },
  {
    "Title": "Proceedings of the Third Conference on Uncertainty in Artificial\n  Intelligence (1987)",
    "Authors": "Laveen Kanal, John Lemmer, Tod Levitt",
    "Published": "2013-04-13T21:34:06Z",
    "Summary": "This is the Proceedings of the Third Conference on Uncertainty in Artificial Intelligence, which was held in Seattle, WA, July 10-12, 1987",
    "Link": "http://arxiv.org/abs/1304.3857v1",
    "PDF Link": "http://arxiv.org/pdf/1304.3857v1"
  },
  {
    "Title": "Proceedings of the First Conference on Uncertainty in Artificial\n  Intelligence (1985)",
    "Authors": "Laveen Kanal, John Lemmer",
    "Published": "2013-04-15T17:35:22Z",
    "Summary": "This is the Proceedings of the First Conference on Uncertainty in Artificial Intelligence, which was held in Los Angeles, CA, July 10-12, 1985",
    "Link": "http://arxiv.org/abs/1304.4182v2",
    "PDF Link": "http://arxiv.org/pdf/1304.4182v2"
  },
  {
    "Title": "Philosophy in the Face of Artificial Intelligence",
    "Authors": "Vincent Conitzer",
    "Published": "2016-05-19T16:45:12Z",
    "Summary": "In this article, I discuss how the AI community views concerns about the emergence of superintelligent AI and related philosophical issues.",
    "Link": "http://arxiv.org/abs/1605.06048v1",
    "PDF Link": "http://arxiv.org/pdf/1605.06048v1"
  },
  {
    "Title": "Advances in Artificial Intelligence Require Progress Across all of\n  Computer Science",
    "Authors": "Gregory D. Hager, Randal Bryant, Eric Horvitz, Maja Mataric, Vasant Honavar",
    "Published": "2017-07-13T23:11:18Z",
    "Summary": "Advances in Artificial Intelligence require progress across all of computer science.",
    "Link": "http://arxiv.org/abs/1707.04352v1",
    "PDF Link": "http://arxiv.org/pdf/1707.04352v1"
  },
  {
    "Title": "AAAI FSS-19: Artificial Intelligence in Government and Public Sector\n  Proceedings",
    "Authors": "Frank Stein, Alun Preece",
    "Published": "2019-11-04T12:26:51Z",
    "Summary": "Proceedings of the AAAI Fall Symposium on Artificial Intelligence in Government and Public Sector, Arlington, Virginia, USA, November 7-8, 2019",
    "Link": "http://arxiv.org/abs/1911.01156v2",
    "PDF Link": "http://arxiv.org/pdf/1911.01156v2"
  },
  {
    "Title": "Proceedings of the Twenty-Ninth Conference on Uncertainty in Artificial\n  Intelligence (2013)",
    "Authors": "Ann Nicholson, Padhriac Smyth",
    "Published": "2013-09-30T19:16:53Z",
    "Summary": "This is the Proceedings of the Twenty-Ninth Conference on Uncertainty in Artificial Intelligence, which was held in Bellevue, WA, August 11-15, 2013",
    "Link": "http://arxiv.org/abs/1309.7971v2",
    "PDF Link": "http://arxiv.org/pdf/1309.7971v2"
  },
  {
    "Title": "AAAI FSS-21: Artificial Intelligence in Government and Public Sector\n  Proceedings",
    "Authors": "Mihai Boicu, Erik Blasch, Alun Preece",
    "Published": "2021-12-10T15:48:31Z",
    "Summary": "Proceedings of the AAAI Fall Symposium on Artificial Intelligence in Government and Public Sector, Washington, DC, USA, November 4-6, 2021",
    "Link": "http://arxiv.org/abs/2112.05614v1",
    "PDF Link": "http://arxiv.org/pdf/2112.05614v1"
  },
  {
    "Title": "AAAI FSS-20: Artificial Intelligence in Government and Public Sector\n  Proceedings",
    "Authors": "Frank Stein, Alun Preece",
    "Published": "2020-11-09T16:08:42Z",
    "Summary": "Proceedings of the AAAI Fall Symposium on Artificial Intelligence in Government and Public Sector, Washington, DC, USA, November 13-14, 2020",
    "Link": "http://arxiv.org/abs/2011.04527v2",
    "PDF Link": "http://arxiv.org/pdf/2011.04527v2"
  },
  {
    "Title": "Can transformative AI shape a new age for our civilization?: Navigating\n  between speculation and reality",
    "Authors": "Jesus L. Lobo, Javier Del Ser",
    "Published": "2024-12-11T10:44:47Z",
    "Summary": "Artificial Intelligence is widely regarded as a transformative force with the potential to redefine numerous sectors of human civilization. While Artificial Intelligence has evolved from speculative fiction to a pivotal element of technological progress, its role as a truly transformative agent, or transformative Artificial Intelligence, remains a subject of debate. This work explores the historical precedents of technological breakthroughs, examining whether Artificial Intelligence can achieve a comparable impact, and it delves into various ethical frameworks that shape the perception and development of Artificial Intelligence. Additionally, it considers the societal, technical, and regulatory challenges that must be addressed for Artificial Intelligence to become a catalyst for global change. We also examine not only the strategies and methodologies that could lead to transformative Artificial Intelligence but also the barriers that could ultimately make these goals unattainable. We end with a critical inquiry into whether reaching a transformative Artificial Intelligence might compel humanity to adopt an entirely new ethical approach, tailored to the complexities of advanced Artificial Intelligence. By addressing the ethical, social, and scientific dimensions of Artificial Intelligence's development, this work contributes to the broader discourse on the long-term implications of Artificial Intelligence and its capacity to drive civilization toward a new era of progress or, conversely, exacerbate existing inequalities and risks.",
    "Link": "http://arxiv.org/abs/2412.08273v1",
    "PDF Link": "http://arxiv.org/pdf/2412.08273v1"
  },
  {
    "Title": "Intelligence of Astronomical Optical Telescope: Present Status and\n  Future Perspectives",
    "Authors": "Kang Huang, Tianzhu Hu, Jingyi Cai, Xiushan Pang, Yonghui Hou, Yong Zhang, Huaiqing Wang, Xiangqun Cui",
    "Published": "2023-06-29T10:17:28Z",
    "Summary": "Artificial intelligence technology has been widely used in astronomy, and new artificial intelligence technologies and application scenarios are constantly emerging. There have been a large number of papers reviewing the application of artificial intelligence technology in astronomy. However, relevant articles seldom mention telescope intelligence separately, and it is difficult to understand the current development status and research hotspots of telescope intelligence from these papers. This paper combines the development history of artificial intelligence technology and the difficulties of critical technologies of telescopes, comprehensively introduces the development and research hotspots of telescope intelligence, then conducts statistical analysis on various research directions of telescope intelligence and defines the research directions' merits. All kinds of research directions are evaluated, and the research trend of each telescope's intelligence is pointed out. Finally, according to the advantages of artificial intelligence technology and the development trend of telescopes, future research hotspots of telescope intelligence are given.",
    "Link": "http://arxiv.org/abs/2306.16834v2",
    "PDF Link": "http://arxiv.org/pdf/2306.16834v2"
  },
  {
    "Title": "AAAI-2019 Workshop on Games and Simulations for Artificial Intelligence",
    "Authors": "Marwan Mattar, Roozbeh Mottaghi, Julian Togelius, Danny Lange",
    "Published": "2019-03-06T04:49:07Z",
    "Summary": "This volume represents the accepted submissions from the AAAI-2019 Workshop on Games and Simulations for Artificial Intelligence held on January 29, 2019 in Honolulu, Hawaii, USA. https://www.gamesim.ai",
    "Link": "http://arxiv.org/abs/1903.02172v1",
    "PDF Link": "http://arxiv.org/pdf/1903.02172v1"
  },
  {
    "Title": "Opening the black box of deep learning",
    "Authors": "Dian Lei, Xiaoxiao Chen, Jianfei Zhao",
    "Published": "2018-05-22T02:12:33Z",
    "Summary": "The great success of deep learning shows that its technology contains profound truth, and understanding its internal mechanism not only has important implications for the development of its technology and effective application in various fields, but also provides meaningful insights into the understanding of human brain mechanism. At present, most of the theoretical research on deep learning is based on mathematics. This dissertation proposes that the neural network of deep learning is a physical system, examines deep learning from three different perspectives: microscopic, macroscopic, and physical world views, answers multiple theoretical puzzles in deep learning by using physics principles. For example, from the perspective of quantum mechanics and statistical physics, this dissertation presents the calculation methods for convolution calculation, pooling, normalization, and Restricted Boltzmann Machine, as well as the selection of cost functions, explains why deep learning must be deep, what characteristics are learned in deep learning, why Convolutional Neural Networks do not have to be trained layer by layer, and the limitations of deep learning, etc., and proposes the theoretical direction and basis for the further development of deep learning now and in the future. The brilliance of physics flashes in deep learning, we try to establish the deep learning technology based on the scientific theory of physics.",
    "Link": "http://arxiv.org/abs/1805.08355v1",
    "PDF Link": "http://arxiv.org/pdf/1805.08355v1"
  },
  {
    "Title": "Concept-Oriented Deep Learning",
    "Authors": "Daniel T Chang",
    "Published": "2018-06-05T15:50:30Z",
    "Summary": "Concepts are the foundation of human deep learning, understanding, and knowledge integration and transfer. We propose concept-oriented deep learning (CODL) which extends (machine) deep learning with concept representations and conceptual understanding capability. CODL addresses some of the major limitations of deep learning: interpretability, transferability, contextual adaptation, and requirement for lots of labeled training data. We discuss the major aspects of CODL including concept graph, concept representations, concept exemplars, and concept representation learning systems supporting incremental and continual learning.",
    "Link": "http://arxiv.org/abs/1806.01756v1",
    "PDF Link": "http://arxiv.org/pdf/1806.01756v1"
  },
  {
    "Title": "Deep learning research landscape & roadmap in a nutshell: past, present\n  and future -- Towards deep cortical learning",
    "Authors": "Aras R. Dargazany",
    "Published": "2019-07-30T16:57:38Z",
    "Summary": "The past, present and future of deep learning is presented in this work. Given this landscape & roadmap, we predict that deep cortical learning will be the convergence of deep learning & cortical learning which builds an artificial cortical column ultimately.",
    "Link": "http://arxiv.org/abs/1908.02130v1",
    "PDF Link": "http://arxiv.org/pdf/1908.02130v1"
  },
  {
    "Title": "A First Look at Deep Learning Apps on Smartphones",
    "Authors": "Mengwei Xu, Jiawei Liu, Yuanqiang Liu, Felix Xiaozhu Lin, Yunxin Liu, Xuanzhe Liu",
    "Published": "2018-11-08T07:59:23Z",
    "Summary": "We are in the dawn of deep learning explosion for smartphones. To bridge the gap between research and practice, we present the first empirical study on 16,500 the most popular Android apps, demystifying how smartphone apps exploit deep learning in the wild. To this end, we build a new static tool that dissects apps and analyzes their deep learning functions. Our study answers threefold questions: what are the early adopter apps of deep learning, what do they use deep learning for, and how do their deep learning models look like. Our study has strong implications for app developers, smartphone vendors, and deep learning R\\&D. On one hand, our findings paint a promising picture of deep learning for smartphones, showing the prosperity of mobile deep learning frameworks as well as the prosperity of apps building their cores atop deep learning. On the other hand, our findings urge optimizations on deep learning models deployed on smartphones, the protection of these models, and validation of research ideas on these models.",
    "Link": "http://arxiv.org/abs/1812.05448v4",
    "PDF Link": "http://arxiv.org/pdf/1812.05448v4"
  },
  {
    "Title": "Geometrization of deep networks for the interpretability of deep\n  learning systems",
    "Authors": "Xiao Dong, Ling Zhou",
    "Published": "2019-01-06T14:32:45Z",
    "Summary": "How to understand deep learning systems remains an open problem. In this paper we propose that the answer may lie in the geometrization of deep networks. Geometrization is a bridge to connect physics, geometry, deep network and quantum computation and this may result in a new scheme to reveal the rule of the physical world. By comparing the geometry of image matching and deep networks, we show that geometrization of deep networks can be used to understand existing deep learning systems and it may also help to solve the interpretability problem of deep learning systems.",
    "Link": "http://arxiv.org/abs/1901.02354v2",
    "PDF Link": "http://arxiv.org/pdf/1901.02354v2"
  },
  {
    "Title": "Why & When Deep Learning Works: Looking Inside Deep Learnings",
    "Authors": "Ronny Ronen",
    "Published": "2017-05-10T18:52:26Z",
    "Summary": "The Intel Collaborative Research Institute for Computational Intelligence (ICRI-CI) has been heavily supporting Machine Learning and Deep Learning research from its foundation in 2012. We have asked six leading ICRI-CI Deep Learning researchers to address the challenge of \"Why & When Deep Learning works\", with the goal of looking inside Deep Learning, providing insights on how deep networks function, and uncovering key observations on their expressiveness, limitations, and potential. The output of this challenge resulted in five papers that address different facets of deep learning. These different facets include a high-level understating of why and when deep networks work (and do not work), the impact of geometry on the expressiveness of deep networks, and making deep networks interpretable.",
    "Link": "http://arxiv.org/abs/1705.03921v1",
    "PDF Link": "http://arxiv.org/pdf/1705.03921v1"
  },
  {
    "Title": "Learning Task-aware Robust Deep Learning Systems",
    "Authors": "Keji Han, Yun Li, Xianzhong Long, Yao Ge",
    "Published": "2020-10-11T01:06:49Z",
    "Summary": "Many works demonstrate that deep learning system is vulnerable to adversarial attack. A deep learning system consists of two parts: the deep learning task and the deep model. Nowadays, most existing works investigate the impact of the deep model on robustness of deep learning systems, ignoring the impact of the learning task. In this paper, we adopt the binary and interval label encoding strategy to redefine the classification task and design corresponding loss to improve robustness of the deep learning system. Our method can be viewed as improving the robustness of deep learning systems from both the learning task and deep model. Experimental results demonstrate that our learning task-aware method is much more robust than traditional classification while retaining the accuracy.",
    "Link": "http://arxiv.org/abs/2010.05125v2",
    "PDF Link": "http://arxiv.org/pdf/2010.05125v2"
  },
  {
    "Title": "Deep Learning in Software Engineering",
    "Authors": "Xiaochen Li, He Jiang, Zhilei Ren, Ge Li, Jingxuan Zhang",
    "Published": "2018-05-13T06:01:39Z",
    "Summary": "Recent years, deep learning is increasingly prevalent in the field of Software Engineering (SE). However, many open issues still remain to be investigated. How do researchers integrate deep learning into SE problems? Which SE phases are facilitated by deep learning? Do practitioners benefit from deep learning? The answers help practitioners and researchers develop practical deep learning models for SE tasks. To answer these questions, we conduct a bibliography analysis on 98 research papers in SE that use deep learning techniques. We find that 41 SE tasks in all SE phases have been facilitated by deep learning integrated solutions. In which, 84.7% papers only use standard deep learning models and their variants to solve SE problems. The practicability becomes a concern in utilizing deep learning techniques. How to improve the effectiveness, efficiency, understandability, and testability of deep learning based solutions may attract more SE researchers in the future.",
    "Link": "http://arxiv.org/abs/1805.04825v1",
    "PDF Link": "http://arxiv.org/pdf/1805.04825v1"
  },
  {
    "Title": "Moving Deep Learning into Web Browser: How Far Can We Go?",
    "Authors": "Yun Ma, Dongwei Xiang, Shuyu Zheng, Deyu Tian, Xuanzhe Liu",
    "Published": "2019-01-27T14:54:51Z",
    "Summary": "Recently, several JavaScript-based deep learning frameworks have emerged, making it possible to perform deep learning tasks directly in browsers. However, little is known on what and how well we can do with these frameworks for deep learning in browsers. To bridge the knowledge gap, in this paper, we conduct the first empirical study of deep learning in browsers. We survey 7 most popular JavaScript-based deep learning frameworks, investigating to what extent deep learning tasks have been supported in browsers so far. Then we measure the performance of different frameworks when running different deep learning tasks. Finally, we dig out the performance gap between deep learning in browsers and on native platforms by comparing the performance of TensorFlow.js and TensorFlow in Python. Our findings could help application developers, deep-learning framework vendors and browser vendors to improve the efficiency of deep learning in browsers.",
    "Link": "http://arxiv.org/abs/1901.09388v2",
    "PDF Link": "http://arxiv.org/pdf/1901.09388v2"
  },
  {
    "Title": "Greedy Deep Dictionary Learning",
    "Authors": "Snigdha Tariyal, Angshul Majumdar, Richa Singh, Mayank Vatsa",
    "Published": "2016-01-31T06:12:58Z",
    "Summary": "In this work we propose a new deep learning tool called deep dictionary learning. Multi-level dictionaries are learnt in a greedy fashion, one layer at a time. This requires solving a simple (shallow) dictionary learning problem, the solution to this is well known. We apply the proposed technique on some benchmark deep learning datasets. We compare our results with other deep learning tools like stacked autoencoder and deep belief network; and state of the art supervised dictionary learning tools like discriminative KSVD and label consistent KSVD. Our method yields better results than all.",
    "Link": "http://arxiv.org/abs/1602.00203v1",
    "PDF Link": "http://arxiv.org/pdf/1602.00203v1"
  },
  {
    "Title": "Quantum Neural Networks: Concepts, Applications, and Challenges",
    "Authors": "Yunseok Kwak, Won Joon Yun, Soyi Jung, Joongheon Kim",
    "Published": "2021-08-02T04:32:15Z",
    "Summary": "Quantum deep learning is a research field for the use of quantum computing techniques for training deep neural networks. The research topics and directions of deep learning and quantum computing have been separated for long time, however by discovering that quantum circuits can act like artificial neural networks, quantum deep learning research is widely adopted. This paper explains the backgrounds and basic principles of quantum deep learning and also introduces major achievements. After that, this paper discusses the challenges of quantum deep learning research in multiple perspectives. Lastly, this paper presents various future research directions and application fields of quantum deep learning.",
    "Link": "http://arxiv.org/abs/2108.01468v1",
    "PDF Link": "http://arxiv.org/pdf/2108.01468v1"
  },
  {
    "Title": "NetBooster: Empowering Tiny Deep Learning By Standing on the Shoulders\n  of Deep Giants",
    "Authors": "Zhongzhi Yu, Yonggan Fu, Jiayi Yuan, Haoran You, Yingyan Lin",
    "Published": "2023-06-23T16:14:25Z",
    "Summary": "Tiny deep learning has attracted increasing attention driven by the substantial demand for deploying deep learning on numerous intelligent Internet-of-Things devices. However, it is still challenging to unleash tiny deep learning's full potential on both large-scale datasets and downstream tasks due to the under-fitting issues caused by the limited model capacity of tiny neural networks (TNNs). To this end, we propose a framework called NetBooster to empower tiny deep learning by augmenting the architectures of TNNs via an expansion-then-contraction strategy. Extensive experiments show that NetBooster consistently outperforms state-of-the-art tiny deep learning solutions.",
    "Link": "http://arxiv.org/abs/2306.13586v1",
    "PDF Link": "http://arxiv.org/pdf/2306.13586v1"
  },
  {
    "Title": "Deep Reinforcement Learning in Computer Vision: A Comprehensive Survey",
    "Authors": "Ngan Le, Vidhiwar Singh Rathour, Kashu Yamazaki, Khoa Luu, Marios Savvides",
    "Published": "2021-08-25T23:01:48Z",
    "Summary": "Deep reinforcement learning augments the reinforcement learning framework and utilizes the powerful representation of deep neural networks. Recent works have demonstrated the remarkable successes of deep reinforcement learning in various domains including finance, medicine, healthcare, video games, robotics, and computer vision. In this work, we provide a detailed review of recent and state-of-the-art research advances of deep reinforcement learning in computer vision. We start with comprehending the theories of deep learning, reinforcement learning, and deep reinforcement learning. We then propose a categorization of deep reinforcement learning methodologies and discuss their advantages and limitations. In particular, we divide deep reinforcement learning into seven main categories according to their applications in computer vision, i.e. (i)landmark localization (ii) object detection; (iii) object tracking; (iv) registration on both 2D image and 3D image volumetric data (v) image segmentation; (vi) videos analysis; and (vii) other applications. Each of these categories is further analyzed with reinforcement learning techniques, network design, and performance. Moreover, we provide a comprehensive analysis of the existing publicly available datasets and examine source code availability. Finally, we present some open issues and discuss future research directions on deep reinforcement learning in computer vision",
    "Link": "http://arxiv.org/abs/2108.11510v1",
    "PDF Link": "http://arxiv.org/pdf/2108.11510v1"
  },
  {
    "Title": "Probabilistic Deep Learning with Probabilistic Neural Networks and Deep\n  Probabilistic Models",
    "Authors": "Daniel T. Chang",
    "Published": "2021-05-31T22:13:21Z",
    "Summary": "Probabilistic deep learning is deep learning that accounts for uncertainty, both model uncertainty and data uncertainty. It is based on the use of probabilistic models and deep neural networks. We distinguish two approaches to probabilistic deep learning: probabilistic neural networks and deep probabilistic models. The former employs deep neural networks that utilize probabilistic layers which can represent and process uncertainty; the latter uses probabilistic models that incorporate deep neural network components which capture complex non-linear stochastic relationships between the random variables. We discuss some major examples of each approach including Bayesian neural networks and mixture density networks (for probabilistic neural networks), and variational autoencoders, deep Gaussian processes and deep mixed effects models (for deep probabilistic models). TensorFlow Probability is a library for probabilistic modeling and inference which can be used for both approaches of probabilistic deep learning. We include its code examples for illustration.",
    "Link": "http://arxiv.org/abs/2106.00120v3",
    "PDF Link": "http://arxiv.org/pdf/2106.00120v3"
  },
  {
    "Title": "Towards energy-efficient Deep Learning: An overview of energy-efficient\n  approaches along the Deep Learning Lifecycle",
    "Authors": "Vanessa Mehlin, Sigurd Schacht, Carsten Lanquillon",
    "Published": "2023-02-05T11:36:51Z",
    "Summary": "Deep Learning has enabled many advances in machine learning applications in the last few years. However, since current Deep Learning algorithms require much energy for computations, there are growing concerns about the associated environmental costs. Energy-efficient Deep Learning has received much attention from researchers and has already made much progress in the last couple of years. This paper aims to gather information about these advances from the literature and show how and at which points along the lifecycle of Deep Learning (IT-Infrastructure, Data, Modeling, Training, Deployment, Evaluation) it is possible to reduce energy consumption.",
    "Link": "http://arxiv.org/abs/2303.01980v1",
    "PDF Link": "http://arxiv.org/pdf/2303.01980v1"
  },
  {
    "Title": "A Unified Framework of Deep Neural Networks by Capsules",
    "Authors": "Yujian Li, Chuanhui Shan",
    "Published": "2018-05-09T14:23:17Z",
    "Summary": "With the growth of deep learning, how to describe deep neural networks unifiedly is becoming an important issue. We first formalize neural networks mathematically with their directed graph representations, and prove a generation theorem about the induced networks of connected directed acyclic graphs. Then, we set up a unified framework for deep learning with capsule networks. This capsule framework could simplify the description of existing deep neural networks, and provide a theoretical basis of graphic designing and programming techniques for deep learning models, thus would be of great significance to the advancement of deep learning.",
    "Link": "http://arxiv.org/abs/1805.03551v2",
    "PDF Link": "http://arxiv.org/pdf/1805.03551v2"
  },
  {
    "Title": "Integrating Learning and Reasoning with Deep Logic Models",
    "Authors": "Giuseppe Marra, Francesco Giannini, Michelangelo Diligenti, Marco Gori",
    "Published": "2019-01-14T09:06:28Z",
    "Summary": "Deep learning is very effective at jointly learning feature representations and classification models, especially when dealing with high dimensional input patterns. Probabilistic logic reasoning, on the other hand, is capable to take consistent and robust decisions in complex environments. The integration of deep learning and logic reasoning is still an open-research problem and it is considered to be the key for the development of real intelligent agents. This paper presents Deep Logic Models, which are deep graphical models integrating deep learning and logic reasoning both for learning and inference. Deep Logic Models create an end-to-end differentiable architecture, where deep learners are embedded into a network implementing a continuous relaxation of the logic knowledge. The learning process allows to jointly learn the weights of the deep learners and the meta-parameters controlling the high-level reasoning. The experimental results show that the proposed methodology overtakes the limitations of the other approaches that have been proposed to bridge deep learning and reasoning.",
    "Link": "http://arxiv.org/abs/1901.04195v1",
    "PDF Link": "http://arxiv.org/pdf/1901.04195v1"
  },
  {
    "Title": "Deep Learning in the Field of Biometric Template Protection: An Overview",
    "Authors": "Christian Rathgeb, Jascha Kolberg, Andreas Uhl, Christoph Busch",
    "Published": "2023-03-05T17:06:40Z",
    "Summary": "Today, deep learning represents the most popular and successful form of machine learning. Deep learning has revolutionised the field of pattern recognition, including biometric recognition. Biometric systems utilising deep learning have been shown to achieve auspicious recognition accuracy, surpassing human performance. Apart from said breakthrough advances in terms of biometric performance, the use of deep learning was reported to impact different covariates of biometrics such as algorithmic fairness, vulnerability to attacks, or template protection. Technologies of biometric template protection are designed to enable a secure and privacy-preserving deployment of biometrics. In the recent past, deep learning techniques have been frequently applied in biometric template protection systems for various purposes. This work provides an overview of how advances in deep learning take influence on the field of biometric template protection. The interrelation between improved biometric performance rates and security in biometric template protection is elaborated. Further, the use of deep learning for obtaining feature representations that are suitable for biometric template protection is discussed. Novel methods that apply deep learning to achieve various goals of biometric template protection are surveyed along with deep learning-based attacks.",
    "Link": "http://arxiv.org/abs/2303.02715v1",
    "PDF Link": "http://arxiv.org/pdf/2303.02715v1"
  },
  {
    "Title": "A Survey Analyzing Generalization in Deep Reinforcement Learning",
    "Authors": "Ezgi Korkmaz",
    "Published": "2024-01-04T16:45:01Z",
    "Summary": "Reinforcement learning research obtained significant success and attention with the utilization of deep neural networks to solve problems in high dimensional state or action spaces. While deep reinforcement learning policies are currently being deployed in many different fields from medical applications to large language models, there are still ongoing questions the field is trying to answer on the generalization capabilities of deep reinforcement learning policies. In this paper, we will formalize and analyze generalization in deep reinforcement learning. We will explain the fundamental reasons why deep reinforcement learning policies encounter overfitting problems that limit their generalization capabilities. Furthermore, we will categorize and explain the manifold solution approaches to increase generalization, and overcome overfitting in deep reinforcement learning policies. From exploration to adversarial analysis and from regularization to robustness our paper provides an analysis on a wide range of subfields within deep reinforcement learning with a broad scope and in-depth view. We believe our study can provide a compact guideline for the current advancements in deep reinforcement learning, and help to construct robust deep neural policies with higher generalization skills.",
    "Link": "http://arxiv.org/abs/2401.02349v2",
    "PDF Link": "http://arxiv.org/pdf/2401.02349v2"
  },
  {
    "Title": "What Really is Deep Learning Doing?",
    "Authors": "Chuyu Xiong",
    "Published": "2017-11-06T23:00:13Z",
    "Summary": "Deep learning has achieved a great success in many areas, from computer vision to natural language processing, to game playing, and much more. Yet, what deep learning is really doing is still an open question. There are a lot of works in this direction. For example, [5] tried to explain deep learning by group renormalization, and [6] tried to explain deep learning from the view of functional approximation. In order to address this very crucial question, here we see deep learning from perspective of mechanical learning and learning machine (see [1], [2]). From this particular angle, we can see deep learning much better and answer with confidence: What deep learning is really doing? why it works well, how it works, and how much data is necessary for learning. We also will discuss advantages and disadvantages of deep learning at the end of this work.",
    "Link": "http://arxiv.org/abs/1711.03577v1",
    "PDF Link": "http://arxiv.org/pdf/1711.03577v1"
  },
  {
    "Title": "Transferability in Deep Learning: A Survey",
    "Authors": "Junguang Jiang, Yang Shu, Jianmin Wang, Mingsheng Long",
    "Published": "2022-01-15T15:03:17Z",
    "Summary": "The success of deep learning algorithms generally depends on large-scale data, while humans appear to have inherent ability of knowledge transfer, by recognizing and applying relevant knowledge from previous learning experiences when encountering and solving unseen tasks. Such an ability to acquire and reuse knowledge is known as transferability in deep learning. It has formed the long-term quest towards making deep learning as data-efficient as human learning, and has been motivating fruitful design of more powerful deep learning algorithms. We present this survey to connect different isolated areas in deep learning with their relation to transferability, and to provide a unified and complete view to investigating transferability through the whole lifecycle of deep learning. The survey elaborates the fundamental goals and challenges in parallel with the core principles and methods, covering recent cornerstones in deep architectures, pre-training, task adaptation and domain adaptation. This highlights unanswered questions on the appropriate objectives for learning transferable knowledge and for adapting the knowledge to new tasks and domains, avoiding catastrophic forgetting and negative transfer. Finally, we implement a benchmark and an open-source library, enabling a fair evaluation of deep learning methods in terms of transferability.",
    "Link": "http://arxiv.org/abs/2201.05867v1",
    "PDF Link": "http://arxiv.org/pdf/2201.05867v1"
  },
  {
    "Title": "Feature versus Raw Sequence: Deep Learning Comparative Study on\n  Predicting Pre-miRNA",
    "Authors": "Jaya Thomas, Sonia Thomas, Lee Sael",
    "Published": "2017-10-17T14:09:00Z",
    "Summary": "Should we input known genome sequence features or input sequence itself in deep learning framework? As deep learning more popular in various applications, researchers often come to question whether to generate features or use raw sequences for deep learning. To answer this question, we study the prediction accuracy of precursor miRNA prediction of feature-based deep belief network and sequence-based convolution neural network. Tested on a variant of six-layer convolution neural net and three-layer deep belief network, we find the raw sequence input based convolution neural network model performs similar or slightly better than feature based deep belief networks with best accuracy values of 0.995 and 0.990, respectively. Both the models outperform existing benchmarks models. The results shows us that if provided large enough data, well devised raw sequence based deep learning models can replace feature based deep learning models. However, construction of well behaved deep learning model can be very challenging. In cased features can be easily extracted, feature-based deep learning models may be a better alternative.",
    "Link": "http://arxiv.org/abs/1710.06798v1",
    "PDF Link": "http://arxiv.org/pdf/1710.06798v1"
  },
  {
    "Title": "Distributed Deep Reinforcement Learning: A Survey and A Multi-Player\n  Multi-Agent Learning Toolbox",
    "Authors": "Qiyue Yin, Tongtong Yu, Shengqi Shen, Jun Yang, Meijing Zhao, Kaiqi Huang, Bin Liang, Liang Wang",
    "Published": "2022-12-01T03:39:24Z",
    "Summary": "With the breakthrough of AlphaGo, deep reinforcement learning becomes a recognized technique for solving sequential decision-making problems. Despite its reputation, data inefficiency caused by its trial and error learning mechanism makes deep reinforcement learning hard to be practical in a wide range of areas. Plenty of methods have been developed for sample efficient deep reinforcement learning, such as environment modeling, experience transfer, and distributed modifications, amongst which, distributed deep reinforcement learning has shown its potential in various applications, such as human-computer gaming, and intelligent transportation. In this paper, we conclude the state of this exciting field, by comparing the classical distributed deep reinforcement learning methods, and studying important components to achieve efficient distributed learning, covering single player single agent distributed deep reinforcement learning to the most complex multiple players multiple agents distributed deep reinforcement learning. Furthermore, we review recently released toolboxes that help to realize distributed deep reinforcement learning without many modifications of their non-distributed versions. By analyzing their strengths and weaknesses, a multi-player multi-agent distributed deep reinforcement learning toolbox is developed and released, which is further validated on Wargame, a complex environment, showing usability of the proposed toolbox for multiple players and multiple agents distributed deep reinforcement learning under complex games. Finally, we try to point out challenges and future trends, hoping this brief review can provide a guide or a spark for researchers who are interested in distributed deep reinforcement learning.",
    "Link": "http://arxiv.org/abs/2212.00253v1",
    "PDF Link": "http://arxiv.org/pdf/2212.00253v1"
  },
  {
    "Title": "Are Efficient Deep Representations Learnable?",
    "Authors": "Maxwell Nye, Andrew Saxe",
    "Published": "2018-07-17T13:08:21Z",
    "Summary": "Many theories of deep learning have shown that a deep network can require dramatically fewer resources to represent a given function compared to a shallow network. But a question remains: can these efficient representations be learned using current deep learning techniques? In this work, we test whether standard deep learning methods can in fact find the efficient representations posited by several theories of deep representation. Specifically, we train deep neural networks to learn two simple functions with known efficient solutions: the parity function and the fast Fourier transform. We find that using gradient-based optimization, a deep network does not learn the parity function, unless initialized very close to a hand-coded exact solution. We also find that a deep linear neural network does not learn the fast Fourier transform, even in the best-case scenario of infinite training data, unless the weights are initialized very close to the exact hand-coded solution. Our results suggest that not every element of the class of compositional functions can be learned efficiently by a deep network, and further restrictions are necessary to understand what functions are both efficiently representable and learnable.",
    "Link": "http://arxiv.org/abs/1807.06399v1",
    "PDF Link": "http://arxiv.org/pdf/1807.06399v1"
  },
  {
    "Title": "Deep Learning: A Critical Appraisal",
    "Authors": "Gary Marcus",
    "Published": "2018-01-02T12:49:35Z",
    "Summary": "Although deep learning has historical roots going back decades, neither the term \"deep learning\" nor the approach was popular just over five years ago, when the field was reignited by papers such as Krizhevsky, Sutskever and Hinton's now classic (2012) deep network model of Imagenet. What has the field discovered in the five subsequent years? Against a background of considerable progress in areas such as speech recognition, image recognition, and game playing, and considerable enthusiasm in the popular press, I present ten concerns for deep learning, and suggest that deep learning must be supplemented by other techniques if we are to reach artificial general intelligence.",
    "Link": "http://arxiv.org/abs/1801.00631v1",
    "PDF Link": "http://arxiv.org/pdf/1801.00631v1"
  },
  {
    "Title": "Deep Learning for Sentiment Analysis : A Survey",
    "Authors": "Lei Zhang, Shuai Wang, Bing Liu",
    "Published": "2018-01-24T07:32:29Z",
    "Summary": "Deep learning has emerged as a powerful machine learning technique that learns multiple layers of representations or features of the data and produces state-of-the-art prediction results. Along with the success of deep learning in many other application domains, deep learning is also popularly used in sentiment analysis in recent years. This paper first gives an overview of deep learning and then provides a comprehensive survey of its current applications in sentiment analysis.",
    "Link": "http://arxiv.org/abs/1801.07883v2",
    "PDF Link": "http://arxiv.org/pdf/1801.07883v2"
  },
  {
    "Title": "Deep Learning for Visual Navigation of Underwater Robots",
    "Authors": "M. Sunbeam",
    "Published": "2023-10-30T12:37:49Z",
    "Summary": "This paper aims to briefly survey deep learning methods for visual navigation of underwater robotics. The scope of this paper includes the visual perception of underwater robotics with deep learning methods, the available visual underwater datasets, imitation learning, and reinforcement learning methods for navigation. Additionally, relevant works will be categorized under the imitation learning or deep learning paradigm for underwater robots for clarity of the training methodologies in the current landscape. Literature that uses deep learning algorithms to process non-visual data for underwater navigation will not be considered, except as contrasting examples.",
    "Link": "http://arxiv.org/abs/2310.19495v1",
    "PDF Link": "http://arxiv.org/pdf/2310.19495v1"
  },
  {
    "Title": "When deep learning meets security",
    "Authors": "Majd Latah",
    "Published": "2018-07-12T17:44:42Z",
    "Summary": "Deep learning is an emerging research field that has proven its effectiveness towards deploying more efficient intelligent systems. Security, on the other hand, is one of the most essential issues in modern communication systems. Recently many papers have shown that using deep learning models can achieve promising results when applied to the security domain. In this work, we provide an overview for the recent studies that apply deep learning techniques to the field of security.",
    "Link": "http://arxiv.org/abs/1807.04739v1",
    "PDF Link": "http://arxiv.org/pdf/1807.04739v1"
  },
  {
    "Title": "Deep Causal Learning for Robotic Intelligence",
    "Authors": "Yangming Li",
    "Published": "2022-12-23T21:44:31Z",
    "Summary": "This invited review discusses causal learning in the context of robotic intelligence. The paper introduced the psychological findings on causal learning in human cognition, then it introduced the traditional statistical solutions on causal discovery and causal inference. The paper reviewed recent deep causal learning algorithms with a focus on their architectures and the benefits of using deep nets and discussed the gap between deep causal learning and the needs of robotic intelligence.",
    "Link": "http://arxiv.org/abs/2212.12597v1",
    "PDF Link": "http://arxiv.org/pdf/2212.12597v1"
  },
  {
    "Title": "Deep learning in radiology: an overview of the concepts and a survey of\n  the state of the art",
    "Authors": "Maciej A. Mazurowski, Mateusz Buda, Ashirbani Saha, Mustafa R. Bashir",
    "Published": "2018-02-10T04:00:55Z",
    "Summary": "Deep learning is a branch of artificial intelligence where networks of simple interconnected units are used to extract patterns from data in order to solve complex problems. Deep learning algorithms have shown groundbreaking performance in a variety of sophisticated tasks, especially those related to images. They have often matched or exceeded human performance. Since the medical field of radiology mostly relies on extracting useful information from images, it is a very natural application area for deep learning, and research in this area has rapidly grown in recent years. In this article, we review the clinical reality of radiology and discuss the opportunities for application of deep learning algorithms. We also introduce basic concepts of deep learning including convolutional neural networks. Then, we present a survey of the research in deep learning applied to radiology. We organize the studies by the types of specific tasks that they attempt to solve and review the broad range of utilized deep learning algorithms. Finally, we briefly discuss opportunities and challenges for incorporating deep learning in the radiology practice of the future.",
    "Link": "http://arxiv.org/abs/1802.08717v1",
    "PDF Link": "http://arxiv.org/pdf/1802.08717v1"
  },
  {
    "Title": "A Survey on Deep Learning Methods for Robot Vision",
    "Authors": "Javier Ruiz-del-Solar, Patricio Loncomilla, Naiomi Soto",
    "Published": "2018-03-28T21:37:14Z",
    "Summary": "Deep learning has allowed a paradigm shift in pattern recognition, from using hand-crafted features together with statistical classifiers to using general-purpose learning procedures for learning data-driven representations, features, and classifiers together. The application of this new paradigm has been particularly successful in computer vision, in which the development of deep learning methods for vision applications has become a hot research topic. Given that deep learning has already attracted the attention of the robot vision community, the main purpose of this survey is to address the use of deep learning in robot vision. To achieve this, a comprehensive overview of deep learning and its usage in computer vision is given, that includes a description of the most frequently used neural models and their main application areas. Then, the standard methodology and tools used for designing deep-learning based vision systems are presented. Afterwards, a review of the principal work using deep learning in robot vision is presented, as well as current and future trends related to the use of deep learning in robotics. This survey is intended to be a guide for the developers of robot vision systems.",
    "Link": "http://arxiv.org/abs/1803.10862v1",
    "PDF Link": "http://arxiv.org/pdf/1803.10862v1"
  },
  {
    "Title": "Interpretations of Deep Learning by Forests and Haar Wavelets",
    "Authors": "Changcun Huang",
    "Published": "2019-06-16T14:38:41Z",
    "Summary": "This paper presents a basic property of region dividing of ReLU (rectified linear unit) deep learning when new layers are successively added, by which two new perspectives of interpreting deep learning are given. The first is related to decision trees and forests; we construct a deep learning structure equivalent to a forest in classification abilities, which means that certain kinds of ReLU deep learning can be considered as forests. The second perspective is that Haar wavelet represented functions can be approximated by ReLU deep learning with arbitrary precision; and then a general conclusion of function approximation abilities of ReLU deep learning is given. Finally, generalize some of the conclusions of ReLU deep learning to the case of sigmoid-unit deep learning.",
    "Link": "http://arxiv.org/abs/1906.06706v7",
    "PDF Link": "http://arxiv.org/pdf/1906.06706v7"
  },
  {
    "Title": "A Selective Overview of Deep Learning",
    "Authors": "Jianqing Fan, Cong Ma, Yiqiao Zhong",
    "Published": "2019-04-10T17:53:15Z",
    "Summary": "Deep learning has arguably achieved tremendous success in recent years. In simple words, deep learning uses the composition of many nonlinear functions to model the complex dependency between input features and labels. While neural networks have a long history, recent advances have greatly improved their performance in computer vision, natural language processing, etc. From the statistical and scientific perspective, it is natural to ask: What is deep learning? What are the new characteristics of deep learning, compared with classical methods? What are the theoretical foundations of deep learning? To answer these questions, we introduce common neural network models (e.g., convolutional neural nets, recurrent neural nets, generative adversarial nets) and training techniques (e.g., stochastic gradient descent, dropout, batch normalization) from a statistical point of view. Along the way, we highlight new characteristics of deep learning (including depth and over-parametrization) and explain their practical and theoretical benefits. We also sample recent results on theories of deep learning, many of which are only suggestive. While a complete understanding of deep learning remains elusive, we hope that our perspectives and discussions serve as a stimulus for new statistical research.",
    "Link": "http://arxiv.org/abs/1904.05526v2",
    "PDF Link": "http://arxiv.org/pdf/1904.05526v2"
  },
  {
    "Title": "A Brief Survey of Deep Reinforcement Learning",
    "Authors": "Kai Arulkumaran, Marc Peter Deisenroth, Miles Brundage, Anil Anthony Bharath",
    "Published": "2017-08-19T15:55:31Z",
    "Summary": "Deep reinforcement learning is poised to revolutionise the field of AI and represents a step towards building autonomous systems with a higher level understanding of the visual world. Currently, deep learning is enabling reinforcement learning to scale to problems that were previously intractable, such as learning to play video games directly from pixels. Deep reinforcement learning algorithms are also applied to robotics, allowing control policies for robots to be learned directly from camera inputs in the real world. In this survey, we begin with an introduction to the general field of reinforcement learning, then progress to the main streams of value-based and policy-based methods. Our survey will cover central algorithms in deep reinforcement learning, including the deep $Q$-network, trust region policy optimisation, and asynchronous advantage actor-critic. In parallel, we highlight the unique advantages of deep neural networks, focusing on visual understanding via reinforcement learning. To conclude, we describe several current areas of research within the field.",
    "Link": "http://arxiv.org/abs/1708.05866v2",
    "PDF Link": "http://arxiv.org/pdf/1708.05866v2"
  },
  {
    "Title": "Topological Deep Learning: A Review of an Emerging Paradigm",
    "Authors": "Ali Zia, Abdelwahed Khamis, James Nichols, Zeeshan Hayder, Vivien Rolland, Lars Petersson",
    "Published": "2023-02-08T02:11:24Z",
    "Summary": "Topological data analysis (TDA) provides insight into data shape. The summaries obtained by these methods are principled global descriptions of multi-dimensional data whilst exhibiting stable properties such as robustness to deformation and noise. Such properties are desirable in deep learning pipelines but they are typically obtained using non-TDA strategies. This is partly caused by the difficulty of combining TDA constructs (e.g. barcode and persistence diagrams) with current deep learning algorithms. Fortunately, we are now witnessing a growth of deep learning applications embracing topologically-guided components. In this survey, we review the nascent field of topological deep learning by first revisiting the core concepts of TDA. We then explore how the use of TDA techniques has evolved over time to support deep learning frameworks, and how they can be integrated into different aspects of deep learning. Furthermore, we touch on TDA usage for analyzing existing deep models; deep topological analytics. Finally, we discuss the challenges and future prospects of topological deep learning.",
    "Link": "http://arxiv.org/abs/2302.03836v1",
    "PDF Link": "http://arxiv.org/pdf/2302.03836v1"
  },
  {
    "Title": "Generalization and Expressivity for Deep Nets",
    "Authors": "Shao-Bo Lin",
    "Published": "2018-03-10T07:41:25Z",
    "Summary": "Along with the rapid development of deep learning in practice, the theoretical explanations for its success become urgent. Generalization and expressivity are two widely used measurements to quantify theoretical behaviors of deep learning. The expressivity focuses on finding functions expressible by deep nets but cannot be approximated by shallow nets with the similar number of neurons. It usually implies the large capacity. The generalization aims at deriving fast learning rate for deep nets. It usually requires small capacity to reduce the variance. Different from previous studies on deep learning, pursuing either expressivity or generalization, we take both factors into account to explore the theoretical advantages of deep nets. For this purpose, we construct a deep net with two hidden layers possessing excellent expressivity in terms of localized and sparse approximation. Then, utilizing the well known covering number to measure the capacity, we find that deep nets possess excellent expressive power (measured by localized and sparse approximation) without enlarging the capacity of shallow nets. As a consequence, we derive near optimal learning rates for implementing empirical risk minimization (ERM) on the constructed deep nets. These results theoretically exhibit the advantage of deep nets from learning theory viewpoints.",
    "Link": "http://arxiv.org/abs/1803.03772v2",
    "PDF Link": "http://arxiv.org/pdf/1803.03772v2"
  },
  {
    "Title": "Deep Incremental Boosting",
    "Authors": "Alan Mosca, George D Magoulas",
    "Published": "2017-08-11T21:05:58Z",
    "Summary": "This paper introduces Deep Incremental Boosting, a new technique derived from AdaBoost, specifically adapted to work with Deep Learning methods, that reduces the required training time and improves generalisation. We draw inspiration from Transfer of Learning approaches to reduce the start-up time to training each incremental Ensemble member. We show a set of experiments that outlines some preliminary results on some common Deep Learning datasets and discuss the potential improvements Deep Incremental Boosting brings to traditional Ensemble methods in Deep Learning.",
    "Link": "http://arxiv.org/abs/1708.03704v1",
    "PDF Link": "http://arxiv.org/pdf/1708.03704v1"
  },
  {
    "Title": "Combining Deep Learning with Good Old-Fashioned Machine Learning",
    "Authors": "Moshe Sipper",
    "Published": "2022-07-08T08:58:43Z",
    "Summary": "We present a comprehensive, stacking-based framework for combining deep learning with good old-fashioned machine learning, called Deep GOld. Our framework involves ensemble selection from 51 retrained pretrained deep networks as first-level models, and 10 machine-learning algorithms as second-level models. Enabled by today's state-of-the-art software tools and hardware platforms, Deep GOld delivers consistent improvement when tested on four image-classification datasets: Fashion MNIST, CIFAR10, CIFAR100, and Tiny ImageNet. Of 120 experiments, in all but 10 Deep GOld improved the original networks' performance.",
    "Link": "http://arxiv.org/abs/2207.03757v2",
    "PDF Link": "http://arxiv.org/pdf/2207.03757v2"
  },
  {
    "Title": "Deep frequency principle towards understanding why deeper learning is\n  faster",
    "Authors": "Zhi-Qin John Xu, Hanxu Zhou",
    "Published": "2020-07-28T15:35:49Z",
    "Summary": "Understanding the effect of depth in deep learning is a critical problem. In this work, we utilize the Fourier analysis to empirically provide a promising mechanism to understand why feedforward deeper learning is faster. To this end, we separate a deep neural network, trained by normal stochastic gradient descent, into two parts during analysis, i.e., a pre-condition component and a learning component, in which the output of the pre-condition one is the input of the learning one. We use a filtering method to characterize the frequency distribution of a high-dimensional function. Based on experiments of deep networks and real dataset, we propose a deep frequency principle, that is, the effective target function for a deeper hidden layer biases towards lower frequency during the training. Therefore, the learning component effectively learns a lower frequency function if the pre-condition component has more layers. Due to the well-studied frequency principle, i.e., deep neural networks learn lower frequency functions faster, the deep frequency principle provides a reasonable explanation to why deeper learning is faster. We believe these empirical studies would be valuable for future theoretical studies of the effect of depth in deep learning.",
    "Link": "http://arxiv.org/abs/2007.14313v2",
    "PDF Link": "http://arxiv.org/pdf/2007.14313v2"
  },
  {
    "Title": "Deep Bayesian Active Learning with Image Data",
    "Authors": "Yarin Gal, Riashat Islam, Zoubin Ghahramani",
    "Published": "2017-03-08T16:53:57Z",
    "Summary": "Even though active learning forms an important pillar of machine learning, deep learning tools are not prevalent within it. Deep learning poses several difficulties when used in an active learning setting. First, active learning (AL) methods generally rely on being able to learn and update models from small amounts of data. Recent advances in deep learning, on the other hand, are notorious for their dependence on large amounts of data. Second, many AL acquisition functions rely on model uncertainty, yet deep learning methods rarely represent such model uncertainty. In this paper we combine recent advances in Bayesian deep learning into the active learning framework in a practical way. We develop an active learning framework for high dimensional data, a task which has been extremely challenging so far, with very sparse existing literature. Taking advantage of specialised models such as Bayesian convolutional neural networks, we demonstrate our active learning techniques with image data, obtaining a significant improvement on existing active learning approaches. We demonstrate this on both the MNIST dataset, as well as for skin cancer diagnosis from lesion images (ISIC2016 task).",
    "Link": "http://arxiv.org/abs/1703.02910v1",
    "PDF Link": "http://arxiv.org/pdf/1703.02910v1"
  },
  {
    "Title": "Deep reinforcement learning for optical systems: A case study of\n  mode-locked lasers",
    "Authors": "Chang Sun, Eurika Kaiser, Steven L. Brunton, J. Nathan Kutz",
    "Published": "2020-06-10T00:30:36Z",
    "Summary": "We demonstrate that deep reinforcement learning (deep RL) provides a highly effective strategy for the control and self-tuning of optical systems. Deep RL integrates the two leading machine learning architectures of deep neural networks and reinforcement learning to produce robust and stable learning for control. Deep RL is ideally suited for optical systems as the tuning and control relies on interactions with its environment with a goal-oriented objective to achieve optimal immediate or delayed rewards. This allows the optical system to recognize bi-stable structures and navigate, via trajectory planning, to optimally performing solutions, the first such algorithm demonstrated to do so in optical systems. We specifically demonstrate the deep RL architecture on a mode-locked laser, where robust self-tuning and control can be established through access of the deep RL agent to its waveplates and polarizers. We further integrate transfer learning to help the deep RL agent rapidly learn new parameter regimes and generalize its control authority. Additionally, the deep RL learning can be easily integrated with other control paradigms to provide a broad framework to control any optical system.",
    "Link": "http://arxiv.org/abs/2006.05579v1",
    "PDF Link": "http://arxiv.org/pdf/2006.05579v1"
  },
  {
    "Title": "Error Bounds for a Matrix-Vector Product Approximation with Deep ReLU\n  Neural Networks",
    "Authors": "Tilahun M. Getu",
    "Published": "2021-11-25T08:14:55Z",
    "Summary": "Among the several paradigms of artificial intelligence (AI) or machine learning (ML), a remarkably successful paradigm is deep learning. Deep learning's phenomenal success has been hoped to be interpreted via fundamental research on the theory of deep learning. Accordingly, applied research on deep learning has spurred the theory of deep learning-oriented depth and breadth of developments. Inspired by such developments, we pose these fundamental questions: can we accurately approximate an arbitrary matrix-vector product using deep rectified linear unit (ReLU) feedforward neural networks (FNNs)? If so, can we bound the resulting approximation error? In light of these questions, we derive error bounds in Lebesgue and Sobolev norms that comprise our developed deep approximation theory. Guided by this theory, we have successfully trained deep ReLU FNNs whose test results justify our developed theory. The developed theory is also applicable for guiding and easing the training of teacher deep ReLU FNNs in view of the emerging teacher-student AI or ML paradigms that are essential for solving several AI or ML problems in wireless communications and signal processing; network science and graph signal processing; and network neuroscience and brain physics.",
    "Link": "http://arxiv.org/abs/2111.12963v1",
    "PDF Link": "http://arxiv.org/pdf/2111.12963v1"
  },
  {
    "Title": "Joint Training of Deep Boltzmann Machines",
    "Authors": "Ian Goodfellow, Aaron Courville, Yoshua Bengio",
    "Published": "2012-12-12T01:59:27Z",
    "Summary": "We introduce a new method for training deep Boltzmann machines jointly. Prior methods require an initial learning pass that trains the deep Boltzmann machine greedily, one layer at a time, or do not perform well on classifi- cation tasks.",
    "Link": "http://arxiv.org/abs/1212.2686v1",
    "PDF Link": "http://arxiv.org/pdf/1212.2686v1"
  },
  {
    "Title": "Introduction to deep learning",
    "Authors": "Lihi Shiloh-Perl, Raja Giryes",
    "Published": "2020-02-29T14:52:28Z",
    "Summary": "Deep Learning (DL) has made a major impact on data science in the last decade. This chapter introduces the basic concepts of this field. It includes both the basic structures used to design deep neural networks and a brief survey of some of its popular use cases.",
    "Link": "http://arxiv.org/abs/2003.03253v1",
    "PDF Link": "http://arxiv.org/pdf/2003.03253v1"
  },
  {
    "Title": "Deep Learning: From Basics to Building Deep Neural Networks with Python",
    "Authors": "Milad Vazan",
    "Published": "2022-04-22T11:57:19Z",
    "Summary": "This book is intended for beginners who have no familiarity with deep learning. Our only expectation from readers is that they already have the basic programming skills in Python.",
    "Link": "http://arxiv.org/abs/2205.01069v1",
    "PDF Link": "http://arxiv.org/pdf/2205.01069v1"
  },
  {
    "Title": "A Nesterov's Accelerated quasi-Newton method for Global Routing using\n  Deep Reinforcement Learning",
    "Authors": "S. Indrapriyadarsini, Shahrzad Mahboubi, Hiroshi Ninomiya, Takeshi Kamio, Hideki Asai",
    "Published": "2020-10-15T07:30:17Z",
    "Summary": "Deep Q-learning method is one of the most popularly used deep reinforcement learning algorithms which uses deep neural networks to approximate the estimation of the action-value function. Training of the deep Q-network (DQN) is usually restricted to first order gradient based methods. This paper attempts to accelerate the training of deep Q-networks by introducing a second order Nesterov's accelerated quasi-Newton method. We evaluate the performance of the proposed method on deep reinforcement learning using double DQNs for global routing. The results show that the proposed method can obtain better routing solutions compared to the DQNs trained with first order Adam and RMSprop methods.",
    "Link": "http://arxiv.org/abs/2010.09465v1",
    "PDF Link": "http://arxiv.org/pdf/2010.09465v1"
  },
  {
    "Title": "Augmented Q Imitation Learning (AQIL)",
    "Authors": "Xiao Lei Zhang, Anish Agarwal",
    "Published": "2020-03-31T18:08:23Z",
    "Summary": "The study of unsupervised learning can be generally divided into two categories: imitation learning and reinforcement learning. In imitation learning the machine learns by mimicking the behavior of an expert system whereas in reinforcement learning the machine learns via direct environment feedback. Traditional deep reinforcement learning takes a significant time before the machine starts to converge to an optimal policy. This paper proposes Augmented Q-Imitation-Learning, a method by which deep reinforcement learning convergence can be accelerated by applying Q-imitation-learning as the initial training process in traditional Deep Q-learning.",
    "Link": "http://arxiv.org/abs/2004.00993v2",
    "PDF Link": "http://arxiv.org/pdf/2004.00993v2"
  },
  {
    "Title": "Deep Reinforcement Learning for Conversational AI",
    "Authors": "Mahipal Jadeja, Neelanshi Varia, Agam Shah",
    "Published": "2017-09-15T06:18:33Z",
    "Summary": "Deep reinforcement learning is revolutionizing the artificial intelligence field. Currently, it serves as a good starting point for constructing intelligent autonomous systems which offer a better knowledge of the visual world. It is possible to scale deep reinforcement learning with the use of deep learning and do amazing tasks such as use of pixels in playing video games. In this paper, key concepts of deep reinforcement learning including reward function, differences between reinforcement learning and supervised learning and models for implementation of reinforcement are discussed. Key challenges related to the implementation of reinforcement learning in conversational AI domain are identified as well as discussed in detail. Various conversational models which are based on deep reinforcement learning (as well as deep learning) are also discussed. In summary, this paper discusses key aspects of deep reinforcement learning which are crucial for designing an efficient conversational AI.",
    "Link": "http://arxiv.org/abs/1709.05067v1",
    "PDF Link": "http://arxiv.org/pdf/1709.05067v1"
  },
  {
    "Title": "An Overview of Deep Semi-Supervised Learning",
    "Authors": "Yassine Ouali, Céline Hudelot, Myriam Tami",
    "Published": "2020-06-09T14:08:03Z",
    "Summary": "Deep neural networks demonstrated their ability to provide remarkable performances on a wide range of supervised learning tasks (e.g., image classification) when trained on extensive collections of labeled data (e.g., ImageNet). However, creating such large datasets requires a considerable amount of resources, time, and effort. Such resources may not be available in many practical cases, limiting the adoption and the application of many deep learning methods. In a search for more data-efficient deep learning methods to overcome the need for large annotated datasets, there is a rising research interest in semi-supervised learning and its applications to deep neural networks to reduce the amount of labeled data required, by either developing novel methods or adopting existing semi-supervised learning frameworks for a deep learning setting. In this paper, we provide a comprehensive overview of deep semi-supervised learning, starting with an introduction to the field, followed by a summarization of the dominant semi-supervised approaches in deep learning.",
    "Link": "http://arxiv.org/abs/2006.05278v2",
    "PDF Link": "http://arxiv.org/pdf/2006.05278v2"
  },
  {
    "Title": "Accelerating Deep Learning with Shrinkage and Recall",
    "Authors": "Shuai Zheng, Abhinav Vishnu, Chris Ding",
    "Published": "2016-05-04T18:17:37Z",
    "Summary": "Deep Learning is a very powerful machine learning model. Deep Learning trains a large number of parameters for multiple layers and is very slow when data is in large scale and the architecture size is large. Inspired from the shrinking technique used in accelerating computation of Support Vector Machines (SVM) algorithm and screening technique used in LASSO, we propose a shrinking Deep Learning with recall (sDLr) approach to speed up deep learning computation. We experiment shrinking Deep Learning with recall (sDLr) using Deep Neural Network (DNN), Deep Belief Network (DBN) and Convolution Neural Network (CNN) on 4 data sets. Results show that the speedup using shrinking Deep Learning with recall (sDLr) can reach more than 2.0 while still giving competitive classification performance.",
    "Link": "http://arxiv.org/abs/1605.01369v2",
    "PDF Link": "http://arxiv.org/pdf/1605.01369v2"
  },
  {
    "Title": "A framework for understanding data science",
    "Authors": "Michael L Brodie",
    "Published": "2024-02-14T15:55:40Z",
    "Summary": "The objective of this research is to provide a framework with which the data science community can understand, define, and develop data science as a field of inquiry. The framework is based on the classical reference framework (axiology, ontology, epistemology, methodology) used for 200 years to define knowledge discovery paradigms and disciplines in the humanities, sciences, algorithms, and now data science. I augmented it for automated problem-solving with (methods, technology, community). The resulting data science reference framework is used to define the data science knowledge discovery paradigm in terms of the philosophy of data science addressed in previous papers and the data science problem-solving paradigm, i.e., the data science method, and the data science problem-solving workflow, both addressed in this paper. The framework is a much called for unifying framework for data science as it contains the components required to define data science. For insights to better understand data science, this paper uses the framework to define the emerging, often enigmatic, data science problem-solving paradigm and workflow, and to compare them with their well-understood scientific counterparts, scientific problem-solving paradigm and workflow.",
    "Link": "http://arxiv.org/abs/2403.00776v1",
    "PDF Link": "http://arxiv.org/pdf/2403.00776v1"
  },
  {
    "Title": "Defining Data Science",
    "Authors": "Yangyong Zhu, Yun Xiong",
    "Published": "2015-01-21T02:41:55Z",
    "Summary": "Data science is gaining more and more and widespread attention, but no consensus viewpoint on what data science is has emerged. As a new science, its objects of study and scientific issues should not be covered by established sciences. Data in cyberspace have formed what we call datanature. In the present paper, data science is defined as the science of exploring datanature.",
    "Link": "http://arxiv.org/abs/1501.05039v1",
    "PDF Link": "http://arxiv.org/pdf/1501.05039v1"
  },
  {
    "Title": "Data Science in Perspective",
    "Authors": "Rogerio Rossi",
    "Published": "2022-01-15T13:51:12Z",
    "Summary": "Data and Science has stood out in the generation of results, whether in the projects of the scientific domain or business domain. CERN Project, Scientific Institutes, companies like Walmart, Google, Apple, among others, need data to present their results and make predictions in the competitive data world. Data and Science are words that together culminated in a globally recognized term called Data Science. Data Science is in its initial phase, possibly being part of formal sciences and also being presented as part of applied sciences, capable of generating value and supporting decision making. Data Science considers science and, consequently, the scientific method to promote decision making through data intelligence. In many cases, the application of the method (or part of it) is considered in Data Science projects in scientific domain (social sciences, bioinformatics, geospatial projects) or business domain (finance, logistic, retail), among others. In this sense, this article addresses the perspectives of Data Science as a multidisciplinary area, considering science and the scientific method, and its formal structure which integrate Statistics, Computer Science, and Business Science, also taking into account Artificial Intelligence, emphasizing Machine Learning, among others. The article also deals with the perspective of applied Data Science, since Data Science is used for generating value through scientific and business projects. Data Science persona is also discussed in the article, concerning the education of Data Science professionals and its corresponding profiles, since its projection changes the field of data in the world.",
    "Link": "http://arxiv.org/abs/2201.05852v1",
    "PDF Link": "http://arxiv.org/pdf/2201.05852v1"
  },
  {
    "Title": "Data Science: A Comprehensive Overview",
    "Authors": "Longbing Cao",
    "Published": "2020-07-01T02:33:58Z",
    "Summary": "The twenty-first century has ushered in the age of big data and data economy, in which data DNA, which carries important knowledge, insights and potential, has become an intrinsic constituent of all data-based organisms. An appropriate understanding of data DNA and its organisms relies on the new field of data science and its keystone, analytics. Although it is widely debated whether big data is only hype and buzz, and data science is still in a very early phase, significant challenges and opportunities are emerging or have been inspired by the research, innovation, business, profession, and education of data science. This paper provides a comprehensive survey and tutorial of the fundamental aspects of data science: the evolution from data analysis to data science, the data science concepts, a big picture of the era of data science, the major challenges and directions in data innovation, the nature of data analytics, new industrialization and service opportunities in the data economy, the profession and competency of data education, and the future of data science. This article is the first in the field to draw a comprehensive big picture, in addition to offering rich observations, lessons and thinking about data science and analytics.",
    "Link": "http://arxiv.org/abs/2007.03606v1",
    "PDF Link": "http://arxiv.org/pdf/2007.03606v1"
  },
  {
    "Title": "Ten Research Challenge Areas in Data Science",
    "Authors": "Jeannette M. Wing",
    "Published": "2020-01-27T21:39:57Z",
    "Summary": "Although data science builds on knowledge from computer science, mathematics, statistics, and other disciplines, data science is a unique field with many mysteries to unlock: challenging scientific questions and pressing questions of societal importance. This article starts with meta-questions about data science as a discipline and then elaborates on ten ideas for the basis of a research agenda for data science.",
    "Link": "http://arxiv.org/abs/2002.05658v1",
    "PDF Link": "http://arxiv.org/pdf/2002.05658v1"
  },
  {
    "Title": "The Art and Practice of Data Science Pipelines: A Comprehensive Study of\n  Data Science Pipelines In Theory, In-The-Small, and In-The-Large",
    "Authors": "Sumon Biswas, Mohammad Wardat, Hridesh Rajan",
    "Published": "2021-12-02T20:16:03Z",
    "Summary": "Increasingly larger number of software systems today are including data science components for descriptive, predictive, and prescriptive analytics. The collection of data science stages from acquisition, to cleaning/curation, to modeling, and so on are referred to as data science pipelines. To facilitate research and practice on data science pipelines, it is essential to understand their nature. What are the typical stages of a data science pipeline? How are they connected? Do the pipelines differ in the theoretical representations and that in the practice? Today we do not fully understand these architectural characteristics of data science pipelines. In this work, we present a three-pronged comprehensive study to answer this for the state-of-the-art, data science in-the-small, and data science in-the-large. Our study analyzes three datasets: a collection of 71 proposals for data science pipelines and related concepts in theory, a collection of over 105 implementations of curated data science pipelines from Kaggle competitions to understand data science in-the-small, and a collection of 21 mature data science projects from GitHub to understand data science in-the-large. Our study has led to three representations of data science pipelines that capture the essence of our subjects in theory, in-the-small, and in-the-large.",
    "Link": "http://arxiv.org/abs/2112.01590v3",
    "PDF Link": "http://arxiv.org/pdf/2112.01590v3"
  },
  {
    "Title": "A Systematic Literature Review of Undergraduate Data Science Education\n  Research",
    "Authors": "Mine Dogucu, Sinem Demirci, Harry Bendekgey, Federica Zoe Ricci, Catalina M. Medina",
    "Published": "2024-03-06T00:49:08Z",
    "Summary": "The presence of data science has been profound in the scientific community in almost every discipline. An important part of the data science education expansion has been at the undergraduate level. We conducted a systematic literature review to (1) portray current evidence and knowledge gaps in self-proclaimed undergraduate data science education research and (2) inform policymakers and the data science education community about what educators may encounter when searching for literature using the general keyword 'data science education.' While open-access publications that target a broader audience of data science educators and include multiple examples of data science programs and courses are a strength, significant knowledge gaps remain. The undergraduate data science literature that we identified often lacks empirical data, research questions and reproducibility. Certain disciplines are less visible. We recommend that we should (1) cherish data science as an interdisciplinary field; (2) adopt a consistent set of keywords/terminology to ensure data science education literature is easily identifiable; (3) prioritize investments in empirical studies.",
    "Link": "http://arxiv.org/abs/2403.03387v2",
    "PDF Link": "http://arxiv.org/pdf/2403.03387v2"
  },
  {
    "Title": "Proceedings of the 3rd Italian Conference on Big Data and Data Science\n  (ITADATA2024)",
    "Authors": "Nicola Bena, Claudia Diamantini, Michela Natilli, Luigi Romano, Giovanni Stilo, Valentina Pansanella, Claudio A. Ardagna, Anna Monreale, Roberto Trasarti",
    "Published": "2025-03-19T06:48:18Z",
    "Summary": "Proceedings of the 3rd Italian Conference on Big Data and Data Science (ITADATA2024), held in Pisa, Italy, September 17-19, 2024.   The Italian Conference on Big Data and Data Science (ITADATA2024) is the annual event supported by the CINI Big Data National Laboratory and ISTI CNR that aims to put together Italian researchers and professionals from academia, industry, government, and public administration working in the field of big data and data science, as well as related fields (e.g., security and privacy, HPC, Cloud).   ITADATA2024 covered research on all theoretical and practical aspects of Big Data and data science including data governance, data processing, data analysis, data reporting, data protection, as well as experimental studies and lessons learned. In particular, ITADATA2024 focused on   - Data spaces   - Data processing life cycle   - Machine learning and Large Language Models   - Applications of big data and data science in healthcare, finance, industry 5.0, and beyond   - Data science for social network analysis",
    "Link": "http://arxiv.org/abs/2503.14937v1",
    "PDF Link": "http://arxiv.org/pdf/2503.14937v1"
  },
  {
    "Title": "Embracing Data Science",
    "Authors": "Adam Loy",
    "Published": "2016-07-04T12:40:15Z",
    "Summary": "Statistics is running the risk of appearing irrelevant to today's undergraduate students. Today's undergraduate students are familiar with data science projects and they judge statistics against what they have seen. Statistics, especially at the introductory level, should take inspiration from data science so that the discipline is not seen as somehow lesser than data science. This article provides a brief overview of data science, outlines ideas for how introductory courses could take inspiration from data science, and provides a reference to materials for developing stand-alone data science courses.",
    "Link": "http://arxiv.org/abs/1607.00858v1",
    "PDF Link": "http://arxiv.org/pdf/1607.00858v1"
  },
  {
    "Title": "Defining data science: a new field of inquiry",
    "Authors": "Michael L Brodie",
    "Published": "2023-06-28T12:58:42Z",
    "Summary": "Data science is not a science. It is a research paradigm. Its power, scope, and scale will surpass science, our most powerful research paradigm, to enable knowledge discovery and change our world. We have yet to understand and define it, vital to realizing its potential and managing its risks. Modern data science is in its infancy. Emerging slowly since 1962 and rapidly since 2000, it is a fundamentally new field of inquiry, one of the most active, powerful, and rapidly evolving 21st century innovations. Due to its value, power, and applicability, it is emerging in over 40 disciplines, hundreds of research areas, and thousands of applications. Millions of data science publications contain myriad definitions of data science and data science problem solving. Due to its infancy, many definitions are independent, application specific, mutually incomplete, redundant, or inconsistent, hence so is data science. This research addresses this data science multiple definitions challenge by proposing the development of coherent, unified definition based on a data science reference framework using a data science journal for the data science community to achieve such a definition. This paper provides candidate definitions for essential data science artifacts that are required to discuss such a definition. They are based on the classical research paradigm concept consisting of a philosophy of data science, the data science problem solving paradigm, and the six component data science reference framework (axiology, ontology, epistemology, methodology, methods, technology) that is a frequently called for unifying framework with which to define, unify, and evolve data science. It presents challenges for defining data science, solution approaches, i.e., means for defining data science, and their requirements and benefits as the basis of a comprehensive solution.",
    "Link": "http://arxiv.org/abs/2306.16177v3",
    "PDF Link": "http://arxiv.org/pdf/2306.16177v3"
  },
  {
    "Title": "Why Data Science Projects Fail",
    "Authors": "Balaram Panda",
    "Published": "2023-08-08T06:45:15Z",
    "Summary": "Data Science is a modern Data Intelligence practice, which is the core of many businesses and helps businesses build smart strategies around to deal with businesses challenges more efficiently. Data Science practice also helps in automating business processes using the algorithm, and it has several other benefits, which also deliver in a non-profitable framework. In regards to data science, three key components primarily influence the effective outcome of a data science project. Those are 1.Availability of Data 2.Algorithm 3.Processing power or infrastructure",
    "Link": "http://arxiv.org/abs/2308.04896v1",
    "PDF Link": "http://arxiv.org/pdf/2308.04896v1"
  },
  {
    "Title": "Citizen Science in the European Open Science Cloud",
    "Authors": "Stephen Serjeant",
    "Published": "2023-07-13T16:48:53Z",
    "Summary": "The European Open Science Cloud aims to make all data Findable, Accessible, Interoperable and Reusable. By far the largest community of users of the European Open Science Cloud is the science-inclined public. These users need a more curated experience of open science than subject specialists, but nevertheless make very substantial research contributions in open science, especially in crowdsourced data mining, i.e. citizen science. This short, non-technical invited review presents applications of citizen science in the European Open Science Cloud, with a particular focus on astrophysics and astroparticle physics.",
    "Link": "http://arxiv.org/abs/2307.06896v1",
    "PDF Link": "http://arxiv.org/pdf/2307.06896v1"
  },
  {
    "Title": "Building Data Science Capabilities into University Data Warehouse to\n  Predict Graduation",
    "Authors": "Joonas Pesonen, Anna Fomkin, Lauri Jokipii",
    "Published": "2018-05-04T12:28:03Z",
    "Summary": "The discipline of data science emerged to combine statistical methods with computing. At Aalto University, Finland, we have taken first steps to bring educational data science as a part of daily operations of Management Information Services. This required changes in IT environment: we enhanced data warehouse infrastructure with a data science lab, where we can read predictive model training data from data warehouse database and use the created predictive models in database queries. We then conducted a data science pilot with an objective to predict students' graduation probability and time-to-degree with student registry data. Further ethical and legal considerations are needed before using predictions in daily operations of the university.",
    "Link": "http://arxiv.org/abs/1805.05401v1",
    "PDF Link": "http://arxiv.org/pdf/1805.05401v1"
  },
  {
    "Title": "Starting with data: advancing spatial data science by building and\n  sharing high-quality datasets",
    "Authors": "Yingjie Hu",
    "Published": "2020-07-16T03:15:56Z",
    "Summary": "Spatial data science has emerged in recent years as an interdisciplinary field. This position paper discusses the importance of building and sharing high-quality datasets for spatial data science.",
    "Link": "http://arxiv.org/abs/2007.08087v1",
    "PDF Link": "http://arxiv.org/pdf/2007.08087v1"
  },
  {
    "Title": "Data Science in Biomedicine",
    "Authors": "Yovaninna Alarcón-Soto, Jenifer Espasandín-Domínguez, Ipek Guler, Mercedes Conde-Amboage, Francisco Gude-Sampedro, Klaus Langohr, Carmen Cadarso-Suárez, Guadalupe Gómez-Melis",
    "Published": "2019-09-09T11:31:40Z",
    "Summary": "We highlight the role of Data Science in Biomedicine. Our manuscript goes from the general to the particular, presenting a global definition of Data Science and showing the trend for this discipline together with the terms of cloud computing and big data. In addition, since Data Science is mostly related to areas like economy or business, we describe its importance in biomedicine. Biomedical Data Science (BDS) presents the challenge of dealing with data coming from a range of biological and medical research, focusing on methodologies to advance the biomedical science discoveries, in an interdisciplinary context.",
    "Link": "http://arxiv.org/abs/1909.04486v1",
    "PDF Link": "http://arxiv.org/pdf/1909.04486v1"
  },
  {
    "Title": "Perspectives on Surgical Data Science",
    "Authors": "S. Swaroop Vedula, Masaru Ishii, Gregory D. Hager",
    "Published": "2016-10-13T22:06:46Z",
    "Summary": "The availability of large amounts of data together with advances in analytical techniques afford an opportunity to address difficult challenges in ensuring that healthcare is safe, effective, efficient, patient-centered, equitable, and timely. Surgical care and training stand to tremendously gain through surgical data science. Herein, we discuss a few perspectives on the scope and objectives for surgical data science.",
    "Link": "http://arxiv.org/abs/1610.04276v1",
    "PDF Link": "http://arxiv.org/pdf/1610.04276v1"
  },
  {
    "Title": "Data Science: Nature and Pitfalls",
    "Authors": "Longbing Cao",
    "Published": "2020-06-28T02:06:54Z",
    "Summary": "Data science is creating very exciting trends as well as significant controversy. A critical matter for the healthy development of data science in its early stages is to deeply understand the nature of data and data science, and to discuss the various pitfalls. These important issues motivate the discussions in this article.",
    "Link": "http://arxiv.org/abs/2006.16964v1",
    "PDF Link": "http://arxiv.org/pdf/2006.16964v1"
  },
  {
    "Title": "A Mathematical Lens for Teaching Data Science",
    "Authors": "Johanna Hardin",
    "Published": "2025-01-03T22:36:20Z",
    "Summary": "Using the National Academies report, {\\em Data Science for Undergraduates: Opportunities and Options}, we connect data science curricula to the more familiar pedagogy used by many mathematical scientists. We use their list of ``data acumen\" components to ground a discussion, which hopes to connect data science curricula to the more familiar pedagogy used by many mathematical scientists.",
    "Link": "http://arxiv.org/abs/2501.02126v1",
    "PDF Link": "http://arxiv.org/pdf/2501.02126v1"
  },
  {
    "Title": "Navigating Diverse Data Science Learning: Critical Reflections Towards\n  Future Practice",
    "Authors": "Yehia Elkhatib",
    "Published": "2018-07-05T21:32:18Z",
    "Summary": "Data Science is currently a popular field of science attracting expertise from very diverse backgrounds. Current learning practices need to acknowledge this and adapt to it. This paper summarises some experiences relating to such learning approaches from teaching a postgraduate Data Science module, and draws some learned lessons that are of relevance to others teaching Data Science.",
    "Link": "http://arxiv.org/abs/1807.03750v1",
    "PDF Link": "http://arxiv.org/pdf/1807.03750v1"
  },
  {
    "Title": "ChatGPT for Teaching and Learning: An Experience from Data Science\n  Education",
    "Authors": "Yong Zheng",
    "Published": "2023-07-31T13:31:19Z",
    "Summary": "ChatGPT, an implementation and application of large language models, has gained significant popularity since its initial release. Researchers have been exploring ways to harness the practical benefits of ChatGPT in real-world scenarios. Educational researchers have investigated its potential in various subjects, e.g., programming, mathematics, finance, clinical decision support, etc. However, there has been limited attention given to its application in data science education. This paper aims to bridge that gap by utilizing ChatGPT in a data science course, gathering perspectives from students, and presenting our experiences and feedback on using ChatGPT for teaching and learning in data science education. The findings not only distinguish data science education from other disciplines but also uncover new opportunities and challenges associated with incorporating ChatGPT into the data science curriculum.",
    "Link": "http://arxiv.org/abs/2307.16650v1",
    "PDF Link": "http://arxiv.org/pdf/2307.16650v1"
  },
  {
    "Title": "Natural Language Processing using Hadoop and KOSHIK",
    "Authors": "Emre Erturk, Hong Shi",
    "Published": "2016-08-15T23:09:21Z",
    "Summary": "Natural language processing, as a data analytics related technology, is used widely in many research areas such as artificial intelligence, human language processing, and translation. At present, due to explosive growth of data, there are many challenges for natural language processing. Hadoop is one of the platforms that can process the large amount of data required for natural language processing. KOSHIK is one of the natural language processing architectures, and utilizes Hadoop and contains language processing components such as Stanford CoreNLP and OpenNLP. This study describes how to build a KOSHIK platform with the relevant tools, and provides the steps to analyze wiki data. Finally, it evaluates and discusses the advantages and disadvantages of the KOSHIK architecture, and gives recommendations on improving the processing performance.",
    "Link": "http://arxiv.org/abs/1608.04434v1",
    "PDF Link": "http://arxiv.org/pdf/1608.04434v1"
  },
  {
    "Title": "NLI4DB: A Systematic Review of Natural Language Interfaces for Databases",
    "Authors": "Mengyi Liu, Jianqiu Xu",
    "Published": "2025-03-04T09:22:50Z",
    "Summary": "As the demand for querying databases in all areas of life continues to grow, researchers have devoted significant attention to the natural language interface for databases (NLIDB). This paper presents a comprehensive survey of recently proposed NLIDBs. We begin with a brief introduction to natural language processing techniques, executable database languages and the intermediate representation between natural language and executable language, and then provide an overview of the translation process from natural language to executable database language. The translation process is divided into three stages: (i) natural language preprocessing, (ii) natural language understanding, and (iii) natural language translation. Traditional and data-driven methods are utilized in the preprocessing stage. Traditional approaches rely on predefined rules and grammars, and involve techniques such as regular expressions, dependency parsing and named entity recognition. Data-driven approaches depend on large-scale data and machine learning models, using techniques including word embedding and pattern linking. Natural language understanding methods are classified into three categories: (i) rule-based, (ii) machine learning-based, and (iii) hybrid. We then describe a general construction process for executable languages over relational and spatio-temporal databases. Subsequently, common benchmarks and evaluation metrics for transforming natural language into executable language are presented, and methods for generating new benchmarks are explored. Finally, we summarize the classification, development, and enhancement of NLIDB systems, and discuss deep language understanding and database interaction techniques related to NLIDB, including (i) using LLM for Text2SQL tasks, (ii) generating natural language interpretations from SQL, and (iii) transforming speech queries into SQL.",
    "Link": "http://arxiv.org/abs/2503.02435v1",
    "PDF Link": "http://arxiv.org/pdf/2503.02435v1"
  },
  {
    "Title": "Integrating AI Planning with Natural Language Processing: A Combination\n  of Explicit and Tacit Knowledge",
    "Authors": "Kebing Jin, Hankz Hankui Zhuo",
    "Published": "2022-02-15T02:19:09Z",
    "Summary": "Natural language processing (NLP) aims at investigating the interactions between agents and humans, processing and analyzing large amounts of natural language data. Large-scale language models play an important role in current natural language processing. However, the challenges of explainability and complexity come along with the developments of language models. One way is to introduce logical relations and rules into natural language processing models, such as making use of Automated Planning. Automated planning (AI planning) focuses on building symbolic domain models and synthesizing plans to transit initial states to goals based on domain models. Recently, there have been plenty of works related to these two fields, which have the abilities to generate explicit knowledge, e.g., preconditions and effects of action models, and learn from tacit knowledge, e.g., neural models, respectively. Integrating AI planning and natural language processing effectively improves the communication between human and intelligent agents. This paper outlines the commons and relations between AI planning and natural language processing, argues that each of them can effectively impact on the other one by five areas: (1) planning-based text understanding, (2) planning-based natural language processing, (3) planning-based explainability, (4) text-based human-robot interaction, and (5) applications. We also explore some potential future issues between AI planning and natural language processing. To the best of our knowledge, this survey is the first work that addresses the deep connections between AI planning and Natural language processing.",
    "Link": "http://arxiv.org/abs/2202.07138v2",
    "PDF Link": "http://arxiv.org/pdf/2202.07138v2"
  },
  {
    "Title": "Simple Natural Language Processing Tools for Danish",
    "Authors": "Leon Derczynski",
    "Published": "2019-06-27T13:15:12Z",
    "Summary": "This technical note describes a set of baseline tools for automatic processing of Danish text. The tools are machine-learning based, using natural language processing models trained over previously annotated documents. They are maintained at ITU Copenhagen and will always be freely available.",
    "Link": "http://arxiv.org/abs/1906.11608v2",
    "PDF Link": "http://arxiv.org/pdf/1906.11608v2"
  },
  {
    "Title": "Natural Language Generation",
    "Authors": "Emiel van Miltenburg, Chenghua Lin",
    "Published": "2025-03-20T22:12:08Z",
    "Summary": "This article provides a brief overview of the field of Natural Language Generation. The term Natural Language Generation (NLG), in its broadest definition, refers to the study of systems that verbalize some form of information through natural language. That information could be stored in a large database or knowledge graph (in data-to-text applications), but NLG researchers may also study summarisation (text-to-text) or image captioning (image-to-text), for example. As a subfield of Natural Language Processing, NLG is closely related to other sub-disciplines such as Machine Translation (MT) and Dialog Systems. Some NLG researchers exclude MT from their definition of the field, since there is no content selection involved where the system has to determine what to say. Conversely, dialog systems do not typically fall under the header of Natural Language Generation since NLG is just one component of dialog systems (the others being Natural Language Understanding and Dialog Management). However, with the rise of Large Language Models (LLMs), different subfields of Natural Language Processing have converged on similar methodologies for the production of natural language and the evaluation of automatically generated text.",
    "Link": "http://arxiv.org/abs/2503.16728v2",
    "PDF Link": "http://arxiv.org/pdf/2503.16728v2"
  },
  {
    "Title": "Towards the Study of Morphological Processing of the Tangkhul Language",
    "Authors": "Mirinso Shadang, Navanath Saharia, Thoudam Doren Singh",
    "Published": "2020-06-29T17:24:09Z",
    "Summary": "There is no or little work on natural language processing of Tangkhul language. The current work is a humble beginning of morphological processing of this language using an unsupervised approach. We use a small corpus collected from different sources of text books, short stories and articles of other topics. Based on the experiments carried out, the morpheme identification task using morphessor gives reasonable and interesting output despite using a small corpus.",
    "Link": "http://arxiv.org/abs/2006.16212v1",
    "PDF Link": "http://arxiv.org/pdf/2006.16212v1"
  },
  {
    "Title": "A Primer on Neural Network Models for Natural Language Processing",
    "Authors": "Yoav Goldberg",
    "Published": "2015-10-02T20:17:33Z",
    "Summary": "Over the past few years, neural networks have re-emerged as powerful machine-learning models, yielding state-of-the-art results in fields such as image recognition and speech processing. More recently, neural network models started to be applied also to textual natural language signals, again with very promising results. This tutorial surveys neural network models from the perspective of natural language processing research, in an attempt to bring natural-language researchers up to speed with the neural techniques. The tutorial covers input encoding for natural language tasks, feed-forward networks, convolutional networks, recurrent networks and recursive networks, as well as the computation graph abstraction for automatic gradient computation.",
    "Link": "http://arxiv.org/abs/1510.00726v1",
    "PDF Link": "http://arxiv.org/pdf/1510.00726v1"
  },
  {
    "Title": "Natural Language Understanding with Distributed Representation",
    "Authors": "Kyunghyun Cho",
    "Published": "2015-11-24T23:23:13Z",
    "Summary": "This is a lecture note for the course DS-GA 3001 <Natural Language Understanding with Distributed Representation> at the Center for Data Science , New York University in Fall, 2015. As the name of the course suggests, this lecture note introduces readers to a neural network based approach to natural language understanding/processing. In order to make it as self-contained as possible, I spend much time on describing basics of machine learning and neural networks, only after which how they are used for natural languages is introduced. On the language front, I almost solely focus on language modelling and machine translation, two of which I personally find most fascinating and most fundamental to natural language understanding.",
    "Link": "http://arxiv.org/abs/1511.07916v1",
    "PDF Link": "http://arxiv.org/pdf/1511.07916v1"
  },
  {
    "Title": "Deploying Technology to Save Endangered Languages",
    "Authors": "Hilaria Cruz, Joseph Waring",
    "Published": "2019-08-23T18:31:35Z",
    "Summary": "Computer scientists working on natural language processing, native speakers of endangered languages, and field linguists to discuss ways to harness Automatic Speech Recognition, especially neural networks, to automate annotation, speech tagging, and text parsing on endangered languages.",
    "Link": "http://arxiv.org/abs/1908.08971v2",
    "PDF Link": "http://arxiv.org/pdf/1908.08971v2"
  },
  {
    "Title": "Multilingual Text Classification for Dravidian Languages",
    "Authors": "Xiaotian Lin, Nankai Lin, Kanoksak Wattanachote, Shengyi Jiang, Lianxi Wang",
    "Published": "2021-12-03T04:26:49Z",
    "Summary": "As the fourth largest language family in the world, the Dravidian languages have become a research hotspot in natural language processing (NLP). Although the Dravidian languages contain a large number of languages, there are relatively few public available resources. Besides, text classification task, as a basic task of natural language processing, how to combine it to multiple languages in the Dravidian languages, is still a major difficulty in Dravidian Natural Language Processing. Hence, to address these problems, we proposed a multilingual text classification framework for the Dravidian languages. On the one hand, the framework used the LaBSE pre-trained model as the base model. Aiming at the problem of text information bias in multi-task learning, we propose to use the MLM strategy to select language-specific words, and used adversarial training to perturb them. On the other hand, in view of the problem that the model cannot well recognize and utilize the correlation among languages, we further proposed a language-specific representation module to enrich semantic information for the model. The experimental results demonstrated that the framework we proposed has a significant performance in multilingual text classification tasks with each strategy achieving certain improvements.",
    "Link": "http://arxiv.org/abs/2112.01705v1",
    "PDF Link": "http://arxiv.org/pdf/2112.01705v1"
  },
  {
    "Title": "Supporting Undotted Arabic with Pre-trained Language Models",
    "Authors": "Aviad Rom, Kfir Bar",
    "Published": "2021-11-18T16:47:56Z",
    "Summary": "We observe a recent behaviour on social media, in which users intentionally remove consonantal dots from Arabic letters, in order to bypass content-classification algorithms. Content classification is typically done by fine-tuning pre-trained language models, which have been recently employed by many natural-language-processing applications. In this work we study the effect of applying pre-trained Arabic language models on \"undotted\" Arabic texts. We suggest several ways of supporting undotted texts with pre-trained models, without additional training, and measure their performance on two Arabic natural-language-processing downstream tasks. The results are encouraging; in one of the tasks our method shows nearly perfect performance.",
    "Link": "http://arxiv.org/abs/2111.09791v1",
    "PDF Link": "http://arxiv.org/pdf/2111.09791v1"
  },
  {
    "Title": "A Precis of Language Models are not Models of Language",
    "Authors": "Csaba Veres",
    "Published": "2022-05-16T12:50:58Z",
    "Summary": "Natural Language Processing is one of the leading application areas in the current resurgence of Artificial Intelligence, spearheaded by Artificial Neural Networks. We show that despite their many successes at performing linguistic tasks, Large Neural Language Models are ill-suited as comprehensive models of natural language. The wider implication is that, in spite of the often overbearing optimism about AI, modern neural models do not represent a revolution in our understanding of cognition.",
    "Link": "http://arxiv.org/abs/2205.07634v1",
    "PDF Link": "http://arxiv.org/pdf/2205.07634v1"
  },
  {
    "Title": "Fence - An Efficient Parser with Ambiguity Support for Model-Driven\n  Language Specification",
    "Authors": "Luis Quesada, Fernando Berzal, Francisco J. Cortijo",
    "Published": "2011-07-23T12:56:02Z",
    "Summary": "Model-based language specification has applications in the implementation of language processors, the design of domain-specific languages, model-driven software development, data integration, text mining, natural language processing, and corpus-based induction of models. Model-based language specification decouples language design from language processing and, unlike traditional grammar-driven approaches, which constrain language designers to specific kinds of grammars, it needs general parser generators able to deal with ambiguities. In this paper, we propose Fence, an efficient bottom-up parsing algorithm with lexical and syntactic ambiguity support that enables the use of model-based language specification in practice.",
    "Link": "http://arxiv.org/abs/1107.4687v2",
    "PDF Link": "http://arxiv.org/pdf/1107.4687v2"
  },
  {
    "Title": "Including Signed Languages in Natural Language Processing",
    "Authors": "Kayo Yin, Amit Moryossef, Julie Hochgesang, Yoav Goldberg, Malihe Alikhani",
    "Published": "2021-05-11T17:37:55Z",
    "Summary": "Signed languages are the primary means of communication for many deaf and hard of hearing individuals. Since signed languages exhibit all the fundamental linguistic properties of natural language, we believe that tools and theories of Natural Language Processing (NLP) are crucial towards its modeling. However, existing research in Sign Language Processing (SLP) seldom attempt to explore and leverage the linguistic organization of signed languages. This position paper calls on the NLP community to include signed languages as a research area with high social and scientific impact. We first discuss the linguistic properties of signed languages to consider during their modeling. Then, we review the limitations of current SLP models and identify the open challenges to extend NLP to signed languages. Finally, we urge (1) the adoption of an efficient tokenization method; (2) the development of linguistically-informed models; (3) the collection of real-world signed language data; (4) the inclusion of local signed language communities as an active and leading voice in the direction of research.",
    "Link": "http://arxiv.org/abs/2105.05222v2",
    "PDF Link": "http://arxiv.org/pdf/2105.05222v2"
  },
  {
    "Title": "Self-move and Other-move: Quantum Categorical Foundations of Japanese",
    "Authors": "Ryder Dale Walton",
    "Published": "2022-10-10T06:26:59Z",
    "Summary": "The purpose of this work is to contribute toward the larger goal of creating a Quantum Natural Language Processing (QNLP) translator program. This work contributes original diagrammatic representations of the Japanese language based on prior work that accomplished on the English language based on category theory. The germane differences between the English and Japanese languages are emphasized to help address English language bias in the current body of research. Additionally, topological principles of these diagrams and many potential avenues for further research are proposed. Why is this endeavor important? Hundreds of languages have developed over the course of millennia coinciding with the evolution of human interaction across time and geographic location. These languages are foundational to human survival, experience, flourishing, and living the good life. They are also, however, the strongest barrier between people groups. Over the last several decades, advancements in Natural Language Processing (NLP) have made it easier to bridge the gap between individuals who do not share a common language or culture. Tools like Google Translate and DeepL make it easier than ever before to share our experiences with people globally. Nevertheless, these tools are still inadequate as they fail to convey our ideas across the language barrier fluently, leaving people feeling anxious and embarrassed. This is particularly true of languages born out of substantially different cultures, such as English and Japanese. Quantum computers offer the best chance to achieve translation fluency in that they are better suited to simulating the natural world and natural phenomenon such as natural speech.   Keywords: category theory, DisCoCat, DisCoCirc, Japanese grammar, English grammar, translation, topology, Quantum Natural Language Processing, Natural Language Processing",
    "Link": "http://arxiv.org/abs/2210.04451v1",
    "PDF Link": "http://arxiv.org/pdf/2210.04451v1"
  },
  {
    "Title": "PersianLLaMA: Towards Building First Persian Large Language Model",
    "Authors": "Mohammad Amin Abbasi, Arash Ghafouri, Mahdi Firouzmandi, Hassan Naderi, Behrouz Minaei Bidgoli",
    "Published": "2023-12-25T12:48:55Z",
    "Summary": "Despite the widespread use of the Persian language by millions globally, limited efforts have been made in natural language processing for this language. The use of large language models as effective tools in various natural language processing tasks typically requires extensive textual data and robust hardware resources. Consequently, the scarcity of Persian textual data and the unavailability of powerful hardware resources have hindered the development of large language models for Persian. This paper introduces the first large Persian language model, named PersianLLaMA, trained on a collection of Persian texts and datasets. This foundational model comes in two versions, with 7 and 13 billion parameters, trained on formal and colloquial Persian texts using two different approaches. PersianLLaMA has been evaluated for natural language generation tasks based on the latest evaluation methods, namely using larger language models, and for natural language understanding tasks based on automated machine metrics. The results indicate that PersianLLaMA significantly outperforms its competitors in both understanding and generating Persian text. PersianLLaMA marks an important step in the development of Persian natural language processing and can be a valuable resource for the Persian-speaking community. This large language model can be used for various natural language processing tasks, especially text generation like chatbots, question-answering, machine translation, and text summarization",
    "Link": "http://arxiv.org/abs/2312.15713v1",
    "PDF Link": "http://arxiv.org/pdf/2312.15713v1"
  },
  {
    "Title": "Problems and Countermeasures in Natural Language Processing Evaluation",
    "Authors": "Qingxiu Dong, Zhifang Sui, Weidong Zhan, Baobao Chang",
    "Published": "2021-04-20T01:35:16Z",
    "Summary": "Evaluation in natural language processing guides and promotes research on models and methods. In recent years, new evalua-tion data sets and evaluation tasks have been continuously proposed. At the same time, a series of problems exposed by ex-isting evaluation have also restricted the progress of natural language processing technology. Starting from the concept, com-position, development and meaning of natural language evaluation, this article classifies and summarizes the tasks and char-acteristics of mainstream natural language evaluation, and then summarizes the problems and causes of natural language pro-cessing evaluation. Finally, this article refers to the human language ability evaluation standard, puts forward the concept of human-like machine language ability evaluation, and proposes a series of basic principles and implementation ideas for hu-man-like machine language ability evaluation from the three aspects of reliability, difficulty and validity.",
    "Link": "http://arxiv.org/abs/2104.09712v1",
    "PDF Link": "http://arxiv.org/pdf/2104.09712v1"
  },
  {
    "Title": "Continuous multilinguality with language vectors",
    "Authors": "Robert Östling, Jörg Tiedemann",
    "Published": "2016-12-22T08:29:25Z",
    "Summary": "Most existing models for multilingual natural language processing (NLP) treat language as a discrete category, and make predictions for either one language or the other. In contrast, we propose using continuous vector representations of language. We show that these can be learned efficiently with a character-based neural language model, and used to improve inference about language varieties not seen during training. In experiments with 1303 Bible translations into 990 different languages, we empirically explore the capacity of multilingual language models, and also show that the language vectors capture genetic relationships between languages.",
    "Link": "http://arxiv.org/abs/1612.07486v2",
    "PDF Link": "http://arxiv.org/pdf/1612.07486v2"
  },
  {
    "Title": "Specifying Logic Programs in Controlled Natural Language",
    "Authors": "Norbert E. Fuchs, Rolf Schwitter",
    "Published": "1995-07-21T17:44:05Z",
    "Summary": "Writing specifications for computer programs is not easy since one has to take into account the disparate conceptual worlds of the application domain and of software development. To bridge this conceptual gap we propose controlled natural language as a declarative and application-specific specification language. Controlled natural language is a subset of natural language that can be accurately and efficiently processed by a computer, but is expressive enough to allow natural usage by non-specialists. Specifications in controlled natural language are automatically translated into Prolog clauses, hence become formal and executable. The translation uses a definite clause grammar (DCG) enhanced by feature structures. Inter-text references of the specification, e.g. anaphora, are resolved with the help of discourse representation theory (DRT). The generated Prolog clauses are added to a knowledge base. We have implemented a prototypical specification system that successfully processes the specification of a simple automated teller machine.",
    "Link": "http://arxiv.org/abs/cmp-lg/9507009v1",
    "PDF Link": "http://arxiv.org/pdf/cmp-lg/9507009v1"
  },
  {
    "Title": "A natural language interface to a graph-based bibliographic information\n  retrieval system",
    "Authors": "Yongjun Zhu, Erjia Yan, Il-Yeol Song",
    "Published": "2016-12-10T00:32:28Z",
    "Summary": "With the ever-increasing scientific literature, there is a need on a natural language interface to bibliographic information retrieval systems to retrieve related information effectively. In this paper, we propose a natural language interface, NLI-GIBIR, to a graph-based bibliographic information retrieval system. In designing NLI-GIBIR, we developed a novel framework that can be applicable to graph-based bibliographic information retrieval systems. Our framework integrates algorithms/heuristics for interpreting and analyzing natural language bibliographic queries. NLI-GIBIR allows users to search for a variety of bibliographic data through natural language. A series of text- and linguistic-based techniques are used to analyze and answer natural language queries, including tokenization, named entity recognition, and syntactic analysis. We find that our framework can effectively represents and addresses complex bibliographic information needs. Thus, the contributions of this paper are as follows: First, to our knowledge, it is the first attempt to propose a natural language interface to graph-based bibliographic information retrieval. Second, we propose a novel customized natural language processing framework that integrates a few original algorithms/heuristics for interpreting and analyzing natural language bibliographic queries. Third, we show that the proposed framework and natural language interface provide a practical solution in building real-world natural language interface-based bibliographic information retrieval systems. Our experimental results show that the presented system can correctly answer 39 out of 40 example natural language queries with varying lengths and complexities.",
    "Link": "http://arxiv.org/abs/1612.03231v1",
    "PDF Link": "http://arxiv.org/pdf/1612.03231v1"
  },
  {
    "Title": "A Survey of Resources and Methods for Natural Language Processing of\n  Serbian Language",
    "Authors": "Ulfeta A. Marovac, Aldina R. Avdić, Nikola Lj. Milošević",
    "Published": "2023-04-11T19:33:41Z",
    "Summary": "The Serbian language is a Slavic language spoken by over 12 million speakers and well understood by over 15 million people. In the area of natural language processing, it can be considered a low-resourced language. Also, Serbian is considered a high-inflectional language. The combination of many word inflections and low availability of language resources makes natural language processing of Serbian challenging. Nevertheless, over the past three decades, there have been a number of initiatives to develop resources and methods for natural language processing of Serbian, ranging from developing a corpus of free text from books and the internet, annotated corpora for classification and named entity recognition tasks to various methods and models performing these tasks. In this paper, we review the initiatives, resources, methods, and their availability.",
    "Link": "http://arxiv.org/abs/2304.05468v1",
    "PDF Link": "http://arxiv.org/pdf/2304.05468v1"
  },
  {
    "Title": "ANGLEr: A Next-Generation Natural Language Exploratory Framework",
    "Authors": "Timotej Knez, Marko Bajec, Slavko Žitnik",
    "Published": "2022-05-10T13:32:13Z",
    "Summary": "Natural language processing is used for solving a wide variety of problems. Some scholars and interest groups working with language resources are not well versed in programming, so there is a need for a good graphical framework that allows users to quickly design and test natural language processing pipelines without the need for programming. The existing frameworks do not satisfy all the requirements for such a tool. We, therefore, propose a new framework that provides a simple way for its users to build language processing pipelines. It also allows a simple programming language agnostic way for adding new modules, which will help the adoption by natural language processing developers and researchers. The main parts of the proposed framework consist of (a) a pluggable Docker-based architecture, (b) a general data model, and (c) APIs description along with the graphical user interface. The proposed design is being used for implementation of a new natural language processing framework, called ANGLEr.",
    "Link": "http://arxiv.org/abs/2206.08266v1",
    "PDF Link": "http://arxiv.org/pdf/2206.08266v1"
  },
  {
    "Title": "Natural Language Generation Using Link Grammar for General\n  Conversational Intelligence",
    "Authors": "Vignav Ramesh, Anton Kolonin",
    "Published": "2021-04-19T06:16:07Z",
    "Summary": "Many current artificial general intelligence (AGI) and natural language processing (NLP) architectures do not possess general conversational intelligence--that is, they either do not deal with language or are unable to convey knowledge in a form similar to the human language without manual, labor-intensive methods such as template-based customization. In this paper, we propose a new technique to automatically generate grammatically valid sentences using the Link Grammar database. This natural language generation method far outperforms current state-of-the-art baselines and may serve as the final component in a proto-AGI question answering pipeline that understandably handles natural language material.",
    "Link": "http://arxiv.org/abs/2105.00830v1",
    "PDF Link": "http://arxiv.org/pdf/2105.00830v1"
  },
  {
    "Title": "Information Flow in Pregroup Models of Natural Language",
    "Authors": "Peter M. Hines",
    "Published": "2018-11-08T05:10:34Z",
    "Summary": "This paper is about pregroup models of natural languages, and how they relate to the explicitly categorical use of pregroups in Compositional Distributional Semantics and Natural Language Processing. These categorical interpretations make certain assumptions about the nature of natural languages that, when stated formally, may be seen to impose strong restrictions on pregroup grammars for natural languages.   We formalize this as a hypothesis about the form that pregroup models of natural languages must take, and demonstrate by an artificial language example that these restrictions are not imposed by the pregroup axioms themselves. We compare and contrast the artificial language examples with natural languages (using Welsh, a language where the 'noun' type cannot be taken as primitive, as an illustrative example).   The hypothesis is simply that there must exist a causal connection, or information flow, between the words of a sentence in a language whose purpose is to communicate information. This is not necessarily the case with formal languages that are simply generated by a series of 'meaning-free' rules. This imposes restrictions on the types of pregroup grammars that we expect to find in natural languages; we formalize this in algebraic, categorical, and graphical terms.   We take some preliminary steps in providing conditions that ensure pregroup models satisfy these conjectured properties, and discuss the more general forms this hypothesis may take.",
    "Link": "http://arxiv.org/abs/1811.03273v1",
    "PDF Link": "http://arxiv.org/pdf/1811.03273v1"
  },
  {
    "Title": "Natural Language Understanding Based on Semantic Relations between\n  Sentences",
    "Authors": "Hyeok Kong",
    "Published": "2012-12-19T14:40:38Z",
    "Summary": "In this paper, we define event expression over sentences of natural language and semantic relations between events. Based on this definition, we formally consider text understanding process having events as basic unit.",
    "Link": "http://arxiv.org/abs/1212.4674v1",
    "PDF Link": "http://arxiv.org/pdf/1212.4674v1"
  },
  {
    "Title": "Thoth: Improved Rapid Serial Visual Presentation using Natural Language\n  Processing",
    "Authors": "David Awad",
    "Published": "2019-08-05T15:45:39Z",
    "Summary": "Thoth is a tool designed to combine many different types of speed reading technology. The largest insight is using natural language parsing for more optimal rapid serial visual presentation and more effective reading information.",
    "Link": "http://arxiv.org/abs/1908.01699v1",
    "PDF Link": "http://arxiv.org/pdf/1908.01699v1"
  },
  {
    "Title": "Language Tasks and Language Games: On Methodology in Current Natural\n  Language Processing Research",
    "Authors": "David Schlangen",
    "Published": "2019-08-28T14:29:13Z",
    "Summary": "This paper introduces a new task and a new dataset\", \"we improve the state of the art in X by Y\" -- it is rare to find a current natural language processing paper (or AI paper more generally) that does not contain such statements. What is mostly left implicit, however, is the assumption that this necessarily constitutes progress, and what it constitutes progress towards. Here, we make more precise the normally impressionistically used notions of language task and language game and ask how a research programme built on these might make progress towards the goal of modelling general language competence.",
    "Link": "http://arxiv.org/abs/1908.10747v1",
    "PDF Link": "http://arxiv.org/pdf/1908.10747v1"
  },
  {
    "Title": "Fuzzy Modeling and Natural Language Processing for Panini's Sanskrit\n  Grammar",
    "Authors": "P. Venkata Subba Reddy",
    "Published": "2010-06-14T20:07:32Z",
    "Summary": "Indian languages have long history in World Natural languages. Panini was the first to define Grammar for Sanskrit language with about 4000 rules in fifth century. These rules contain uncertainty information. It is not possible to Computer processing of Sanskrit language with uncertain information. In this paper, fuzzy logic and fuzzy reasoning are proposed to deal to eliminate uncertain information for reasoning with Sanskrit grammar. The Sanskrit language processing is also discussed in this paper.",
    "Link": "http://arxiv.org/abs/1006.2835v1",
    "PDF Link": "http://arxiv.org/pdf/1006.2835v1"
  },
  {
    "Title": "Categorical Tools for Natural Language Processing",
    "Authors": "Giovanni de Felice",
    "Published": "2022-12-13T15:12:37Z",
    "Summary": "This thesis develops the translation between category theory and computational linguistics as a foundation for natural language processing. The three chapters deal with syntax, semantics and pragmatics. First, string diagrams provide a unified model of syntactic structures in formal grammars. Second, functors compute semantics by turning diagrams into logical, tensor, neural or quantum computation. Third, the resulting functorial models can be composed to form games where equilibria are the solutions of language processing tasks. This framework is implemented as part of DisCoPy, the Python library for computing with string diagrams. We describe the correspondence between categorical, linguistic and computational structures, and demonstrate their applications in compositional natural language processing.",
    "Link": "http://arxiv.org/abs/2212.06636v1",
    "PDF Link": "http://arxiv.org/pdf/2212.06636v1"
  },
  {
    "Title": "Unnatural Language Processing: Bridging the Gap Between Synthetic and\n  Natural Language Data",
    "Authors": "Alana Marzoev, Samuel Madden, M. Frans Kaashoek, Michael Cafarella, Jacob Andreas",
    "Published": "2020-04-28T16:41:00Z",
    "Summary": "Large, human-annotated datasets are central to the development of natural language processing models. Collecting these datasets can be the most challenging part of the development process. We address this problem by introducing a general purpose technique for ``simulation-to-real'' transfer in language understanding problems with a delimited set of target behaviors, making it possible to develop models that can interpret natural utterances without natural training data. We begin with a synthetic data generation procedure, and train a model that can accurately interpret utterances produced by the data generator. To generalize to natural utterances, we automatically find projections of natural language utterances onto the support of the synthetic language, using learned sentence embeddings to define a distance metric. With only synthetic training data, our approach matches or outperforms state-of-the-art models trained on natural language data in several domains. These results suggest that simulation-to-real transfer is a practical framework for developing NLP applications, and that improved models for transfer might provide wide-ranging improvements in downstream tasks.",
    "Link": "http://arxiv.org/abs/2004.13645v1",
    "PDF Link": "http://arxiv.org/pdf/2004.13645v1"
  },
  {
    "Title": "Real-Time Multilingual Sign Language Processing",
    "Authors": "Amit Moryossef",
    "Published": "2024-12-02T21:51:41Z",
    "Summary": "Sign Language Processing (SLP) is an interdisciplinary field comprised of Natural Language Processing (NLP) and Computer Vision. It is focused on the computational understanding, translation, and production of signed languages. Traditional approaches have often been constrained by the use of gloss-based systems that are both language-specific and inadequate for capturing the multidimensional nature of sign language. These limitations have hindered the development of technology capable of processing signed languages effectively.   This thesis aims to revolutionize the field of SLP by proposing a simple paradigm that can bridge this existing technological gap. We propose the use of SignWiring, a universal sign language transcription notation system, to serve as an intermediary link between the visual-gestural modality of signed languages and text-based linguistic representations.   We contribute foundational libraries and resources to the SLP community, thereby setting the stage for a more in-depth exploration of the tasks of sign language translation and production. These tasks encompass the translation of sign language from video to spoken language text and vice versa. Through empirical evaluations, we establish the efficacy of our transcription method as a pivot for enabling faster, more targeted research, that can lead to more natural and accurate translations across a range of languages.   The universal nature of our transcription-based paradigm also paves the way for real-time, multilingual applications in SLP, thereby offering a more inclusive and accessible approach to language technology. This is a significant step toward universal accessibility, enabling a wider reach of AI-driven language technologies to include the deaf and hard-of-hearing community.",
    "Link": "http://arxiv.org/abs/2412.01991v1",
    "PDF Link": "http://arxiv.org/pdf/2412.01991v1"
  },
  {
    "Title": "Curriculum learning for language modeling",
    "Authors": "Daniel Campos",
    "Published": "2021-08-04T16:53:43Z",
    "Summary": "Language Models like ELMo and BERT have provided robust representations of natural language, which serve as the language understanding component for a diverse range of downstream tasks.Curriculum learning is a method that employs a structured training regime instead, which has been leveraged in computer vision and machine translation to improve model training speed and model performance. While language models have proven transformational for the natural language processing community, these models have proven expensive, energy-intensive, and challenging to train. In this work, we explore the effect of curriculum learning on language model pretraining using various linguistically motivated curricula and evaluate transfer performance on the GLUE Benchmark. Despite a broad variety of training methodologies and experiments we do not find compelling evidence that curriculum learning methods improve language model training.",
    "Link": "http://arxiv.org/abs/2108.02170v1",
    "PDF Link": "http://arxiv.org/pdf/2108.02170v1"
  },
  {
    "Title": "On Even Linear Indexed Languages with a Reduction to the Learning of\n  Context-Free Languages",
    "Authors": "Benjamin Caulfield",
    "Published": "2013-12-01T03:16:22Z",
    "Summary": "This paper presents a restricted form of linear indexed grammars, called even linear indexed grammars, which yield the even linear indexed languages. These languages properly contain the context-free languages and are contained in the set of linear indexed languages. We show that several patterns found in natural languages are also generated by these grammars, including crossing dependencies, copying, and multiple agreements. We discuss the learning problem for even linear indexed languages and show that it is reducible to that of the context-free languages. The closure properties for this class of languages are also presented.",
    "Link": "http://arxiv.org/abs/1312.0175v2",
    "PDF Link": "http://arxiv.org/pdf/1312.0175v2"
  },
  {
    "Title": "Challenges Encountered in Turkish Natural Language Processing Studies",
    "Authors": "Kadir Tohma, Yakup Kutlu",
    "Published": "2021-01-21T08:30:33Z",
    "Summary": "Natural language processing is a branch of computer science that combines artificial intelligence with linguistics. It aims to analyze a language element such as writing or speaking with software and convert it into information. Considering that each language has its own grammatical rules and vocabulary diversity, the complexity of the studies in this field is somewhat understandable. For instance, Turkish is a very interesting language in many ways. Examples of this are agglutinative word structure, consonant/vowel harmony, a large number of productive derivational morphemes (practically infinite vocabulary), derivation and syntactic relations, a complex emphasis on vocabulary and phonological rules. In this study, the interesting features of Turkish in terms of natural language processing are mentioned. In addition, summary info about natural language processing techniques, systems and various sources developed for Turkish are given.",
    "Link": "http://arxiv.org/abs/2101.11436v1",
    "PDF Link": "http://arxiv.org/pdf/2101.11436v1"
  },
  {
    "Title": "Pretraining with Artificial Language: Studying Transferable Knowledge in\n  Language Models",
    "Authors": "Ryokan Ri, Yoshimasa Tsuruoka",
    "Published": "2022-03-19T13:29:48Z",
    "Summary": "We investigate what kind of structural knowledge learned in neural network encoders is transferable to processing natural language. We design artificial languages with structural properties that mimic natural language, pretrain encoders on the data, and see how much performance the encoder exhibits on downstream tasks in natural language. Our experimental results show that pretraining with an artificial language with a nesting dependency structure provides some knowledge transferable to natural language. A follow-up probing analysis indicates that its success in the transfer is related to the amount of encoded contextual information and what is transferred is the knowledge of position-aware context dependence of language. Our results provide insights into how neural network encoders process human languages and the source of cross-lingual transferability of recent multilingual language models.",
    "Link": "http://arxiv.org/abs/2203.10326v2",
    "PDF Link": "http://arxiv.org/pdf/2203.10326v2"
  },
  {
    "Title": "Development of Deep Learning Based Natural Language Processing Model for\n  Turkish",
    "Authors": "Baris Baburoglu, Adem Tekerek, Mehmet Tekerek",
    "Published": "2019-05-07T21:09:49Z",
    "Summary": "Natural language is one of the most fundamental features that distinguish people from other living things and enable people to communicate each other. Language is a tool that enables people to express their feelings and thoughts and to transfers cultures through generations. Texts and audio are examples of natural language in daily life. In the natural language, many words disappear in time, on the other hand new words are derived. Therefore, while the process of natural language processing (NLP) is complex even for human, it is difficult to process in computer system. The area of linguistics examines how people use language. NLP, which requires the collaboration of linguists and computer scientists, plays an important role in human computer interaction. Studies in NLP have increased with the use of artificial intelligence technologies in the field of linguistics. With the deep learning methods which are one of the artificial intelligence study areas, platforms close to natural language are being developed. Developed platforms for language comprehension, machine translation and part of speech (POS) tagging benefit from deep learning methods. Recurrent Neural Network (RNN), one of the deep learning architectures, is preferred for processing sequential data such as text or audio data. In this study, Turkish POS tagging model has been proposed by using Bidirectional Long-Short Term Memory (BLSTM) which is an RNN type. The proposed POS tagging model is provided to natural language researchers with a platform that allows them to perform and use their own analysis. In the development phase of the platform developed by using BLSTM, the error rate of the POS tagger has been reduced by taking feedback with expert opinion.",
    "Link": "http://arxiv.org/abs/1905.05699v1",
    "PDF Link": "http://arxiv.org/pdf/1905.05699v1"
  },
  {
    "Title": "Natural Language Reasoning, A Survey",
    "Authors": "Fei Yu, Hongbo Zhang, Prayag Tiwari, Benyou Wang",
    "Published": "2023-03-26T13:44:18Z",
    "Summary": "This survey paper proposes a clearer view of natural language reasoning in the field of Natural Language Processing (NLP), both conceptually and practically. Conceptually, we provide a distinct definition for natural language reasoning in NLP, based on both philosophy and NLP scenarios, discuss what types of tasks require reasoning, and introduce a taxonomy of reasoning. Practically, we conduct a comprehensive literature review on natural language reasoning in NLP, mainly covering classical logical reasoning, natural language inference, multi-hop question answering, and commonsense reasoning. The paper also identifies and views backward reasoning, a powerful paradigm for multi-step reasoning, and introduces defeasible reasoning as one of the most important future directions in natural language reasoning research. We focus on single-modality unstructured natural language text, excluding neuro-symbolic techniques and mathematical reasoning.",
    "Link": "http://arxiv.org/abs/2303.14725v2",
    "PDF Link": "http://arxiv.org/pdf/2303.14725v2"
  },
  {
    "Title": "Causal Graph in Language Model Rediscovers Cortical Hierarchy in Human\n  Narrative Processing",
    "Authors": "Zhengqi He, Taro Toyoizumi",
    "Published": "2023-11-17T10:09:12Z",
    "Summary": "Understanding how humans process natural language has long been a vital research direction. The field of natural language processing (NLP) has recently experienced a surge in the development of powerful language models. These models have proven to be invaluable tools for studying another complex system known to process human language: the brain. Previous studies have demonstrated that the features of language models can be mapped to fMRI brain activity. This raises the question: is there a commonality between information processing in language models and the human brain? To estimate information flow patterns in a language model, we examined the causal relationships between different layers. Drawing inspiration from the workspace framework for consciousness, we hypothesized that features integrating more information would more accurately predict higher hierarchical brain activity. To validate this hypothesis, we classified language model features into two categories based on causal network measures: 'low in-degree' and 'high in-degree'. We subsequently compared the brain prediction accuracy maps for these two groups. Our results reveal that the difference in prediction accuracy follows a hierarchical pattern, consistent with the cortical hierarchy map revealed by activity time constants. This finding suggests a parallel between how language models and the human brain process linguistic information.",
    "Link": "http://arxiv.org/abs/2311.10431v1",
    "PDF Link": "http://arxiv.org/pdf/2311.10431v1"
  },
  {
    "Title": "File mapping Rule-based DBMS and Natural Language Processing",
    "Authors": "Vjacheslav M. Novikov",
    "Published": "2001-06-10T14:56:51Z",
    "Summary": "This paper describes the system of storage, extract and processing of information structured similarly to the natural language. For recursive inference the system uses the rules having the same representation, as the data. The environment of storage of information is provided with the File Mapping (SHM) mechanism of operating system. In the paper the main principles of construction of dynamic data structure and language for record of the inference rules are stated; the features of available implementation are considered and the description of the application realizing semantic information retrieval on the natural language is given.",
    "Link": "http://arxiv.org/abs/cs/0106016v1",
    "PDF Link": "http://arxiv.org/pdf/cs/0106016v1"
  },
  {
    "Title": "NLTK: The Natural Language Toolkit",
    "Authors": "Edward Loper, Steven Bird",
    "Published": "2002-05-17T12:51:00Z",
    "Summary": "NLTK, the Natural Language Toolkit, is a suite of open source program modules, tutorials and problem sets, providing ready-to-use computational linguistics courseware. NLTK covers symbolic and statistical natural language processing, and is interfaced to annotated corpora. Students augment and replace existing components, learn structured programming by example, and manipulate sophisticated models from the outset.",
    "Link": "http://arxiv.org/abs/cs/0205028v1",
    "PDF Link": "http://arxiv.org/pdf/cs/0205028v1"
  },
  {
    "Title": "Incrementalizing RASA's Open-Source Natural Language Understanding\n  Pipeline",
    "Authors": "Andrew Rafla, Casey Kennington",
    "Published": "2019-07-11T17:35:20Z",
    "Summary": "As spoken dialogue systems and chatbots are gaining more widespread adoption, commercial and open-sourced services for natural language understanding are emerging. In this paper, we explain how we altered the open-source RASA natural language understanding pipeline to process incrementally (i.e., word-by-word), following the incremental unit framework proposed by Schlangen and Skantze. To do so, we altered existing RASA components to process incrementally, and added an update-incremental intent recognition model as a component to RASA. Our evaluations on the Snips dataset show that our changes allow RASA to function as an effective incremental natural language understanding service.",
    "Link": "http://arxiv.org/abs/1907.05403v1",
    "PDF Link": "http://arxiv.org/pdf/1907.05403v1"
  },
  {
    "Title": "An Overview of Natural Language State Representation for Reinforcement\n  Learning",
    "Authors": "Brielen Madureira, David Schlangen",
    "Published": "2020-07-19T20:15:55Z",
    "Summary": "A suitable state representation is a fundamental part of the learning process in Reinforcement Learning. In various tasks, the state can either be described by natural language or be natural language itself. This survey outlines the strategies used in the literature to build natural language state representations. We appeal for more linguistically interpretable and grounded representations, careful justification of design decisions and evaluation of the effectiveness of different approaches.",
    "Link": "http://arxiv.org/abs/2007.09774v1",
    "PDF Link": "http://arxiv.org/pdf/2007.09774v1"
  },
  {
    "Title": "Natural Language Specifications in Proof Assistants",
    "Authors": "Colin S. Gordon, Sergey Matskevich",
    "Published": "2022-05-16T17:05:45Z",
    "Summary": "Interactive proof assistants are computer programs carefully constructed to check a human-designed proof of a mathematical claim with high confidence in the implementation. However, this only validates truth of a formal claim, which may have been mistranslated from a claim made in natural language. This is especially problematic when using proof assistants to formally verify the correctness of software with respect to a natural language specification. The translation from informal to formal remains a challenging, time-consuming process that is difficult to audit for correctness. This paper argues that it is possible to build support for natural language specifications within existing proof assistants, in a way that complements the principles used to establish trust and auditability in proof assistants themselves.",
    "Link": "http://arxiv.org/abs/2205.07811v1",
    "PDF Link": "http://arxiv.org/pdf/2205.07811v1"
  },
  {
    "Title": "Can Transformers Reason in Fragments of Natural Language?",
    "Authors": "Viktor Schlegel, Kamen V. Pavlov, Ian Pratt-Hartmann",
    "Published": "2022-11-10T08:46:53Z",
    "Summary": "State-of-the-art deep-learning-based approaches to Natural Language Processing (NLP) are credited with various capabilities that involve reasoning with natural language texts. In this paper we carry out a large-scale empirical study investigating the detection of formally valid inferences in controlled fragments of natural language for which the satisfiability problem becomes increasingly complex. We find that, while transformer-based language models perform surprisingly well in these scenarios, a deeper analysis re-veals that they appear to overfit to superficial patterns in the data rather than acquiring the logical principles governing the reasoning in these fragments.",
    "Link": "http://arxiv.org/abs/2211.05417v1",
    "PDF Link": "http://arxiv.org/pdf/2211.05417v1"
  },
  {
    "Title": "Attributes as Semantic Units between Natural Language and Visual\n  Recognition",
    "Authors": "Marcus Rohrbach",
    "Published": "2016-04-12T05:23:26Z",
    "Summary": "Impressive progress has been made in the fields of computer vision and natural language processing. However, it remains a challenge to find the best point of interaction for these very different modalities. In this chapter we discuss how attributes allow us to exchange information between the two modalities and in this way lead to an interaction on a semantic level. Specifically we discuss how attributes allow using knowledge mined from language resources for recognizing novel visual categories, how we can generate sentence description about images and video, how we can ground natural language in visual content, and finally, how we can answer natural language questions about images.",
    "Link": "http://arxiv.org/abs/1604.03249v1",
    "PDF Link": "http://arxiv.org/pdf/1604.03249v1"
  },
  {
    "Title": "NLP for The Greek Language: A Longer Survey",
    "Authors": "Katerina Papantoniou, Yannis Tzitzikas",
    "Published": "2024-08-20T15:57:18Z",
    "Summary": "English language is in the spotlight of the Natural Language Processing (NLP) community with other languages, like Greek, lagging behind in terms of offered methods, tools and resources. Due to the increasing interest in NLP, in this paper we try to condense research efforts for the automatic processing of Greek language covering the last three decades. In particular, we list and briefly discuss related works, resources and tools, categorized according to various processing layers and contexts. We are not restricted to the modern form of Greek language but also cover Ancient Greek and various Greek dialects. This survey can be useful for researchers and students interested in NLP tasks, Information Retrieval and Knowledge Management for the Greek language.",
    "Link": "http://arxiv.org/abs/2408.10962v1",
    "PDF Link": "http://arxiv.org/pdf/2408.10962v1"
  },
  {
    "Title": "GANCoder: An Automatic Natural Language-to-Programming Language\n  Translation Approach based on GAN",
    "Authors": "Yabing Zhu, Yanfeng Zhang, Huili Yang, Fangjing Wang",
    "Published": "2019-12-02T07:41:25Z",
    "Summary": "We propose GANCoder, an automatic programming approach based on Generative Adversarial Networks (GAN), which can generate the same functional and logical programming language codes conditioned on the given natural language utterances. The adversarial training between generator and discriminator helps generator learn distribution of dataset and improve code generation quality. Our experimental results show that GANCoder can achieve comparable accuracy with the state-of-the-art methods and is more stable when programming languages.",
    "Link": "http://arxiv.org/abs/1912.00609v1",
    "PDF Link": "http://arxiv.org/pdf/1912.00609v1"
  },
  {
    "Title": "Cross-lingual Adaption Model-Agnostic Meta-Learning for Natural Language\n  Understanding",
    "Authors": "Qianying Liu, Fei Cheng, Sadao Kurohashi",
    "Published": "2021-11-10T16:53:50Z",
    "Summary": "Meta learning with auxiliary languages has demonstrated promising improvements for cross-lingual natural language processing. However, previous studies sample the meta-training and meta-testing data from the same language, which limits the ability of the model for cross-lingual transfer. In this paper, we propose XLA-MAML, which performs direct cross-lingual adaption in the meta-learning stage. We conduct zero-shot and few-shot experiments on Natural Language Inference and Question Answering. The experimental results demonstrate the effectiveness of our method across different languages, tasks, and pretrained models. We also give analysis on various cross-lingual specific settings for meta-learning including sampling strategy and parallelism.",
    "Link": "http://arxiv.org/abs/2111.05805v1",
    "PDF Link": "http://arxiv.org/pdf/2111.05805v1"
  },
  {
    "Title": "Controlled Natural Languages and Default Reasoning",
    "Authors": "Tiantian Gao",
    "Published": "2019-05-11T02:02:55Z",
    "Summary": "Controlled natural languages (CNLs) are effective languages for knowledge representation and reasoning. They are designed based on certain natural languages with restricted lexicon and grammar. CNLs are unambiguous and simple as opposed to their base languages. They preserve the expressiveness and coherence of natural languages. In this report, we focus on a class of CNLs, called machine-oriented CNLs, which have well-defined semantics that can be deterministically translated into formal languages, such as Prolog, to do logical reasoning. Over the past 20 years, a number of machine-oriented CNLs emerged and have been used in many application domains for problem solving and question answering. However, few of them support non-monotonic inference. In our work, we propose non-monotonic extensions of CNL to support defeasible reasoning.   In the first part of this report, we survey CNLs and compare three influential systems: Attempto Controlled English (ACE), Processable English (PENG), and Computer-processable English (CPL). We compare their language design, semantic interpretations, and reasoning services. In the second part of this report, we first identify typical non-monotonicity in natural languages, such as defaults, exceptions and conversational implicatures. Then, we propose their representation in CNL and the corresponding formalizations in a form of defeasible reasoning known as Logic Programming with Defaults and Argumentation Theory (LPDA).",
    "Link": "http://arxiv.org/abs/1905.04422v1",
    "PDF Link": "http://arxiv.org/pdf/1905.04422v1"
  },
  {
    "Title": "Putting Natural in Natural Language Processing",
    "Authors": "Grzegorz Chrupała",
    "Published": "2023-05-08T09:29:31Z",
    "Summary": "Human language is firstly spoken and only secondarily written. Text, however, is a very convenient and efficient representation of language, and modern civilization has made it ubiquitous. Thus the field of NLP has overwhelmingly focused on processing written rather than spoken language. Work on spoken language, on the other hand, has been siloed off within the largely separate speech processing community which has been inordinately preoccupied with transcribing speech into text. Recent advances in deep learning have led to a fortuitous convergence in methods between speech processing and mainstream NLP. Arguably, the time is ripe for a unification of these two fields, and for starting to take spoken language seriously as the primary mode of human communication. Truly natural language processing could lead to better integration with the rest of language science and could lead to systems which are more data-efficient and more human-like, and which can communicate beyond the textual modality.",
    "Link": "http://arxiv.org/abs/2305.04572v2",
    "PDF Link": "http://arxiv.org/pdf/2305.04572v2"
  },
  {
    "Title": "Implications of Computer Vision Driven Assistive Technologies Towards\n  Individuals with Visual Impairment",
    "Authors": "Linda Wang, Alexander Wong",
    "Published": "2019-05-20T02:00:56Z",
    "Summary": "Computer vision based technology is becoming ubiquitous in society. One application area that has seen an increase in computer vision is assistive technologies, specifically for those with visual impairment. Research has shown the ability of computer vision models to achieve tasks such provide scene captions, detect objects and recognize faces. Although assisting individuals with visual impairment with these tasks increases their independence and autonomy, concerns over bias, privacy and potential usefulness arise. This paper addresses the positive and negative implications computer vision based assistive technologies have on individuals with visual impairment, as well as considerations for computer vision researchers and developers in order to mitigate the amount of negative implications.",
    "Link": "http://arxiv.org/abs/1905.07844v1",
    "PDF Link": "http://arxiv.org/pdf/1905.07844v1"
  },
  {
    "Title": "Second Croatian Computer Vision Workshop (CCVW 2013)",
    "Authors": "Sven Lončarić, Siniša Šegvić",
    "Published": "2013-10-01T14:26:29Z",
    "Summary": "Proceedings of the Second Croatian Computer Vision Workshop (CCVW 2013, http://www.fer.unizg.hr/crv/ccvw2013) held September 19, 2013, in Zagreb, Croatia. Workshop was organized by the Center of Excellence for Computer Vision of the University of Zagreb.",
    "Link": "http://arxiv.org/abs/1310.0319v3",
    "PDF Link": "http://arxiv.org/pdf/1310.0319v3"
  },
  {
    "Title": "Multiband NFC for High-Throughput Wireless Computer Vision Sensor\n  Network",
    "Authors": "F. Li, J. Du",
    "Published": "2017-05-28T06:43:29Z",
    "Summary": "Vision sensors lie in the heart of computer vision. In many computer vision applications, such as AR/VR, non-contacting near-field communication (NFC) with high throughput is required to transfer information to algorithms. In this work, we proposed a novel NFC system which utilizes multiple frequency bands to achieve high throughput.",
    "Link": "http://arxiv.org/abs/1707.03720v1",
    "PDF Link": "http://arxiv.org/pdf/1707.03720v1"
  },
  {
    "Title": "Deep Learning vs. Traditional Computer Vision",
    "Authors": "Niall O' Mahony, Sean Campbell, Anderson Carvalho, Suman Harapanahalli, Gustavo Velasco-Hernandez, Lenka Krpalkova, Daniel Riordan, Joseph Walsh",
    "Published": "2019-10-30T12:25:10Z",
    "Summary": "Deep Learning has pushed the limits of what was possible in the domain of Digital Image Processing. However, that is not to say that the traditional computer vision techniques which had been undergoing progressive development in years prior to the rise of DL have become obsolete. This paper will analyse the benefits and drawbacks of each approach. The aim of this paper is to promote a discussion on whether knowledge of classical computer vision techniques should be maintained. The paper will also explore how the two sides of computer vision can be combined. Several recent hybrid methodologies are reviewed which have demonstrated the ability to improve computer vision performance and to tackle problems not suited to Deep Learning. For example, combining traditional computer vision techniques with Deep Learning has been popular in emerging domains such as Panoramic Vision and 3D vision for which Deep Learning models have not yet been fully optimised",
    "Link": "http://arxiv.org/abs/1910.13796v1",
    "PDF Link": "http://arxiv.org/pdf/1910.13796v1"
  },
  {
    "Title": "Enhancing camera surveillance using computer vision: a research note",
    "Authors": "Haroon Idrees, Mubarak Shah, Ray Surette",
    "Published": "2018-08-12T20:01:37Z",
    "Summary": "$\\mathbf{Purpose}$ - The growth of police operated surveillance cameras has out-paced the ability of humans to monitor them effectively. Computer vision is a possible solution. An ongoing research project on the application of computer vision within a municipal police department is described. The paper aims to discuss these issues.   $\\mathbf{Design/methodology/approach}$ - Following the demystification of computer vision technology, its potential for police agencies is developed within a focus on computer vision as a solution for two common surveillance camera tasks (live monitoring of multiple surveillance cameras and summarizing archived video files). Three unaddressed research questions (can specialized computer vision applications for law enforcement be developed at this time, how will computer vision be utilized within existing public safety camera monitoring rooms, and what are the system-wide impacts of a computer vision capability on local criminal justice systems) are considered.   $\\mathbf{Findings}$ - Despite computer vision becoming accessible to law enforcement agencies the impact of computer vision has not been discussed or adequately researched. There is little knowledge of computer vision or its potential in the field.   $\\mathbf{Originality/value}$ - This paper introduces and discusses computer vision from a law enforcement perspective and will be valuable to police personnel tasked with monitoring large camera networks and considering computer vision as a system upgrade.",
    "Link": "http://arxiv.org/abs/1808.03998v1",
    "PDF Link": "http://arxiv.org/pdf/1808.03998v1"
  },
  {
    "Title": "Are object detection assessment criteria ready for maritime computer\n  vision?",
    "Authors": "Dilip K. Prasad, Huixu Dong, Deepu Rajan, Chai Quek",
    "Published": "2018-09-12T20:18:04Z",
    "Summary": "Maritime vessels equipped with visible and infrared cameras can complement other conventional sensors for object detection. However, application of computer vision techniques in maritime domain received attention only recently. The maritime environment offers its own unique requirements and challenges. Assessment of the quality of detections is a fundamental need in computer vision. However, the conventional assessment metrics suitable for usual object detection are deficient in the maritime setting. Thus, a large body of related work in computer vision appears inapplicable to the maritime setting at the first sight. We discuss the problem of defining assessment metrics suitable for maritime computer vision. We consider new bottom edge proximity metrics as assessment metrics for maritime computer vision. These metrics indicate that existing computer vision approaches are indeed promising for maritime computer vision and can play a foundational role in the emerging field of maritime computer vision.",
    "Link": "http://arxiv.org/abs/1809.04659v2",
    "PDF Link": "http://arxiv.org/pdf/1809.04659v2"
  },
  {
    "Title": "BMVC 2019: Workshop on Interpretable and Explainable Machine Vision",
    "Authors": "Alun Preece",
    "Published": "2019-09-16T14:44:19Z",
    "Summary": "Proceedings of the BMVC 2019 Workshop on Interpretable and Explainable Machine Vision, Cardiff, UK, September 12, 2019.",
    "Link": "http://arxiv.org/abs/1909.07245v1",
    "PDF Link": "http://arxiv.org/pdf/1909.07245v1"
  },
  {
    "Title": "SPARK: Multi-Vision Sensor Perception and Reasoning Benchmark for\n  Large-scale Vision-Language Models",
    "Authors": "Youngjoon Yu, Sangyun Chung, Byung-Kwan Lee, Yong Man Ro",
    "Published": "2024-08-22T03:59:48Z",
    "Summary": "Large-scale Vision-Language Models (LVLMs) have significantly advanced with text-aligned vision inputs. They have made remarkable progress in computer vision tasks by aligning text modality with vision inputs. There are also endeavors to incorporate multi-vision sensors beyond RGB, including thermal, depth, and medical X-ray images. However, we observe that current LVLMs view images taken from multi-vision sensors as if they were in the same RGB domain without considering the physical characteristics of multi-vision sensors. They fail to convey the fundamental multi-vision sensor information from the dataset and the corresponding contextual knowledge properly. Consequently, alignment between the information from the actual physical environment and the text is not achieved correctly, making it difficult to answer complex sensor-related questions that consider the physical environment. In this paper, we aim to establish a multi-vision Sensor Perception And Reasoning benchmarK called SPARK that can reduce the fundamental multi-vision sensor information gap between images and multi-vision sensors. We generated 6,248 vision-language test samples to investigate multi-vision sensory perception and multi-vision sensory reasoning on physical sensor knowledge proficiency across different formats, covering different types of sensor-related questions. We utilized these samples to assess ten leading LVLMs. The results showed that most models displayed deficiencies in multi-vision sensory reasoning to varying extents. Codes and data are available at https://github.com/top-yun/SPARK",
    "Link": "http://arxiv.org/abs/2408.12114v3",
    "PDF Link": "http://arxiv.org/pdf/2408.12114v3"
  },
  {
    "Title": "Vision Transformers in Medical Computer Vision -- A Contemplative\n  Retrospection",
    "Authors": "Arshi Parvaiz, Muhammad Anwaar Khalid, Rukhsana Zafar, Huma Ameer, Muhammad Ali, Muhammad Moazam Fraz",
    "Published": "2022-03-29T06:32:43Z",
    "Summary": "Recent escalation in the field of computer vision underpins a huddle of algorithms with the magnificent potential to unravel the information contained within images. These computer vision algorithms are being practised in medical image analysis and are transfiguring the perception and interpretation of Imaging data. Among these algorithms, Vision Transformers are evolved as one of the most contemporary and dominant architectures that are being used in the field of computer vision. These are immensely utilized by a plenty of researchers to perform new as well as former experiments. Here, in this article we investigate the intersection of Vision Transformers and Medical images and proffered an overview of various ViTs based frameworks that are being used by different researchers in order to decipher the obstacles in Medical Computer Vision. We surveyed the application of Vision transformers in different areas of medical computer vision such as image-based disease classification, anatomical structure segmentation, registration, region-based lesion Detection, captioning, report generation, reconstruction using multiple medical imaging modalities that greatly assist in medical diagnosis and hence treatment process. Along with this, we also demystify several imaging modalities used in Medical Computer Vision. Moreover, to get more insight and deeper understanding, self-attention mechanism of transformers is also explained briefly. Conclusively, we also put some light on available data sets, adopted methodology, their performance measures, challenges and their solutions in form of discussion. We hope that this review article will open future directions for researchers in medical computer vision.",
    "Link": "http://arxiv.org/abs/2203.15269v1",
    "PDF Link": "http://arxiv.org/pdf/2203.15269v1"
  },
  {
    "Title": "Adapting Computer Vision Algorithms for Omnidirectional Video",
    "Authors": "Hannes Fassold",
    "Published": "2019-07-22T11:12:35Z",
    "Summary": "Omnidirectional (360{\\deg}) video has got quite popular because it provides a highly immersive viewing experience. For computer vision algorithms, it poses several challenges, like the special (equirectangular) projection commonly employed and the huge image size. In this work, we give a high-level overview of these challenges and outline strategies how to adapt computer vision algorithm for the specifics of omnidirectional video.",
    "Link": "http://arxiv.org/abs/1907.09233v1",
    "PDF Link": "http://arxiv.org/pdf/1907.09233v1"
  },
  {
    "Title": "Real-time Tracking Based on Neuromrophic Vision",
    "Authors": "Hongmin Li, Pei Jing, Guoqi Li",
    "Published": "2015-10-18T16:27:36Z",
    "Summary": "Real-time tracking is an important problem in computer vision in which most methods are based on the conventional cameras. Neuromorphic vision is a concept defined by incorporating neuromorphic vision sensors such as silicon retinas in vision processing system. With the development of the silicon technology, asynchronous event-based silicon retinas that mimic neuro-biological architectures has been developed in recent years. In this work, we combine the vision tracking algorithm of computer vision with the information encoding mechanism of event-based sensors which is inspired from the neural rate coding mechanism. The real-time tracking of single object with the advantage of high speed of 100 time bins per second is successfully realized. Our method demonstrates that the computer vision methods could be used for the neuromorphic vision processing and we can realize fast real-time tracking using neuromorphic vision sensors compare to the conventional camera.",
    "Link": "http://arxiv.org/abs/1510.05275v1",
    "PDF Link": "http://arxiv.org/pdf/1510.05275v1"
  },
  {
    "Title": "Reconfiguring the Imaging Pipeline for Computer Vision",
    "Authors": "Mark Buckler, Suren Jayasuriya, Adrian Sampson",
    "Published": "2017-05-11T18:57:01Z",
    "Summary": "Advancements in deep learning have ignited an explosion of research on efficient hardware for embedded computer vision. Hardware vision acceleration, however, does not address the cost of capturing and processing the image data that feeds these algorithms. We examine the role of the image signal processing (ISP) pipeline in computer vision to identify opportunities to reduce computation and save energy. The key insight is that imaging pipelines should be designed to be configurable: to switch between a traditional photography mode and a low-power vision mode that produces lower-quality image data suitable only for computer vision. We use eight computer vision algorithms and a reversible pipeline simulation tool to study the imaging system's impact on vision performance. For both CNN-based and classical vision algorithms, we observe that only two ISP stages, demosaicing and gamma compression, are critical for task performance. We propose a new image sensor design that can compensate for skipping these stages. The sensor design features an adjustable resolution and tunable analog-to-digital converters (ADCs). Our proposed imaging system's vision mode disables the ISP entirely and configures the sensor to produce subsampled, lower-precision image data. This vision mode can save ~75% of the average energy of a baseline photography mode while having only a small impact on vision task accuracy.",
    "Link": "http://arxiv.org/abs/1705.04352v3",
    "PDF Link": "http://arxiv.org/pdf/1705.04352v3"
  },
  {
    "Title": "Integration and Performance Analysis of Artificial Intelligence and\n  Computer Vision Based on Deep Learning Algorithms",
    "Authors": "Bo Liu, Liqiang Yu, Chang Che, Qunwei Lin, Hao Hu, Xinyu Zhao",
    "Published": "2023-12-20T09:37:06Z",
    "Summary": "This paper focuses on the analysis of the application effectiveness of the integration of deep learning and computer vision technologies. Deep learning achieves a historic breakthrough by constructing hierarchical neural networks, enabling end-to-end feature learning and semantic understanding of images. The successful experiences in the field of computer vision provide strong support for training deep learning algorithms. The tight integration of these two fields has given rise to a new generation of advanced computer vision systems, significantly surpassing traditional methods in tasks such as machine vision image classification and object detection. In this paper, typical image classification cases are combined to analyze the superior performance of deep neural network models while also pointing out their limitations in generalization and interpretability, proposing directions for future improvements. Overall, the efficient integration and development trend of deep learning with massive visual data will continue to drive technological breakthroughs and application expansion in the field of computer vision, making it possible to build truly intelligent machine vision systems. This deepening fusion paradigm will powerfully promote unprecedented tasks and functions in computer vision, providing stronger development momentum for related disciplines and industries.",
    "Link": "http://arxiv.org/abs/2312.12872v1",
    "PDF Link": "http://arxiv.org/pdf/2312.12872v1"
  },
  {
    "Title": "Scaling Up Computer Vision Neural Networks Using Fast Fourier Transform",
    "Authors": "Siddharth Agrawal",
    "Published": "2023-02-02T19:19:10Z",
    "Summary": "Deep Learning-based Computer Vision field has recently been trying to explore larger kernels for convolution to effectively scale up Convolutional Neural Networks. Simultaneously, new paradigm of models such as Vision Transformers find it difficult to scale up to larger higher resolution images due to their quadratic complexity in terms of input sequence. In this report, Fast Fourier Transform is utilised in various ways to provide some solutions to these issues.",
    "Link": "http://arxiv.org/abs/2302.12185v1",
    "PDF Link": "http://arxiv.org/pdf/2302.12185v1"
  },
  {
    "Title": "Are Vision-Language Models Truly Understanding Multi-vision Sensor?",
    "Authors": "Sangyun Chung, Youngjoon Yu, Youngchae Chee, Se Yeon Kim, Byung-Kwan Lee, Yong Man Ro",
    "Published": "2024-12-30T06:44:25Z",
    "Summary": "Large-scale Vision-Language Models (VLMs) have advanced by aligning vision inputs with text, significantly improving performance in computer vision tasks. Moreover, for VLMs to be effectively utilized in real-world applications, an understanding of diverse multi-vision sensor data, such as thermal, depth, and X-ray information, is essential. However, we find that current VLMs process multi-vision sensor images without deep understanding of sensor information, disregarding each sensor's unique physical properties. This limitation restricts their capacity to interpret and respond to complex questions requiring multi-vision sensor reasoning. To address this, we propose a novel Multi-vision Sensor Perception and Reasoning (MS-PR) benchmark, assessing VLMs on their capacity for sensor-specific reasoning. Moreover, we introduce Diverse Negative Attributes (DNA) optimization to enable VLMs to perform deep reasoning on multi-vision sensor tasks, helping to bridge the core information gap between images and sensor data. Extensive experimental results validate that the proposed DNA method can significantly improve the multi-vision sensor reasoning for VLMs.",
    "Link": "http://arxiv.org/abs/2412.20750v1",
    "PDF Link": "http://arxiv.org/pdf/2412.20750v1"
  },
  {
    "Title": "Ethics and Creativity in Computer Vision",
    "Authors": "Negar Rostamzadeh, Emily Denton, Linda Petrini",
    "Published": "2021-12-06T15:23:08Z",
    "Summary": "This paper offers a retrospective of what we learnt from organizing the workshop *Ethical Considerations in Creative applications of Computer Vision* at CVPR 2021 conference and, prior to that, a series of workshops on *Computer Vision for Fashion, Art and Design* at ECCV 2018, ICCV 2019, and CVPR 2020. We hope this reflection will bring artists and machine learning researchers into conversation around the ethical and social dimensions of creative applications of computer vision.",
    "Link": "http://arxiv.org/abs/2112.03111v1",
    "PDF Link": "http://arxiv.org/pdf/2112.03111v1"
  },
  {
    "Title": "I'm sorry to say, but your understanding of image processing\n  fundamentals is absolutely wrong",
    "Authors": "Emanuel Diamant",
    "Published": "2008-08-01T04:45:17Z",
    "Summary": "The ongoing discussion whether modern vision systems have to be viewed as visually-enabled cognitive systems or cognitively-enabled vision systems is groundless, because perceptual and cognitive faculties of vision are separate components of human (and consequently, artificial) information processing system modeling.",
    "Link": "http://arxiv.org/abs/0808.0056v1",
    "PDF Link": "http://arxiv.org/pdf/0808.0056v1"
  },
  {
    "Title": "Nomic Embed Vision: Expanding the Latent Space",
    "Authors": "Zach Nussbaum, Brandon Duderstadt, Andriy Mulyar",
    "Published": "2024-06-06T21:02:51Z",
    "Summary": "This technical report describes the training of nomic-embed-vision, a highly performant, open-code, open-weights image embedding model that shares the same latent space as nomic-embed-text. Together, nomic-embed-vision and nomic-embed-text form the first unified latent space to achieve high performance across vision, language, and multimodal tasks.",
    "Link": "http://arxiv.org/abs/2406.18587v1",
    "PDF Link": "http://arxiv.org/pdf/2406.18587v1"
  },
  {
    "Title": "Computer Vision and Abnormal Patient Gait Assessment a Comparison of\n  Machine Learning Models",
    "Authors": "Jasmin Hundall, Benson A. Babu",
    "Published": "2020-03-22T02:00:15Z",
    "Summary": "Abnormal gait, its associated falls and complications have high patient morbidity, mortality. Computer vision detects, predicts patient gait abnormalities, assesses fall risk and serves as clinical decision support tool for physicians. This paper performs a systematic review of how computer vision, machine learning models perform an abnormal patient's gait assessment. Computer vision is beneficial in gait analysis, it helps capture the patient posture. Several literature suggests the use of different machine learning algorithms such as SVM, ANN, K-Star, Random Forest, KNN, among others to perform the classification on the features extracted to study patient gait abnormalities.",
    "Link": "http://arxiv.org/abs/2004.02810v1",
    "PDF Link": "http://arxiv.org/pdf/2004.02810v1"
  },
  {
    "Title": "Tuning computer vision models with task rewards",
    "Authors": "André Susano Pinto, Alexander Kolesnikov, Yuge Shi, Lucas Beyer, Xiaohua Zhai",
    "Published": "2023-02-16T11:49:48Z",
    "Summary": "Misalignment between model predictions and intended usage can be detrimental for the deployment of computer vision models. The issue is exacerbated when the task involves complex structured outputs, as it becomes harder to design procedures which address this misalignment. In natural language processing, this is often addressed using reinforcement learning techniques that align models with a task reward. We adopt this approach and show its surprising effectiveness across multiple computer vision tasks, such as object detection, panoptic segmentation, colorization and image captioning. We believe this approach has the potential to be widely useful for better aligning models with a diverse range of computer vision tasks.",
    "Link": "http://arxiv.org/abs/2302.08242v1",
    "PDF Link": "http://arxiv.org/pdf/2302.08242v1"
  },
  {
    "Title": "The possibility of making \\$138,000 from shredded banknote pieces using\n  computer vision",
    "Authors": "Chung To Kong",
    "Published": "2023-11-17T02:25:31Z",
    "Summary": "Every country must dispose of old banknotes. At the Hong Kong Monetary Authority visitor center, visitors can buy a paperweight souvenir full of shredded banknotes. Even though the shredded banknotes are small, by using computer vision, it is possible to reconstruct the whole banknote like a jigsaw puzzle. Each paperweight souvenir costs \\$100 HKD, and it is claimed to contain shredded banknotes equivalent to 138 complete \\$1000 HKD banknotes. In theory, \\$138,000 HKD can be recovered by using computer vision. This paper discusses the technique of collecting shredded banknote pieces and applying a computer vision program.",
    "Link": "http://arxiv.org/abs/2401.06133v1",
    "PDF Link": "http://arxiv.org/pdf/2401.06133v1"
  },
  {
    "Title": "Computer Stereo Vision for Autonomous Driving",
    "Authors": "Rui Fan, Li Wang, Mohammud Junaid Bocus, Ioannis Pitas",
    "Published": "2020-12-06T06:54:03Z",
    "Summary": "As an important component of autonomous systems, autonomous car perception has had a big leap with recent advances in parallel computing architectures. With the use of tiny but full-feature embedded supercomputers, computer stereo vision has been prevalently applied in autonomous cars for depth perception. The two key aspects of computer stereo vision are speed and accuracy. They are both desirable but conflicting properties, as the algorithms with better disparity accuracy usually have higher computational complexity. Therefore, the main aim of developing a computer stereo vision algorithm for resource-limited hardware is to improve the trade-off between speed and accuracy. In this chapter, we introduce both the hardware and software aspects of computer stereo vision for autonomous car systems. Then, we discuss four autonomous car perception tasks, including 1) visual feature detection, description and matching, 2) 3D information acquisition, 3) object detection/recognition and 4) semantic image segmentation. The principles of computer stereo vision and parallel computing on multi-threading CPU and GPU architectures are then detailed.",
    "Link": "http://arxiv.org/abs/2012.03194v2",
    "PDF Link": "http://arxiv.org/pdf/2012.03194v2"
  },
  {
    "Title": "LM4LV: A Frozen Large Language Model for Low-level Vision Tasks",
    "Authors": "Boyang Zheng, Jinjin Gu, Shijun Li, Chao Dong",
    "Published": "2024-05-24T17:25:00Z",
    "Summary": "The success of large language models (LLMs) has fostered a new research trend of multi-modality large language models (MLLMs), which changes the paradigm of various fields in computer vision. Though MLLMs have shown promising results in numerous high-level vision and vision-language tasks such as VQA and text-to-image, no works have demonstrated how low-level vision tasks can benefit from MLLMs. We find that most current MLLMs are blind to low-level features due to their design of vision modules, thus are inherently incapable for solving low-level vision tasks. In this work, we purpose $\\textbf{LM4LV}$, a framework that enables a FROZEN LLM to solve a range of low-level vision tasks without any multi-modal data or prior. This showcases the LLM's strong potential in low-level vision and bridges the gap between MLLMs and low-level vision tasks. We hope this work can inspire new perspectives on LLMs and deeper understanding of their mechanisms. Code is available at https://github.com/bytetriper/LM4LV.",
    "Link": "http://arxiv.org/abs/2405.15734v2",
    "PDF Link": "http://arxiv.org/pdf/2405.15734v2"
  },
  {
    "Title": "A survey of the Vision Transformers and their CNN-Transformer based\n  Variants",
    "Authors": "Asifullah Khan, Zunaira Rauf, Anabia Sohail, Abdul Rehman, Hifsa Asif, Aqsa Asif, Umair Farooq",
    "Published": "2023-05-17T01:27:27Z",
    "Summary": "Vision transformers have become popular as a possible substitute to convolutional neural networks (CNNs) for a variety of computer vision applications. These transformers, with their ability to focus on global relationships in images, offer large learning capacity. However, they may suffer from limited generalization as they do not tend to model local correlation in images. Recently, in vision transformers hybridization of both the convolution operation and self-attention mechanism has emerged, to exploit both the local and global image representations. These hybrid vision transformers, also referred to as CNN-Transformer architectures, have demonstrated remarkable results in vision applications. Given the rapidly growing number of hybrid vision transformers, it has become necessary to provide a taxonomy and explanation of these hybrid architectures. This survey presents a taxonomy of the recent vision transformer architectures and more specifically that of the hybrid vision transformers. Additionally, the key features of these architectures such as the attention mechanisms, positional embeddings, multi-scale processing, and convolution are also discussed. In contrast to the previous survey papers that are primarily focused on individual vision transformer architectures or CNNs, this survey uniquely emphasizes the emerging trend of hybrid vision transformers. By showcasing the potential of hybrid vision transformers to deliver exceptional performance across a range of computer vision tasks, this survey sheds light on the future directions of this rapidly evolving architecture.",
    "Link": "http://arxiv.org/abs/2305.09880v4",
    "PDF Link": "http://arxiv.org/pdf/2305.09880v4"
  },
  {
    "Title": "Computers Should Be Uniters Not Dividers: A Vision of Computer-Enhanced\n  Happy Future",
    "Authors": "Alexander Titovets, Philip Mills, Vladik Kreinovich",
    "Published": "2014-08-30T19:55:55Z",
    "Summary": "This manifesto provides a vision of how computers can be used to bring people together, to enhance people's use of their natural creativity, and thus, make them happier.",
    "Link": "http://arxiv.org/abs/1409.0158v1",
    "PDF Link": "http://arxiv.org/pdf/1409.0158v1"
  },
  {
    "Title": "A Survey on Deep Learning Methods for Robot Vision",
    "Authors": "Javier Ruiz-del-Solar, Patricio Loncomilla, Naiomi Soto",
    "Published": "2018-03-28T21:37:14Z",
    "Summary": "Deep learning has allowed a paradigm shift in pattern recognition, from using hand-crafted features together with statistical classifiers to using general-purpose learning procedures for learning data-driven representations, features, and classifiers together. The application of this new paradigm has been particularly successful in computer vision, in which the development of deep learning methods for vision applications has become a hot research topic. Given that deep learning has already attracted the attention of the robot vision community, the main purpose of this survey is to address the use of deep learning in robot vision. To achieve this, a comprehensive overview of deep learning and its usage in computer vision is given, that includes a description of the most frequently used neural models and their main application areas. Then, the standard methodology and tools used for designing deep-learning based vision systems are presented. Afterwards, a review of the principal work using deep learning in robot vision is presented, as well as current and future trends related to the use of deep learning in robotics. This survey is intended to be a guide for the developers of robot vision systems.",
    "Link": "http://arxiv.org/abs/1803.10862v1",
    "PDF Link": "http://arxiv.org/pdf/1803.10862v1"
  },
  {
    "Title": "Snapshot of Algebraic Vision",
    "Authors": "Joe Kileel, Kathlén Kohn",
    "Published": "2022-10-20T17:45:22Z",
    "Summary": "In this survey article, we present interactions between algebraic geometry and computer vision, which have recently come under the header of algebraic vision. The subject has given new insights in multiple view geometry and its application to 3D scene reconstruction and carried a host of novel problems and ideas back into algebraic geometry.",
    "Link": "http://arxiv.org/abs/2210.11443v2",
    "PDF Link": "http://arxiv.org/pdf/2210.11443v2"
  },
  {
    "Title": "CloudCV: Large Scale Distributed Computer Vision as a Cloud Service",
    "Authors": "Harsh Agrawal, Clint Solomon Mathialagan, Yash Goyal, Neelima Chavali, Prakriti Banik, Akrit Mohapatra, Ahmed Osman, Dhruv Batra",
    "Published": "2015-06-12T19:50:07Z",
    "Summary": "We are witnessing a proliferation of massive visual data. Unfortunately scaling existing computer vision algorithms to large datasets leaves researchers repeatedly solving the same algorithmic, logistical, and infrastructural problems. Our goal is to democratize computer vision; one should not have to be a computer vision, big data and distributed computing expert to have access to state-of-the-art distributed computer vision algorithms. We present CloudCV, a comprehensive system to provide access to state-of-the-art distributed computer vision algorithms as a cloud service through a Web Interface and APIs.",
    "Link": "http://arxiv.org/abs/1506.04130v3",
    "PDF Link": "http://arxiv.org/pdf/1506.04130v3"
  },
  {
    "Title": "Teaching Computer Vision for Ecology",
    "Authors": "Elijah Cole, Suzanne Stathatos, Björn Lütjens, Tarun Sharma, Justin Kay, Jason Parham, Benjamin Kellenberger, Sara Beery",
    "Published": "2023-01-05T18:30:17Z",
    "Summary": "Computer vision can accelerate ecology research by automating the analysis of raw imagery from sensors like camera traps, drones, and satellites. However, computer vision is an emerging discipline that is rarely taught to ecologists. This work discusses our experience teaching a diverse group of ecologists to prototype and evaluate computer vision systems in the context of an intensive hands-on summer workshop. We explain the workshop structure, discuss common challenges, and propose best practices. This document is intended for computer scientists who teach computer vision across disciplines, but it may also be useful to ecologists or other domain experts who are learning to use computer vision themselves.",
    "Link": "http://arxiv.org/abs/2301.02211v1",
    "PDF Link": "http://arxiv.org/pdf/2301.02211v1"
  },
  {
    "Title": "Negative Results in Computer Vision: A Perspective",
    "Authors": "Ali Borji",
    "Published": "2017-05-11T23:39:18Z",
    "Summary": "A negative result is when the outcome of an experiment or a model is not what is expected or when a hypothesis does not hold. Despite being often overlooked in the scientific community, negative results are results and they carry value. While this topic has been extensively discussed in other fields such as social sciences and biosciences, less attention has been paid to it in the computer vision community. The unique characteristics of computer vision, particularly its experimental aspect, call for a special treatment of this matter. In this paper, I will address what makes negative results important, how they should be disseminated and incentivized, and what lessons can be learned from cognitive vision research in this regard. Further, I will discuss issues such as computer vision and human vision interaction, experimental design and statistical hypothesis testing, explanatory versus predictive modeling, performance evaluation, model comparison, as well as computer vision research culture.",
    "Link": "http://arxiv.org/abs/1705.04402v3",
    "PDF Link": "http://arxiv.org/pdf/1705.04402v3"
  },
  {
    "Title": "Quantifying Visual Image Quality: A Bayesian View",
    "Authors": "Zhengfang Duanmu, Wentao Liu, Zhongling Wang, Zhou Wang",
    "Published": "2021-01-30T09:34:23Z",
    "Summary": "Image quality assessment (IQA) models aim to establish a quantitative relationship between visual images and their perceptual quality by human observers. IQA modeling plays a special bridging role between vision science and engineering practice, both as a test-bed for vision theories and computational biovision models, and as a powerful tool that could potentially make profound impact on a broad range of image processing, computer vision, and computer graphics applications, for design, optimization, and evaluation purposes. IQA research has enjoyed an accelerated growth in the past two decades. Here we present an overview of IQA methods from a Bayesian perspective, with the goals of unifying a wide spectrum of IQA approaches under a common framework and providing useful references to fundamental concepts accessible to vision scientists and image processing practitioners. We discuss the implications of the successes and limitations of modern IQA methods for biological vision and the prospect for vision science to inform the design of future artificial vision systems.",
    "Link": "http://arxiv.org/abs/2102.00195v2",
    "PDF Link": "http://arxiv.org/pdf/2102.00195v2"
  },
  {
    "Title": "Vision Language Transformers: A Survey",
    "Authors": "Clayton Fields, Casey Kennington",
    "Published": "2023-07-06T19:08:56Z",
    "Summary": "Vision language tasks, such as answering questions about or generating captions that describe an image, are difficult tasks for computers to perform. A relatively recent body of research has adapted the pretrained transformer architecture introduced in \\citet{vaswani2017attention} to vision language modeling. Transformer models have greatly improved performance and versatility over previous vision language models. They do so by pretraining models on a large generic datasets and transferring their learning to new tasks with minor changes in architecture and parameter values. This type of transfer learning has become the standard modeling practice in both natural language processing and computer vision. Vision language transformers offer the promise of producing similar advancements in tasks which require both vision and language. In this paper, we provide a broad synthesis of the currently available research on vision language transformer models and offer some analysis of their strengths, limitations and some open questions that remain.",
    "Link": "http://arxiv.org/abs/2307.03254v1",
    "PDF Link": "http://arxiv.org/pdf/2307.03254v1"
  },
  {
    "Title": "DAMamba: Vision State Space Model with Dynamic Adaptive Scan",
    "Authors": "Tanzhe Li, Caoshuo Li, Jiayi Lyu, Hongjuan Pei, Baochang Zhang, Taisong Jin, Rongrong Ji",
    "Published": "2025-02-18T08:12:47Z",
    "Summary": "State space models (SSMs) have recently garnered significant attention in computer vision. However, due to the unique characteristics of image data, adapting SSMs from natural language processing to computer vision has not outperformed the state-of-the-art convolutional neural networks (CNNs) and Vision Transformers (ViTs). Existing vision SSMs primarily leverage manually designed scans to flatten image patches into sequences locally or globally. This approach disrupts the original semantic spatial adjacency of the image and lacks flexibility, making it difficult to capture complex image structures. To address this limitation, we propose Dynamic Adaptive Scan (DAS), a data-driven method that adaptively allocates scanning orders and regions. This enables more flexible modeling capabilities while maintaining linear computational complexity and global modeling capacity. Based on DAS, we further propose the vision backbone DAMamba, which significantly outperforms current state-of-the-art vision Mamba models in vision tasks such as image classification, object detection, instance segmentation, and semantic segmentation. Notably, it surpasses some of the latest state-of-the-art CNNs and ViTs. Code will be available at https://github.com/ltzovo/DAMamba.",
    "Link": "http://arxiv.org/abs/2502.12627v1",
    "PDF Link": "http://arxiv.org/pdf/2502.12627v1"
  },
  {
    "Title": "LOTUS: Improving Transformer Efficiency with Sparsity Pruning and Data\n  Lottery Tickets",
    "Authors": "Ojasw Upadhyay",
    "Published": "2024-05-01T23:30:12Z",
    "Summary": "Vision transformers have revolutionized computer vision, but their computational demands present challenges for training and deployment. This paper introduces LOTUS (LOttery Transformers with Ultra Sparsity), a novel method that leverages data lottery ticket selection and sparsity pruning to accelerate vision transformer training while maintaining accuracy. Our approach focuses on identifying and utilizing the most informative data subsets and eliminating redundant model parameters to optimize the training process. Through extensive experiments, we demonstrate the effectiveness of LOTUS in achieving rapid convergence and high accuracy with significantly reduced computational requirements. This work highlights the potential of combining data selection and sparsity techniques for efficient vision transformer training, opening doors for further research and development in this area.",
    "Link": "http://arxiv.org/abs/2405.00906v1",
    "PDF Link": "http://arxiv.org/pdf/2405.00906v1"
  },
  {
    "Title": "Leveraging Vision Reconstruction Pipelines for Satellite Imagery",
    "Authors": "Kai Zhang, Jin Sun, Noah Snavely",
    "Published": "2019-10-07T18:14:28Z",
    "Summary": "Reconstructing 3D geometry from satellite imagery is an important topic of research. However, disparities exist between how this 3D reconstruction problem is handled in the remote sensing context and how multi-view reconstruction pipelines have been developed in the computer vision community. In this paper, we explore whether state-of-the-art reconstruction pipelines from the vision community can be applied to the satellite imagery. Along the way, we address several challenges adapting vision-based structure from motion and multi-view stereo methods. We show that vision pipelines can offer competitive speed and accuracy in the satellite context.",
    "Link": "http://arxiv.org/abs/1910.02989v2",
    "PDF Link": "http://arxiv.org/pdf/1910.02989v2"
  },
  {
    "Title": "Agriculture-Vision Challenge 2022 -- The Runner-Up Solution for\n  Agricultural Pattern Recognition via Transformer-based Models",
    "Authors": "Zhicheng Yang, Jui-Hsin Lai, Jun Zhou, Hang Zhou, Chen Du, Zhongcheng Lai",
    "Published": "2022-06-23T18:02:12Z",
    "Summary": "The Agriculture-Vision Challenge in CVPR is one of the most famous and competitive challenges for global researchers to break the boundary between computer vision and agriculture sectors, aiming at agricultural pattern recognition from aerial images. In this paper, we propose our solution to the third Agriculture-Vision Challenge in CVPR 2022. We leverage a data pre-processing scheme and several Transformer-based models as well as data augmentation techniques to achieve a mIoU of 0.582, accomplishing the 2nd place in this challenge.",
    "Link": "http://arxiv.org/abs/2206.11920v1",
    "PDF Link": "http://arxiv.org/pdf/2206.11920v1"
  },
  {
    "Title": "Cross-Modal Safety Mechanism Transfer in Large Vision-Language Models",
    "Authors": "Shicheng Xu, Liang Pang, Yunchang Zhu, Huawei Shen, Xueqi Cheng",
    "Published": "2024-10-16T15:20:08Z",
    "Summary": "Vision-language alignment in Large Vision-Language Models (LVLMs) successfully enables LLMs to understand visual input. However, we find that existing vision-language alignment methods fail to transfer the existing safety mechanism for text in LLMs to vision, which leads to vulnerabilities in toxic image. To explore the cause of this problem, we give the insightful explanation of where and how the safety mechanism of LVLMs operates and conduct comparative analysis between text and vision. We find that the hidden states at the specific transformer layers play a crucial role in the successful activation of safety mechanism, while the vision-language alignment at hidden states level in current methods is insufficient. This results in a semantic shift for input images compared to text in hidden states, therefore misleads the safety mechanism. To address this, we propose a novel Text-Guided vision-language Alignment method (TGA) for LVLMs. TGA retrieves the texts related to input vision and uses them to guide the projection of vision into the hidden states space in LLMs. Experiments show that TGA not only successfully transfers the safety mechanism for text in basic LLMs to vision in vision-language alignment for LVLMs without any safety fine-tuning on the visual modality but also maintains the general performance on various vision tasks (Safe and Good).",
    "Link": "http://arxiv.org/abs/2410.12662v2",
    "PDF Link": "http://arxiv.org/pdf/2410.12662v2"
  },
  {
    "Title": "VideoLLM-MoD: Efficient Video-Language Streaming with Mixture-of-Depths\n  Vision Computation",
    "Authors": "Shiwei Wu, Joya Chen, Kevin Qinghong Lin, Qimeng Wang, Yan Gao, Qianli Xu, Tong Xu, Yao Hu, Enhong Chen, Mike Zheng Shou",
    "Published": "2024-08-29T17:21:58Z",
    "Summary": "A well-known dilemma in large vision-language models (e.g., GPT-4, LLaVA) is that while increasing the number of vision tokens generally enhances visual understanding, it also significantly raises memory and computational costs, especially in long-term, dense video frame streaming scenarios. Although learnable approaches like Q-Former and Perceiver Resampler have been developed to reduce the vision token burden, they overlook the context causally modeled by LLMs (i.e., key-value cache), potentially leading to missed visual cues when addressing user queries. In this paper, we introduce a novel approach to reduce vision compute by leveraging redundant vision tokens \"skipping layers\" rather than decreasing the number of vision tokens. Our method, VideoLLM-MoD, is inspired by mixture-of-depths LLMs and addresses the challenge of numerous vision tokens in long-term or streaming video. Specifically, for each transformer layer, we learn to skip the computation for a high proportion (e.g., 80\\%) of vision tokens, passing them directly to the next layer. This approach significantly enhances model efficiency, achieving approximately \\textasciitilde42\\% time and \\textasciitilde30\\% memory savings for the entire training. Moreover, our method reduces the computation in the context and avoid decreasing the vision tokens, thus preserving or even improving performance compared to the vanilla model. We conduct extensive experiments to demonstrate the effectiveness of VideoLLM-MoD, showing its state-of-the-art results on multiple benchmarks, including narration, forecasting, and summarization tasks in COIN, Ego4D, and Ego-Exo4D datasets.",
    "Link": "http://arxiv.org/abs/2408.16730v1",
    "PDF Link": "http://arxiv.org/pdf/2408.16730v1"
  },
  {
    "Title": "Towards Point Cloud Compression for Machine Perception: A Simple and\n  Strong Baseline by Learning the Octree Depth Level Predictor",
    "Authors": "Lei Liu, Zhihao Hu, Zhenghao Chen",
    "Published": "2024-06-02T16:13:57Z",
    "Summary": "Point cloud compression has garnered significant interest in computer vision. However, existing algorithms primarily cater to human vision, while most point cloud data is utilized for machine vision tasks. To address this, we propose a point cloud compression framework that simultaneously handles both human and machine vision tasks. Our framework learns a scalable bit-stream, using only subsets for different machine vision tasks to save bit-rate, while employing the entire bit-stream for human vision tasks. Building on mainstream octree-based frameworks like VoxelContext-Net, OctAttention, and G-PCC, we introduce a new octree depth-level predictor. This predictor adaptively determines the optimal depth level for each octree constructed from a point cloud, controlling the bit-rate for machine vision tasks. For simpler tasks (\\textit{e.g.}, classification) or objects/scenarios, we use fewer depth levels with fewer bits, saving bit-rate. Conversely, for more complex tasks (\\textit{e.g}., segmentation) or objects/scenarios, we use deeper depth levels with more bits to enhance performance. Experimental results on various datasets (\\textit{e.g}., ModelNet10, ModelNet40, ShapeNet, ScanNet, and KITTI) show that our point cloud compression approach improves performance for machine vision tasks without compromising human vision quality.",
    "Link": "http://arxiv.org/abs/2406.00791v1",
    "PDF Link": "http://arxiv.org/pdf/2406.00791v1"
  },
  {
    "Title": "Hypergraph Vision Transformers: Images are More than Nodes, More than\n  Edges",
    "Authors": "Joshua Fixelle",
    "Published": "2025-04-11T17:20:26Z",
    "Summary": "Recent advancements in computer vision have highlighted the scalability of Vision Transformers (ViTs) across various tasks, yet challenges remain in balancing adaptability, computational efficiency, and the ability to model higher-order relationships. Vision Graph Neural Networks (ViGs) offer an alternative by leveraging graph-based methodologies but are hindered by the computational bottlenecks of clustering algorithms used for edge generation. To address these issues, we propose the Hypergraph Vision Transformer (HgVT), which incorporates a hierarchical bipartite hypergraph structure into the vision transformer framework to capture higher-order semantic relationships while maintaining computational efficiency. HgVT leverages population and diversity regularization for dynamic hypergraph construction without clustering, and expert edge pooling to enhance semantic extraction and facilitate graph-based image retrieval. Empirical results demonstrate that HgVT achieves strong performance on image classification and retrieval, positioning it as an efficient framework for semantic-based vision tasks.",
    "Link": "http://arxiv.org/abs/2504.08710v1",
    "PDF Link": "http://arxiv.org/pdf/2504.08710v1"
  },
  {
    "Title": "From Structured to Unstructured:A Comparative Analysis of Computer\n  Vision and Graph Models in solving Mesh-based PDEs",
    "Authors": "Jens Decke, Olaf Wünsch, Bernhard Sick, Christian Gruhl",
    "Published": "2024-05-31T12:21:26Z",
    "Summary": "This article investigates the application of computer vision and graph-based models in solving mesh-based partial differential equations within high-performance computing environments. Focusing on structured, graded structured, and unstructured meshes, the study compares the performance and computational efficiency of three computer vision-based models against three graph-based models across three data\\-sets. The research aims to identify the most suitable models for different mesh topographies, particularly highlighting the exploration of graded meshes, a less studied area. Results demonstrate that computer vision-based models, notably U-Net, outperform the graph models in prediction performance and efficiency in two (structured and graded) out of three mesh topographies. The study also reveals the unexpected effectiveness of computer vision-based models in handling unstructured meshes, suggesting a potential shift in methodological approaches for data-driven partial differential equation learning. The article underscores deep learning as a viable and potentially sustainable way to enhance traditional high-performance computing methods, advocating for informed model selection based on the topography of the mesh.",
    "Link": "http://arxiv.org/abs/2406.00081v1",
    "PDF Link": "http://arxiv.org/pdf/2406.00081v1"
  },
  {
    "Title": "ToVE: Efficient Vision-Language Learning via Knowledge Transfer from\n  Vision Experts",
    "Authors": "Yuanchen Wu, Junlong Du, Ke Yan, Shouhong Ding, Xiaoqiang Li",
    "Published": "2025-04-01T12:02:40Z",
    "Summary": "Vision-language (VL) learning requires extensive visual perception capabilities, such as fine-grained object recognition and spatial perception. Recent works typically rely on training huge models on massive datasets to develop these capabilities. As a more efficient alternative, this paper proposes a new framework that Transfers the knowledge from a hub of Vision Experts (ToVE) for efficient VL learning, leveraging pre-trained vision expert models to promote visual perception capability. Specifically, building on a frozen CLIP encoder that provides vision tokens for image-conditioned language generation, ToVE introduces a hub of multiple vision experts and a token-aware gating network that dynamically routes expert knowledge to vision tokens. In the transfer phase, we propose a \"residual knowledge transfer\" strategy, which not only preserves the generalizability of the vision tokens but also allows detachment of low-contributing experts to improve inference efficiency. Further, we explore to merge these expert knowledge to a single CLIP encoder, creating a knowledge-merged CLIP that produces more informative vision tokens without expert inference during deployment. Experiment results across various VL tasks demonstrate that the proposed ToVE achieves competitive performance with two orders of magnitude fewer training data.",
    "Link": "http://arxiv.org/abs/2504.00691v1",
    "PDF Link": "http://arxiv.org/pdf/2504.00691v1"
  },
  {
    "Title": "WiCV 2019: The Sixth Women In Computer Vision Workshop",
    "Authors": "Irene Amerini, Elena Balashova, Sayna Ebrahimi, Kathryn Leonard, Arsha Nagrani, Amaia Salvador",
    "Published": "2019-09-23T08:52:33Z",
    "Summary": "In this paper we present the Women in Computer Vision Workshop - WiCV 2019, organized in conjunction with CVPR 2019. This event is meant for increasing the visibility and inclusion of women researchers in the computer vision field. Computer vision and machine learning have made incredible progress over the past years, but the number of female researchers is still low both in academia and in industry. WiCV is organized especially for the following reason: to raise visibility of female researchers, to increase collaborations between them, and to provide mentorship to female junior researchers in the field. In this paper, we present a report of trends over the past years, along with a summary of statistics regarding presenters, attendees, and sponsorship for the current workshop.",
    "Link": "http://arxiv.org/abs/1909.10225v1",
    "PDF Link": "http://arxiv.org/pdf/1909.10225v1"
  },
  {
    "Title": "Does Image Anonymization Impact Computer Vision Training?",
    "Authors": "Håkon Hukkelås, Frank Lindseth",
    "Published": "2023-06-08T12:02:03Z",
    "Summary": "Image anonymization is widely adapted in practice to comply with privacy regulations in many regions. However, anonymization often degrades the quality of the data, reducing its utility for computer vision development. In this paper, we investigate the impact of image anonymization for training computer vision models on key computer vision tasks (detection, instance segmentation, and pose estimation). Specifically, we benchmark the recognition drop on common detection datasets, where we evaluate both traditional and realistic anonymization for faces and full bodies. Our comprehensive experiments reflect that traditional image anonymization substantially impacts final model performance, particularly when anonymizing the full body. Furthermore, we find that realistic anonymization can mitigate this decrease in performance, where our experiments reflect a minimal performance drop for face anonymization. Our study demonstrates that realistic anonymization can enable privacy-preserving computer vision development with minimal performance degradation across a range of important computer vision benchmarks.",
    "Link": "http://arxiv.org/abs/2306.05135v1",
    "PDF Link": "http://arxiv.org/pdf/2306.05135v1"
  },
  {
    "Title": "The Evolution of First Person Vision Methods: A Survey",
    "Authors": "Alejandro Betancourt, Pietro Morerio, Carlo S. Regazzoni, Matthias Rauterberg",
    "Published": "2014-09-04T16:38:43Z",
    "Summary": "The emergence of new wearable technologies such as action cameras and smart-glasses has increased the interest of computer vision scientists in the First Person perspective. Nowadays, this field is attracting attention and investments of companies aiming to develop commercial devices with First Person Vision recording capabilities. Due to this interest, an increasing demand of methods to process these videos, possibly in real-time, is expected. Current approaches present a particular combinations of different image features and quantitative methods to accomplish specific objectives like object detection, activity recognition, user machine interaction and so on. This paper summarizes the evolution of the state of the art in First Person Vision video analysis between 1997 and 2014, highlighting, among others, most commonly used features, methods, challenges and opportunities within the field.",
    "Link": "http://arxiv.org/abs/1409.1484v3",
    "PDF Link": "http://arxiv.org/pdf/1409.1484v3"
  },
  {
    "Title": "Deep Learning--Based Scene Simplification for Bionic Vision",
    "Authors": "Nicole Han, Sudhanshu Srivastava, Aiwen Xu, Devi Klein, Michael Beyeler",
    "Published": "2021-01-30T19:35:33Z",
    "Summary": "Retinal degenerative diseases cause profound visual impairment in more than 10 million people worldwide, and retinal prostheses are being developed to restore vision to these individuals. Analogous to cochlear implants, these devices electrically stimulate surviving retinal cells to evoke visual percepts (phosphenes). However, the quality of current prosthetic vision is still rudimentary. Rather than aiming to restore \"natural\" vision, there is potential merit in borrowing state-of-the-art computer vision algorithms as image processing techniques to maximize the usefulness of prosthetic vision. Here we combine deep learning--based scene simplification strategies with a psychophysically validated computational model of the retina to generate realistic predictions of simulated prosthetic vision, and measure their ability to support scene understanding of sighted subjects (virtual patients) in a variety of outdoor scenarios. We show that object segmentation may better support scene understanding than models based on visual saliency and monocular depth estimation. In addition, we highlight the importance of basing theoretical predictions on biologically realistic models of phosphene shape. Overall, this work has the potential to drastically improve the utility of prosthetic vision for people blinded from retinal degenerative diseases.",
    "Link": "http://arxiv.org/abs/2102.00297v1",
    "PDF Link": "http://arxiv.org/pdf/2102.00297v1"
  },
  {
    "Title": "DVHGNN: Multi-Scale Dilated Vision HGNN for Efficient Vision Recognition",
    "Authors": "Caoshuo Li, Tanzhe Li, Xiaobin Hu, Donghao Luo, Taisong Jin",
    "Published": "2025-03-19T03:45:23Z",
    "Summary": "Recently, Vision Graph Neural Network (ViG) has gained considerable attention in computer vision. Despite its groundbreaking innovation, Vision Graph Neural Network encounters key issues including the quadratic computational complexity caused by its K-Nearest Neighbor (KNN) graph construction and the limitation of pairwise relations of normal graphs. To address the aforementioned challenges, we propose a novel vision architecture, termed Dilated Vision HyperGraph Neural Network (DVHGNN), which is designed to leverage multi-scale hypergraph to efficiently capture high-order correlations among objects. Specifically, the proposed method tailors Clustering and Dilated HyperGraph Construction (DHGC) to adaptively capture multi-scale dependencies among the data samples. Furthermore, a dynamic hypergraph convolution mechanism is proposed to facilitate adaptive feature exchange and fusion at the hypergraph level. Extensive qualitative and quantitative evaluations of the benchmark image datasets demonstrate that the proposed DVHGNN significantly outperforms the state-of-the-art vision backbones. For instance, our DVHGNN-S achieves an impressive top-1 accuracy of 83.1% on ImageNet-1K, surpassing ViG-S by +1.0% and ViHGNN-S by +0.6%.",
    "Link": "http://arxiv.org/abs/2503.14867v1",
    "PDF Link": "http://arxiv.org/pdf/2503.14867v1"
  },
  {
    "Title": "Universal Object Detection with Large Vision Model",
    "Authors": "Feng Lin, Wenze Hu, Yaowei Wang, Yonghong Tian, Guangming Lu, Fanglin Chen, Yong Xu, Xiaoyu Wang",
    "Published": "2022-12-19T12:40:13Z",
    "Summary": "Over the past few years, there has been growing interest in developing a broad, universal, and general-purpose computer vision system. Such systems have the potential to address a wide range of vision tasks simultaneously, without being limited to specific problems or data domains. This universality is crucial for practical, real-world computer vision applications. In this study, our focus is on a specific challenge: the large-scale, multi-domain universal object detection problem, which contributes to the broader goal of achieving a universal vision system. This problem presents several intricate challenges, including cross-dataset category label duplication, label conflicts, and the necessity to handle hierarchical taxonomies. To address these challenges, we introduce our approach to label handling, hierarchy-aware loss design, and resource-efficient model training utilizing a pre-trained large vision model. Our method has demonstrated remarkable performance, securing a prestigious second-place ranking in the object detection track of the Robust Vision Challenge 2022 (RVC 2022) on a million-scale cross-dataset object detection benchmark. We believe that our comprehensive study will serve as a valuable reference and offer an alternative approach for addressing similar challenges within the computer vision community. The source code for our work is openly available at https://github.com/linfeng93/Large-UniDet.",
    "Link": "http://arxiv.org/abs/2212.09408v3",
    "PDF Link": "http://arxiv.org/pdf/2212.09408v3"
  },
  {
    "Title": "The Informed Sampler: A Discriminative Approach to Bayesian Inference in\n  Generative Computer Vision Models",
    "Authors": "Varun Jampani, Sebastian Nowozin, Matthew Loper, Peter V. Gehler",
    "Published": "2014-02-04T20:52:26Z",
    "Summary": "Computer vision is hard because of a large variability in lighting, shape, and texture; in addition the image signal is non-additive due to occlusion. Generative models promised to account for this variability by accurately modelling the image formation process as a function of latent variables with prior beliefs. Bayesian posterior inference could then, in principle, explain the observation. While intuitively appealing, generative models for computer vision have largely failed to deliver on that promise due to the difficulty of posterior inference. As a result the community has favoured efficient discriminative approaches. We still believe in the usefulness of generative models in computer vision, but argue that we need to leverage existing discriminative or even heuristic computer vision methods. We implement this idea in a principled way with an \"informed sampler\" and in careful experiments demonstrate it on challenging generative models which contain renderer programs as their components. We concentrate on the problem of inverting an existing graphics rendering engine, an approach that can be understood as \"Inverse Graphics\". The informed sampler, using simple discriminative proposals based on existing computer vision technology, achieves significant improvements of inference.",
    "Link": "http://arxiv.org/abs/1402.0859v3",
    "PDF Link": "http://arxiv.org/pdf/1402.0859v3"
  },
  {
    "Title": "A Comparative Study of Confidence Calibration in Deep Learning: From\n  Computer Vision to Medical Imaging",
    "Authors": "Riqiang Gao, Thomas Li, Yucheng Tang, Zhoubing Xu, Michael Kammer, Sanja L. Antic, Kim Sandler, Fabien Moldonado, Thomas A. Lasko, Bennett Landman",
    "Published": "2022-06-17T15:27:24Z",
    "Summary": "Although deep learning prediction models have been successful in the discrimination of different classes, they can often suffer from poor calibration across challenging domains including healthcare. Moreover, the long-tail distribution poses great challenges in deep learning classification problems including clinical disease prediction. There are approaches proposed recently to calibrate deep prediction in computer vision, but there are no studies found to demonstrate how the representative models work in different challenging contexts. In this paper, we bridge the confidence calibration from computer vision to medical imaging with a comparative study of four high-impact calibration models. Our studies are conducted in different contexts (natural image classification and lung cancer risk estimation) including in balanced vs. imbalanced training sets and in computer vision vs. medical imaging. Our results support key findings: (1) We achieve new conclusions which are not studied under different learning contexts, e.g., combining two calibration models that both mitigate the overconfident prediction can lead to under-confident prediction, and simpler calibration models from the computer vision domain tend to be more generalizable to medical imaging. (2) We highlight the gap between general computer vision tasks and medical imaging prediction, e.g., calibration methods ideal for general computer vision tasks may in fact damage the calibration of medical imaging prediction. (3) We also reinforce previous conclusions in natural image classification settings. We believe that this study has merits to guide readers to choose calibration models and understand gaps between general computer vision and medical imaging domains.",
    "Link": "http://arxiv.org/abs/2206.08833v1",
    "PDF Link": "http://arxiv.org/pdf/2206.08833v1"
  },
  {
    "Title": "Cybersecurity Dynamics",
    "Authors": "Shouhuai Xu",
    "Published": "2015-02-18T01:59:08Z",
    "Summary": "We explore the emerging field of {\\em Cybersecurity Dynamics}, a candidate foundation for the Science of Cybersecurity.",
    "Link": "http://arxiv.org/abs/1502.05100v1",
    "PDF Link": "http://arxiv.org/pdf/1502.05100v1"
  },
  {
    "Title": "Novel Approach for Cybersecurity Workforce Development: A Course in\n  Secure Design",
    "Authors": "Filipo Sharevski, Adam Trowbridge, Jessica Westbrook",
    "Published": "2018-06-04T16:39:46Z",
    "Summary": "Training the future cybersecurity workforce to respond to emerging threats requires introduction of novel educational interventions into the cybersecurity curriculum. To be effective, these interventions have to incorporate trending knowledge from cybersecurity and other related domains while allowing for experiential learning through hands-on experimentation. To date, the traditional interdisciplinary approach for cybersecurity training has infused political science, law, economics or linguistics knowledge into the cybersecurity curriculum, allowing for limited experimentation. Cybersecurity students were left with little opportunity to acquire knowledge, skills, and abilities in domains outside of these. Also, students in outside majors had no options to get into cybersecurity. With this in mind, we developed an interdisciplinary course for experiential learning in the fields of cybersecurity and interaction design. The inaugural course teaches students from cybersecurity, user interaction design, and visual design the principles of designing for secure use - or secure design - and allows them to apply them for prototyping of Internet-of-Things (IoT) products for smart homes. This paper elaborates on the concepts of secure design and how our approach enhances the training of the future cybersecurity workforce.",
    "Link": "http://arxiv.org/abs/1806.01198v1",
    "PDF Link": "http://arxiv.org/pdf/1806.01198v1"
  },
  {
    "Title": "Building a Resilient Cybersecurity Posture: A Framework for Leveraging\n  Prevent, Detect and Respond Functions and Law Enforcement Collaboration",
    "Authors": "Francesco Schiliro",
    "Published": "2023-03-20T05:16:54Z",
    "Summary": "This research paper proposes a framework for building a resilient cybersecurity posture that leverages prevent, detect, and respond functions and law enforcement collaboration. The Cybersecurity Resilience and Law Enforcement Collaboration (CyRLEC) Framework is designed to provide a comprehensive and integrated approach to cybersecurity that emphasizes collaboration with law enforcement agencies to mitigate cyber threats. The paper compares and contrasts the CyRLEC Framework with the NIST Cybersecurity Framework and highlights the critical differences between the two frameworks. While the NIST framework focuses on managing cybersecurity risk, the CyRLEC Framework takes a broader view of cybersecurity, including proactive prevention, early detection, rapid response to cyber-attacks, and close collaboration with law enforcement agencies to investigate and prosecute cybercriminals. The paper also provides a case study of a simulated real-world implementation of the CyRLEC Framework and evaluates its effectiveness in improving an organization's cybersecurity posture. The research findings demonstrate the value of the CyRLEC Framework in enhancing cybersecurity resilience and promoting effective collaboration with law enforcement agencies. Overall, this research paper contributes to the growing knowledge of cybersecurity frameworks and provides practical insights for organizations seeking to improve their cybersecurity posture.",
    "Link": "http://arxiv.org/abs/2303.10874v1",
    "PDF Link": "http://arxiv.org/pdf/2303.10874v1"
  },
  {
    "Title": "Cybersecurity Dynamics: A Foundation for the Science of Cybersecurity",
    "Authors": "Shouhuai Xu",
    "Published": "2020-10-09T04:09:43Z",
    "Summary": "Cybersecurity Dynamics is new concept that aims to achieve the modeling, analysis, quantification, and management of cybersecurity from a holistic perspective, rather than from a building-blocks perspective. It is centered at modeling and analyzing the attack-defense interactions in cyberspace, which cause a ``natural'' phenomenon -- the evolution of the global cybersecurity state. In this Chapter, we systematically introduce and review the Cybersecurity Dynamics foundation for the Science of Cybersecurity. We review the core concepts, technical approaches, research axes, and results that have been obtained in this endeavor. We outline a research roadmap towards the ultimate research goal, including a systematic set of technical barriers.",
    "Link": "http://arxiv.org/abs/2010.05683v1",
    "PDF Link": "http://arxiv.org/pdf/2010.05683v1"
  },
  {
    "Title": "Adopting the Cybersecurity Concepts into Curriculum The Potential\n  Effects on Students Cybersecurity Knowledge",
    "Authors": "Mohammad Azzeh, Ahmad Mousa Altamimi, Mahmood Albashayreh, Mohammad A AL-Oudat",
    "Published": "2022-09-12T16:00:51Z",
    "Summary": "This study examines the effect of adopting cybersecurity concepts on the IT curriculum and determines the potential effect on students' knowledge of cybersecurity practices and level of awareness. To this end, a pilot study was first conducted to measure the current level of cybersecurity awareness. The results revealed that students do not have much knowledge of Cybersecurity. Thus, a four-step approach was proposed to infuse the relevant cybersecurity topics in five matched courses based on the latest Cybersecurity curricular guidelines (CSEC2017). A sample of 42 students was selected purposively without prior knowledge of Cybersecurity and divided identically into experimental and control groups. Students in the experimental group were asked to take five consecutive courses over five semesters. In each course, groups went through a pre-test for the infused topics. Then, the experimental group taught the corresponding infused topics. A post-test was administered to both groups at the end of each course, and the t-test was conducted. The results found significant differences between marks of prior and post-tests for 11 out of 14 infused topics. These satisfactory results would encourage universities to infuse cybersecurity concepts into their curriculum",
    "Link": "http://arxiv.org/abs/2209.10407v1",
    "PDF Link": "http://arxiv.org/pdf/2209.10407v1"
  },
  {
    "Title": "Practical Cybersecurity Ethics: Mapping CyBOK to Ethical Concerns",
    "Authors": "Ivan Flechais, George Chalhoub",
    "Published": "2023-11-16T19:44:03Z",
    "Summary": "Research into the ethics of cybersecurity is an established and growing topic of investigation, however the translation of this research into practice is lacking: there exists a small number of professional codes of ethics or codes of practice in cybersecurity, however these are very broad and do not offer much insight into the ethical dilemmas that can be faced while performing specific cybersecurity activities. In order to address this gap, we leverage ongoing work on the Cyber Security Body of Knowledge (CyBOK) to help elicit and document the responsibilities and ethics of the profession. Based on a literature review of the ethics of cybersecurity, we use CyBOK to frame the exploration of ethical challenges in the cybersecurity profession through a series of 15 interviews with cybersecurity experts. Our approach is qualitative and exploratory, aiming to answer the research question \"What ethical challenges, insights, and solutions arise in different areas of cybersecurity?\". Our findings indicate that there are broad ethical challenges across the whole of cybersecurity, but also that different areas of cybersecurity can face specific ethical considerations for which more detailed guidance can help professionals in those areas. In particular, our findings indicate that security decision-making is expected of all security professionals, but that this requires them to balance a complex mix of technical, objective and subjective points of view, and that resolving conflicts raises challenging ethical dilemmas. We conclude that more work is needed to explore, map, and integrate ethical considerations into cybersecurity practice; the urgent need to conduct further research into the ethics of cybersecurity AI; and highlight the importance of this work for individuals and professional bodies who seek to develop and mature the cybersecurity profession in a responsible manner.",
    "Link": "http://arxiv.org/abs/2311.10165v1",
    "PDF Link": "http://arxiv.org/pdf/2311.10165v1"
  },
  {
    "Title": "Toward a Blockchain-based Platform to Manage Cybersecurity Certification\n  of IoT devices",
    "Authors": "Ricardo Neisse, José L. Hernández-Ramos, Sara N. Matheu, Gianmarco Baldini, Antonio Skarmeta",
    "Published": "2019-09-16T07:42:58Z",
    "Summary": "The goal of this paper is to propose a blockchain-based platform to enhance transparency and traceability of cybersecurity certification information motivated by the recently adopted EU Cybersecurity Act. The proposed platform is generic and intended to support the trusted exchange of cybersecurity certification information for any electronic product, service, or process. However, for the purposes of this paper, we focus on the case study of the cybersecurity certification of IoT devices, which are explicitly referenced in the recently adopted Cybersecurity Act as one of the main domains where it is highlighted the need for an increased level of trust.",
    "Link": "http://arxiv.org/abs/1909.07039v1",
    "PDF Link": "http://arxiv.org/pdf/1909.07039v1"
  },
  {
    "Title": "Assessing and Improving Cybersecurity Maturity for SMEs: Standardization\n  aspects",
    "Authors": "Bilge Yigit Ozkan, Marco Spruit",
    "Published": "2020-07-03T15:18:02Z",
    "Summary": "SMEs constitute a very large part of the economy in every country and they play an important role in economic growth and social development. SMEs are frequent targets of cybersecurity attacks similar to large enterprises. However, unlike large enterprises, SMEs mostly have limited capabilities regarding cybersecurity practices. Given the increasing cybersecurity risks and the large impact that the risks may bring to the SMEs, assessing and improving the cybersecurity capabilities is crucial for SMEs for sustainability. This research aims to provide an approach for SMEs for assessing and improving their cybersecurity capabilities by integrating key elements from existing industry standards.",
    "Link": "http://arxiv.org/abs/2007.01751v1",
    "PDF Link": "http://arxiv.org/pdf/2007.01751v1"
  },
  {
    "Title": "ICAR, a categorical framework to connect vulnerability, threat and asset\n  managements",
    "Authors": "Arnaud Valence",
    "Published": "2023-06-21T12:59:29Z",
    "Summary": "We present ICAR, a mathematical framework derived from category theory for representing cybersecurity NIST and MITRE's ontologies. Designed for cybersecurity, ICAR is a category whose objects are cybersecurity knowledge (weakness, vulnerability, impacted product, attack technique, etc.) and whose morphisms are relations between this knowledge, that make sense for cybersecurity. Within this rigorous and unified framework, we obtain a knowledge graph capable of identifying the attack and weakness structures of an IS, at the interface between description logics, database theory and cybersecurity. We then define ten cybersecurity queries to help understand the risks incurred by IS and organise their defence.",
    "Link": "http://arxiv.org/abs/2306.12240v1",
    "PDF Link": "http://arxiv.org/pdf/2306.12240v1"
  },
  {
    "Title": "Collaborative Approaches to Enhancing Smart Vehicle Cybersecurity by\n  AI-Driven Threat Detection",
    "Authors": "Syed Atif Ali, Salwa Din",
    "Published": "2024-12-31T04:08:42Z",
    "Summary": "The introduction sets the stage for exploring collaborative approaches to bolstering smart vehicle cybersecurity through AI-driven threat detection. As the automotive industry increasingly adopts connected and automated vehicles (CAVs), the need for robust cybersecurity measures becomes paramount. With the emergence of new vulnerabilities and security requirements, the integration of advanced technologies such as 5G networks, blockchain, and quantum computing presents promising avenues for enhancing CAV cybersecurity . Additionally, the roadmap for cybersecurity in autonomous vehicles emphasizes the importance of efficient intrusion detection systems and AI-based techniques, along with the integration of secure hardware, software stacks, and advanced threat intelligence to address cybersecurity challenges in future autonomous vehicles.",
    "Link": "http://arxiv.org/abs/2501.00261v1",
    "PDF Link": "http://arxiv.org/pdf/2501.00261v1"
  },
  {
    "Title": "Cybersecurity in the AWS Cloud",
    "Authors": "Michael Soltys",
    "Published": "2020-03-28T22:45:28Z",
    "Summary": "This paper re-examines the content of a standard advanced course in Cybersecurity from the perspective of Cloud Computing. More precisely, we review the core concepts of Cybersecurity, as presented in a senior undergraduate or graduate class, in light of the Amazon Web Services (AWS) cloud.",
    "Link": "http://arxiv.org/abs/2003.12905v1",
    "PDF Link": "http://arxiv.org/pdf/2003.12905v1"
  },
  {
    "Title": "Ten AI Stepping Stones for Cybersecurity",
    "Authors": "Ricardo Morla",
    "Published": "2019-12-14T09:54:36Z",
    "Summary": "With the turmoil in cybersecurity and the mind-blowing advances in AI, it is only natural that cybersecurity practitioners consider further employing learning techniques to help secure their organizations and improve the efficiency of their security operation centers. But with great fears come great opportunities for both the good and the evil, and a myriad of bad deals. This paper discusses ten issues in cybersecurity that hopefully will make it easier for practitioners to ask detailed questions about what they want from an AI system in their cybersecurity operations. We draw on the state of the art to provide factual arguments for a discussion on well-established AI in cybersecurity issues, including the current scope of AI and its application to cybersecurity, the impact of privacy concerns on the cybersecurity data that can be collected and shared externally to the organization, how an AI decision can be explained to the person running the operations center, and the implications of the adversarial nature of cybersecurity in the learning techniques. We then discuss the use of AI by attackers on a level playing field including several issues in an AI battlefield, and an AI perspective on the old cat-and-mouse game including how the adversary may assess your AI power.",
    "Link": "http://arxiv.org/abs/1912.06817v1",
    "PDF Link": "http://arxiv.org/pdf/1912.06817v1"
  },
  {
    "Title": "A Systems Thinking for Cybersecurity Modeling",
    "Authors": "Dingyu Yan",
    "Published": "2020-01-16T10:44:43Z",
    "Summary": "Solving cybersecurity issues requires a holistic understanding of components, factors, structures and their interactions in cyberspace, but conventional modeling approaches view the field of cybersecurity by their boundaries so that we are still not clear to cybersecurity and its changes. In this paper, we attempt to discuss the application of systems thinking approaches to cybersecurity modeling. This paper reviews the systems thinking approaches and provides the systems theories and methods for tackling cybersecurity challenges, regarding relevant fields, associated impact factors and their interactions. Moreover, an illustrative example of systems thinking frameworks for cybersecurity modeling is developed to help broaden the mind in methodology, theory, technology and practice. This article concludes that systems thinking can be considered as one of the powerful tools of cybersecurity modeling to find, characterize, understand, evaluate and predict cybersecurity.",
    "Link": "http://arxiv.org/abs/2001.05734v1",
    "PDF Link": "http://arxiv.org/pdf/2001.05734v1"
  },
  {
    "Title": "Classifying SMEs for Approaching Cybersecurity Competence and Awareness",
    "Authors": "Alireza Shojaifar, Heini Jarvinen",
    "Published": "2021-10-11T15:59:43Z",
    "Summary": "Cybersecurity is increasingly a concern for small and medium-sized enterprises (SMEs), and there exist many awareness training programs and tools for them. The literature mainly studies SMEs as a unitary type of company and provides one-size-fits-all recommendations and solutions. However, SMEs are not homogeneous. They are diverse with different vulnerabilities, cybersecurity needs, and competencies. Few studies considered such differences in standards and certificates for security tools adoption and cybersecurity tailoring for these SMEs. This study proposes a classification framework with an outline of cybersecurity improvement needs for each class. The framework suggests five SME types based on their characteristics and specific security needs: cybersecurity abandoned SME, unskilled SME, expert-connected SME, capable SME, and cybersecurity provider SME. In addition to describing the five classes, the study explains the framework's usage in sampled SMEs. The framework proposes solutions for each class to approach cybersecurity awareness and competence more consistent with SME needs. The final publication is available at ACM Digital Library via this https URL https://doi.org/10.1145/3465481.3469200",
    "Link": "http://arxiv.org/abs/2110.05370v1",
    "PDF Link": "http://arxiv.org/pdf/2110.05370v1"
  },
  {
    "Title": "A Review of Quantum Cybersecurity: Threats, Risks and Opportunities",
    "Authors": "Md Jobair Hossain Faruk, Sharaban Tahora, Masrura Tasnim, Hossain Shahriar, Nazmus Sakib",
    "Published": "2022-07-07T18:57:51Z",
    "Summary": "The promise of quantum computing is not speeding up conventional computing rather delivering an exponential advantage for certain classes of problems, with profound implications for cybersecurity for instance. With the advent and development of quantum computers, cyberspace security can surely become the most critical problem for the Internet in near future. On contrary, prosaic quantum technology can be promising to transform cybersecurity. This research aims to synthesize basic and fundamental studies concerning quantum cybersecurity that can be emerged both as a threat and solution to critical cybersecurity issues based on a systematic study. We provide a comprehensive, illustrative description of the current state-of-the-art quantum computing and cybersecurity and present the proposed approaches to date. Findings in quantum computing cybersecurity suggest that quantum computing can be adopted for the betterment of cybersecurity threats while it poses the most unexpected threats to cybersecurity. The focus and depth of this systematic survey not only provide quantum and cybersecurity practitioners and researchers with a consolidated body of knowledge about current trends in this area but also underpins a starting point for further research in this field.",
    "Link": "http://arxiv.org/abs/2207.03534v1",
    "PDF Link": "http://arxiv.org/pdf/2207.03534v1"
  },
  {
    "Title": "Ontological Approach toward Cybersecurity in Cloud Computing",
    "Authors": "Takeshi Takahashi, Youki Kadobayashi, Hiroyuki Fujiwara",
    "Published": "2014-04-01T08:10:42Z",
    "Summary": "Widespread deployment of the Internet enabled building of an emerging IT delivery model, i.e., cloud computing. Albeit cloud computing-based services have rapidly developed, their security aspects are still at the initial stage of development. In order to preserve cybersecurity in cloud computing, cybersecurity information that will be exchanged within it needs to be identified and discussed. For this purpose, we propose an ontological approach to cybersecurity in cloud computing. We build an ontology for cybersecurity operational information based on actual cybersecurity operations mainly focused on non-cloud computing. In order to discuss necessary cybersecurity information in cloud computing, we apply the ontology to cloud computing. Through the discussion, we identify essential changes in cloud computing such as data-asset decoupling and clarify the cybersecurity information required by the changes such as data provenance and resource dependency information.",
    "Link": "http://arxiv.org/abs/1405.6169v1",
    "PDF Link": "http://arxiv.org/pdf/1405.6169v1"
  },
  {
    "Title": "Emergent Behavior in Cybersecurity",
    "Authors": "Shouhuai Xu",
    "Published": "2015-02-18T02:05:02Z",
    "Summary": "We argue that emergent behavior is inherent to cybersecurity.",
    "Link": "http://arxiv.org/abs/1502.05102v1",
    "PDF Link": "http://arxiv.org/pdf/1502.05102v1"
  },
  {
    "Title": "Cybersecurity Cost of Quality: Managing the Costs of Cybersecurity Risk\n  Management",
    "Authors": "Nicole M. Radziwill, Morgan C. Benton",
    "Published": "2017-07-09T22:42:31Z",
    "Summary": "There is no standard yet for measuring and controlling the costs associated with implementing cybersecurity programs. To advance research and practice towards this end, we develop a mapping using the well-known concept of quality costs and the Framework Core within the Cybersecurity Framework produced by the National Institute of Standards and Technology (NIST) in response to the Cybersecurity Enhancement Act of 2014. This mapping can be easily adopted by organizations that are already using the NIST CSF for cybersecurity risk management to plan, manage, and continually improve cybersecurity operations. If an organization is not using the NIST CSF, this mapping may still be useful for linking elements in accounting systems that are associated with cybersecurity operations and risk management to a quality cost model.",
    "Link": "http://arxiv.org/abs/1707.02653v1",
    "PDF Link": "http://arxiv.org/pdf/1707.02653v1"
  },
  {
    "Title": "Explaining Cybersecurity with Films and the Arts (Extended Abstract)",
    "Authors": "Luca Viganò",
    "Published": "2019-05-05T18:47:11Z",
    "Summary": "Explaining Cybersecurity with Films and the Arts",
    "Link": "http://arxiv.org/abs/1905.01730v1",
    "PDF Link": "http://arxiv.org/pdf/1905.01730v1"
  },
  {
    "Title": "A Value Driven Framework for Cybersecurity Innovation in Transportation\n  & Infrastructure",
    "Authors": "Lampis Alevizos, Lalit Bhakuni, Stefan Jaschke",
    "Published": "2024-05-12T18:45:11Z",
    "Summary": "This paper introduces a value-driven cybersecurity innovation framework for the transportation and infrastructure sectors, as opposed to the traditional market-centric approaches that have dominated the field. Recontextualizing innovation categories into sustaining, incremental, disruptive, and transformative, we aim to foster a culture of self-innovation within organizations, enabling a strategic focus on cybersecurity measures that directly contribute to business value and strategic goals. This approach enhances operational effectiveness and efficiency of cyber defences primarily, while also aligns cybersecurity initiatives with mission-critical objectives. We detail a practical method for evaluating the business value of cybersecurity innovations and present a pragmatic approach for organizations to funnel innovative ideas in a structured and repeatable manner. The framework is designed to reinforce cybersecurity capabilities against an evolving cyber threat landscape while maintaining infrastructural integrity. Shifting the focus from general market appeal to sector-specific needs, our framework provides cybersecurity leaders with the strategic cyber-foresight necessary for prioritizing impactful initiatives, thereby making cybersecurity a core business enabler rather than a burden.",
    "Link": "http://arxiv.org/abs/2405.07358v1",
    "PDF Link": "http://arxiv.org/pdf/2405.07358v1"
  },
  {
    "Title": "Exploring the Cybersecurity-Resilience Gap: An Analysis of Student\n  Attitudes and Behaviors in Higher Education",
    "Authors": "Steve Goliath, Pitso Tsibolane, Dirk Snyman",
    "Published": "2024-11-05T16:09:37Z",
    "Summary": "Cyberattacks frequently target higher educational institutions, making cybersecurity awareness and resilience critical for students. However, limited research exists on cybersecurity awareness, attitudes, and resilience among students in higher education. This study addresses this gap using the Theory of Planned Behavior as a theoretical framework. A modified Human Aspects of Information Security Questionnaire was employed to gather 266 valid responses from undergraduate and postgraduate students at a South African higher education institution. Key dimensions of cybersecurity awareness and behavior, including password management, email usage, social media practices, and mobile device security, were assessed. A significant disparity in cybersecurity awareness and practices, with postgraduate students demonstrating superior performance across several dimensions was noted. This research postulates the existence of a Cybersecurity-Education Inflection Point during the transition to postgraduate studies, coined as the Cybersecurity-Resilience Gap. These concepts provide a foundation for developing targeted cybersecurity education initiatives in higher education, particularly highlighting the need for earlier intervention at the undergraduate level.",
    "Link": "http://arxiv.org/abs/2411.03219v1",
    "PDF Link": "http://arxiv.org/pdf/2411.03219v1"
  },
  {
    "Title": "A Review of Topological Data Analysis for Cybersecurity",
    "Authors": "Thomas Davies",
    "Published": "2022-02-16T13:03:52Z",
    "Summary": "In cybersecurity it is often the case that malicious or anomalous activity can only be detected by combining many weak indicators of compromise, any one of which may not raise suspicion when taken alone. The path that such indicators take can also be critical. This makes the problem of analysing cybersecurity data particularly well suited to Topological Data Analysis (TDA), a field that studies the high level structure of data using techniques from algebraic topology, both for exploratory analysis and as part of a machine learning workflow. By introducing TDA and reviewing the work done on its application to cybersecurity, we hope to highlight to researchers a promising new area with strong potential to improve cybersecurity data science.",
    "Link": "http://arxiv.org/abs/2202.08037v1",
    "PDF Link": "http://arxiv.org/pdf/2202.08037v1"
  },
  {
    "Title": "Cybersecurity Entity Alignment via Masked Graph Attention Networks",
    "Authors": "Yue Qin, Xiaojing Liao",
    "Published": "2022-07-04T14:19:32Z",
    "Summary": "Cybersecurity vulnerability information is often recorded by multiple channels, including government vulnerability repositories, individual-maintained vulnerability-gathering platforms, or vulnerability-disclosure email lists and forums. Integrating vulnerability information from different channels enables comprehensive threat assessment and quick deployment to various security mechanisms. Efforts to automatically gather such information, however, are impeded by the limitations of today's entity alignment techniques. In our study, we annotate the first cybersecurity-domain entity alignment dataset and reveal the unique characteristics of security entities. Based on these observations, we propose the first cybersecurity entity alignment model, CEAM, which equips GNN-based entity alignment with two mechanisms: asymmetric masked aggregation and partitioned attention. Experimental results on cybersecurity-domain entity alignment datasets demonstrate that CEAM significantly outperforms state-of-the-art entity alignment methods.",
    "Link": "http://arxiv.org/abs/2207.01434v1",
    "PDF Link": "http://arxiv.org/pdf/2207.01434v1"
  },
  {
    "Title": "Elicitation of SME Requirements for Cybersecurity Solutions by Studying\n  Adherence to Recommendations",
    "Authors": "Alireza Shojaifar, Samuel A. Fricker, Martin Gwerder",
    "Published": "2020-07-16T08:36:40Z",
    "Summary": "Small and medium-sized enterprises (SME) have become the weak spot of our economy for cyber attacks. These companies are large in number and often do not have the controls in place to prevent successful attacks, respectively are not prepared to systematically manage their cybersecurity capabilities. One of the reasons for why many SME do not adopt cybersecurity is that developers of cybersecurity solutions understand little the SME context and the requirements for successful use of these solutions. We elicit requirements by studying how cybersecurity experts provide advice to SME. The experts recommendations offer insights into what important capabilities of the solution are and how these capabilities ought to be used for mitigating cybersecurity threats. The adoption of a recommendation hints at a correct match of the solution, hence successful consideration of requirements. Abandoned recommendations point to a misalignment that can be used as a source to inquire missed requirements. Re-occurrence of adoption or abandonment decisions corroborate the presence of requirements. This poster describes the challenges of SME regarding cybersecurity and introduces our proposed approach to elicit requirements for cybersecurity solutions. The poster describes CYSEC, our tool used to capture cybersecurity advice and help to scale cybersecurity requirements elicitation to a large number of participating SME. We conclude by outlining the planned research to develop and validate CYSEC.",
    "Link": "http://arxiv.org/abs/2007.08177v1",
    "PDF Link": "http://arxiv.org/pdf/2007.08177v1"
  },
  {
    "Title": "An Assessment Methodology and Instrument for Cybersecurity: The Ireland\n  Use Case",
    "Authors": "Marco Alfano, Viviana Bastidas, Paul Heynen, Markus Helfert",
    "Published": "2023-02-10T10:47:29Z",
    "Summary": "Governments around the world are required to strengthen their national cybersecurity capabilities to respond effectively to the growing, changing, and sophisticated cyber threats and attacks, thus protecting society and the way of life as a whole. Responsible government institutions need to revise, evaluate, and bolster their national cybersecurity capabilities to fulfill the new requirements, for example regarding new trends affecting cybersecurity, key supporting laws and regulations, and implementations risk and challenges. This report presents a comprehensive assessment instrument for cybersecurity at the national level in order to help countries to ensure optimum response capability and more effective use of critical resources of each state. More precisely, the report - builds a common understanding of the critical cybersecurity capabilities and competence to be assessed at the national level, - adds value to national strategic planning and implementation which impact the development and adaptation of national cybersecurity strategies, - provides an overview of the assessment approaches at the national level, including capabilities, frameworks, and controls, - introduces a comprehensive cybersecurity instrument for countries to determine areas of improvement and develop enduring national capabilities, - describes how to apply the proposed national cybersecurity assessment framework in a real-world case, and - presents the results and lessons learned of the application of the assessment framework at the national level to assist governments in further building cybersecurity capabilities.",
    "Link": "http://arxiv.org/abs/2302.05166v1",
    "PDF Link": "http://arxiv.org/pdf/2302.05166v1"
  },
  {
    "Title": "Towards a Systematic View on Cybersecurity Ecology",
    "Authors": "Wojciech Mazurczyk, Szymon Drobniak, Sean Moore",
    "Published": "2015-05-15T21:32:38Z",
    "Summary": "Current network security systems are progressively showing their limitations. One credible estimate is that only about 45% of new threats are detected. Therefore it is vital to find a new direction that cybersecurity development should follow. We argue that the next generation of cybersecurity systems should seek inspiration in nature. This approach has been used before in the first generation of cybersecurity systems; however, since then cyber threats and environment have evolved significantly, and accordingly the first-generation systems have lost their effectiveness. A next generation of bio-inspired cybersecurity research is emerging, but progress is hindered by the lack of a framework for mapping biological security systems to their cyber analogies. In this paper, using terminology and concepts from biology, we describe a cybersecurity ecology and a framework that may be used to systematically research and develop bio-inspired cybersecurity.",
    "Link": "http://arxiv.org/abs/1505.04207v2",
    "PDF Link": "http://arxiv.org/pdf/1505.04207v2"
  },
  {
    "Title": "Collecting Indicators of Compromise from Unstructured Text of\n  Cybersecurity Articles using Neural-Based Sequence Labelling",
    "Authors": "Zi Long, Lianzhi Tan, Shengping Zhou, Chaoyang He, Xin Liu",
    "Published": "2019-07-04T02:54:02Z",
    "Summary": "Indicators of Compromise (IOCs) are artifacts observed on a network or in an operating system that can be utilized to indicate a computer intrusion and detect cyber-attacks in an early stage. Thus, they exert an important role in the field of cybersecurity. However, state-of-the-art IOCs detection systems rely heavily on hand-crafted features with expert knowledge of cybersecurity, and require large-scale manually annotated corpora to train an IOC classifier. In this paper, we propose using an end-to-end neural-based sequence labelling model to identify IOCs automatically from cybersecurity articles without expert knowledge of cybersecurity. By using a multi-head self-attention module and contextual features, we find that the proposed model is capable of gathering contextual information from texts of cybersecurity articles and performs better in the task of IOC identification. Experiments show that the proposed model outperforms other sequence labelling models, achieving the average F1-score of 89.0% on English cybersecurity article test set, and approximately the average F1-score of 81.8% on Chinese test set.",
    "Link": "http://arxiv.org/abs/1907.02636v2",
    "PDF Link": "http://arxiv.org/pdf/1907.02636v2"
  },
  {
    "Title": "Cybersecurity and Sustainable Development",
    "Authors": "Adam Sulich, Malgorzata Rutkowska, Agnieszka Krawczyk-Jezierska, Jaroslaw Jezierski, Tomasz Zema",
    "Published": "2021-05-28T07:58:46Z",
    "Summary": "Growing interdependencies between organizations lead them towards the creation of inter-organizational networks where cybersecurity and sustainable development have become one of the most important issues. The Environmental Goods and Services Sector (EGSS) is one of the fastest developing sectors of the economy fueled by the growing relationships between network entities based on ICT usage. In this sector, Green Cybersecurity is an emerging issue because it secures processes related directly and indirectly to environmental management and protection. In the future, the multidimensional development of the EGSS can help European Union to overcome the upcoming crises. At the same time, computer technologies and cybersecurity can contribute to the implementation of the concept of sustainable development. The development of environmental technologies along with their cybersecurity is one of the aims of the realization of sustainable production and domestic security concepts among the EU countries. Hence, the aim of this article is a theoretical discussion and research on the relationships between cybersecurity and sustainable development in inter-organizational networks. Therefore, the article is an attempt to give an answer to the question about the current state of the implementation of cybersecurity in relation to the EGSS part of the economy in different EU countries.",
    "Link": "http://arxiv.org/abs/2105.13652v1",
    "PDF Link": "http://arxiv.org/pdf/2105.13652v1"
  },
  {
    "Title": "A Model-Driven Methodology for Automotive Cybersecurity Test Case\n  Generation",
    "Authors": "Stefan Marksteiner, Peter Priller",
    "Published": "2021-07-13T12:23:18Z",
    "Summary": "Through international regulations (most prominently the latest UNECE regulation) and standards, the already widely perceived higher need for cybersecurity in automotive systems has been recognized and will mandate higher efforts for cybersecurity engineering. T he UNECE also demands the effectiveness of these engineering to be verified and validated through testing. T his requires both a significantly higher rate and more comprehensiveness of cybersecurity testing that is not effectively to cope with using current, predominantly manual, automotive cybersecurity testing techniques. To allow for comprehensive and efficient testing at all stages of the automotive life cycle, including supply chain parts not at band, and to facilitate efficient third party testing, as well as to test under real-world conditions, also methodologies for testing the cybersecurity of vehicular systems as a black box are necessary. T his paper therefore presents a model and attack tree-based approach to (semi-)automate automotive cybersecurity testing, as well as considerations for automatically black box-deriving models for the use in attack modeling.",
    "Link": "http://arxiv.org/abs/2107.06024v1",
    "PDF Link": "http://arxiv.org/pdf/2107.06024v1"
  },
  {
    "Title": "Understanding parents' perceptions of children's cybersecurity awareness\n  in Norway",
    "Authors": "Farzana Quayyum, Jonas Bueie, Daniela S. Cruzes, Letizia Jaccheri, Juan Carlos Torrado Vidal",
    "Published": "2021-08-05T10:42:49Z",
    "Summary": "Children are increasingly using the internet nowadays. While internet use exposes children to various privacy and security risks, few studies have examined how parents perceive and address their children's cybersecurity risks. To address this gap, we conducted a qualitative study with 25 parents living in Norway with children aged between 10 to 15. We conducted semi-structured interviews with the parents and performed a thematic analysis of the interview data. The results of this paper include a list of cybersecurity awareness needs for children from a parental perspective, a list of learning resources for children, and a list of challenges for parents to ensure cybersecurity at home. Our results are useful for developers and educators in developing cybersecurity solutions for children. Future research should focus on defining cybersecurity theories and practices that contribute to children's and parents' awareness about cybersecurity risks, needs, and solutions.",
    "Link": "http://arxiv.org/abs/2108.02512v1",
    "PDF Link": "http://arxiv.org/pdf/2108.02512v1"
  },
  {
    "Title": "Multidimensional Cybersecurity Framework for Strategic Foresight",
    "Authors": "Cyril Onwubiko, Karim Ouazzane",
    "Published": "2022-02-05T12:30:31Z",
    "Summary": "Cybersecurity is now at the forefront of most organisational digital transformative agendas and National economic, social and political programmes. Hence its impact to society can no longer be seen to be one dimensional. The rise in National cybersecurity laws and regulations is a good indicator of its perceived importance to nations. And the recent awakening for social and ethical transparency in society and coupled with sustainability issues demonstrate the need for a paradigm shift in how cybersecurity discourses can now happen. In response to this shift, a multidimensional cybersecurity framework for strategic foresight underpinned on situational awareness is proposed. The conceptual cybersecurity framework comprising six domains such as Physical, Cultural, Economic, Social, Political and Cyber, is discussed. The guiding principles underpinning the framework are outlined, followed by in-depth reflection on the Business, Operational, Technological and Human (BOTH) factors and their implications for strategic foresight for cybersecurity.",
    "Link": "http://arxiv.org/abs/2202.02537v1",
    "PDF Link": "http://arxiv.org/pdf/2202.02537v1"
  },
  {
    "Title": "The Future of Cybersecurity in Southeast Asia along the Maritime Silk\n  Road",
    "Authors": "Roberto Dillon",
    "Published": "2023-08-14T06:37:15Z",
    "Summary": "This paper proposes an analysis of the prospects of the cyber security industry and educational ecosystems in four Southeast Asian countries, namely Vietnam, Singapore, Malaysia, and Indonesia, which are along the Maritime Silk Road, by using two novel metrics: the \"Cybersecurity Education Prospects Index\" (CEPI) and the \"Cybersecurity Industry Prospects Index\" (CIPI). The CEPI evaluates the state of cybersecurity education by assessing the availability and quality of cybersecurity degrees together with their ability to attract new students. On the other hand, the CIPI measures the potential for the cybersecurity industry's growth and development by assessing the talent pool needed to build and sustain its growth. Ultimately, this study emphasizes the vital importance of a healthy cybersecurity ecosystem where education is responsible for supporting the industry to ensure the security and reliability of commercial operations in these countries against a complex and evolving cyber threat landscape.",
    "Link": "http://arxiv.org/abs/2308.06963v1",
    "PDF Link": "http://arxiv.org/pdf/2308.06963v1"
  },
  {
    "Title": "Unaware, Unfunded and Uneducated: A Systematic Review of SME\n  Cybersecurity",
    "Authors": "Carlos Rombaldo Junior, Ingolf Becker, Shane Johnson",
    "Published": "2023-09-29T12:32:49Z",
    "Summary": "Small and Medium Enterprises (SMEs) are pivotal in the global economy, accounting for over 90% of businesses and 60% of employment worldwide. Despite their significance, SMEs have been disregarded from cybersecurity initiatives, rendering them ill-equipped to deal with the growing frequency, sophistication, and destructiveness of cyber-attacks. We systematically reviewed the cybersecurity literature on SMEs published between 2017 and 2023.   We focus on research discussing cyber threats, adopted controls, challenges, and constraints SMEs face in pursuing cybersecurity resilience.   Our search yielded 916 studies that we narrowed to 77 relevant papers. We identified 44 unique themes and categorised them as novel findings or established knowledge. This distinction revealed that research on SMEs is shallow and has made little progress in understanding SMEs' roles, threats, and needs. Studies often repeated early discoveries without replicating or offering new insights.   The existing research indicates that the main challenges to attaining cybersecurity resilience of SMEs are a lack of awareness of the cybersecurity risks, limited cybersecurity literacy and constrained financial resources. However, resource availability varied between developed and developing countries. Our analysis indicated a relationship among these themes, suggesting that limited literacy is the root cause of awareness and resource constraint issues.",
    "Link": "http://arxiv.org/abs/2309.17186v1",
    "PDF Link": "http://arxiv.org/pdf/2309.17186v1"
  },
  {
    "Title": "Data Driven Approaches to Cybersecurity Governance for Board\n  Decision-Making -- A Systematic Review",
    "Authors": "Anita Modi, Ievgeniia Kuzminykh, Bogdan Ghita",
    "Published": "2023-11-29T12:14:01Z",
    "Summary": "Cybersecurity governance influences the quality of strategic decision-making to ensure cyber risks are managed effectively. Board of Directors are the decisions-makers held accountable for managing this risk; however, they lack adequate and efficient information necessary for making such decisions. In addition to the myriad of challenges they face, they are often insufficiently versed in the technology or cybersecurity terminology or not provided with the correct tools to support them to make sound decisions to govern cybersecurity effectively. A different approach is needed to ensure BoDs are clear on the approach the business is taking to build a cyber resilient organization. This systematic literature review investigates the existing risk measurement instruments, cybersecurity metrics, and associated models for supporting BoDs. We identified seven conceptual themes through literature analysis that form the basis of this study's main contribution. The findings showed that, although sophisticated cybersecurity tools exist and are developing, there is limited information for Board of Directors to support them in terms of metrics and models to govern cybersecurity in a language they understand. The review also provides some recommendations on theories and models that can be further investigated to provide support to Board of Directors.",
    "Link": "http://arxiv.org/abs/2311.17578v1",
    "PDF Link": "http://arxiv.org/pdf/2311.17578v1"
  },
  {
    "Title": "Designing Cybersecurity Awareness Solutions for the Young People in\n  Rural Developing Countries: The Need for Diversity and Inclusion",
    "Authors": "Farzana Quayyum, Giske Naper Freberg",
    "Published": "2023-12-19T11:42:56Z",
    "Summary": "Cybersecurity challenges and the need for awareness are well-recognized in developed countries, but this still needs attention in less-developed countries. With the expansion of technology, security concerns are also becoming more prevalent worldwide. This paper presents a design and creation research study exploring which factors we should consider when designing cybersecurity awareness solutions for young people in developing countries. We have developed prototypes of mini-cybersecurity awareness applications and conducted a pilot study with eight participants (aged 16-30) from Gambia, Eritrea, and Syria. Our findings show that factors like the influence of culture and social constructs, literacy, and language competence, the way of introducing cybersecurity terms and concepts, and the need for reflection are essential to consider when designing and developing cybersecurity awareness solutions for target users in developing countries. The findings of this study will guide future researchers to design more inclusive cybersecurity awareness solutions for users in developing countries.",
    "Link": "http://arxiv.org/abs/2312.12073v1",
    "PDF Link": "http://arxiv.org/pdf/2312.12073v1"
  },
  {
    "Title": "Toward a Quantum Information System Cybersecurity Taxonomy and Testbed:\n  Exploiting a Unique Opportunity for Early Impact",
    "Authors": "Benjamin Blakely, Joaquin Chung, Alec Poczatek, Ryan Syed, Raj Kettimuthu",
    "Published": "2024-04-18T18:56:21Z",
    "Summary": "Any human-designed system can potentially be exploited in ways that its designers did not envision, and information systems or networks using quantum components do not escape this reality. We are presented with a unique but quickly waning opportunity to bring cybersecurity concerns to the forefront for quantum information systems before they become widely deployed. The resources and knowledge required to do so, however, may not be common in the cybersecurity community. Yet, a nexus exist. Cybersecurity starts with risk, and there are good taxonomies for security vulnerabilities and impacts in classical systems. In this paper, we propose a preliminary taxonomy for quantum cybersecurity vulnerabilities that accounts for the latest advances in quantum information systems, and must evolve to incorporate well-established cybersecurity principles and methodologies. We envision a testbed environment designed and instrumented with the specific purpose of enabling a broad collaborative community of cybersecurity and quantum information system experts to conduct experimental evaluation of software and hardware security including both physical and virtual quantum components. Furthermore, we envision that such a resource may be available as a user facility to the open science research community.",
    "Link": "http://arxiv.org/abs/2404.12465v1",
    "PDF Link": "http://arxiv.org/pdf/2404.12465v1"
  },
  {
    "Title": "When LLMs Meet Cybersecurity: A Systematic Literature Review",
    "Authors": "Jie Zhang, Haoyu Bu, Hui Wen, Yongji Liu, Haiqiang Fei, Rongrong Xi, Lun Li, Yun Yang, Hongsong Zhu, Dan Meng",
    "Published": "2024-05-06T17:07:28Z",
    "Summary": "The rapid development of large language models (LLMs) has opened new avenues across various fields, including cybersecurity, which faces an evolving threat landscape and demand for innovative technologies. Despite initial explorations into the application of LLMs in cybersecurity, there is a lack of a comprehensive overview of this research area. This paper addresses this gap by providing a systematic literature review, covering the analysis of over 300 works, encompassing 25 LLMs and more than 10 downstream scenarios. Our comprehensive overview addresses three key research questions: the construction of cybersecurity-oriented LLMs, the application of LLMs to various cybersecurity tasks, the challenges and further research in this area. This study aims to shed light on the extensive potential of LLMs in enhancing cybersecurity practices and serve as a valuable resource for applying LLMs in this field. We also maintain and regularly update a list of practical guides on LLMs for cybersecurity at https://github.com/tmylla/Awesome-LLM4Cybersecurity.",
    "Link": "http://arxiv.org/abs/2405.03644v2",
    "PDF Link": "http://arxiv.org/pdf/2405.03644v2"
  },
  {
    "Title": "Gender of Recruiter Makes a Difference: A study into Cybersecurity\n  Graduate Recruitment",
    "Authors": "Joanne L. Hall, Asha Rao",
    "Published": "2024-08-12T02:18:27Z",
    "Summary": "An ever-widening workforce gap exists in the global cybersecurity industry but diverse talent is underutilized. The global cybersecurity workforce is only 25% female. Much research exists on the effect of gender bias on the hiring of women into the technical workforce, but little on how the gender of the recruiter (gender difference) affects recruitment decisions. This research reveals differences between the non-technical skills sought by female vs non-female cybersecurity recruiters. The former look for recruits with people-focused skills while the latter look for task-focused skills, highlighting the need for gender diversity in recruitment panels.   Recruiters are increasingly seeking non-technical (soft) skills in technical graduate recruits. This requires STEM curriculum in Universities to adapt to match. Designing an industry-ready cybersecurity curriculum requires knowledge of these non-technical skills. An online survey of cybersecurity professionals was used to determine the most sought after non-technical skills in the field. Analysis of the data reveals distinct gender differences in the non-technical skills most valued in a recruit, based on the gender of the recruiter (not the recruited). The gender differences discovered do not correspond to the higher proportion of women employed in non-technical cybersecurity roles.",
    "Link": "http://arxiv.org/abs/2408.05895v1",
    "PDF Link": "http://arxiv.org/pdf/2408.05895v1"
  },
  {
    "Title": "SoK: Identifying Limitations and Bridging Gaps of Cybersecurity\n  Capability Maturity Models (CCMMs)",
    "Authors": "Lasini Liyanage, Nalin Asanka Gamagedara Arachchilage, Giovanni Russello",
    "Published": "2024-08-28T21:00:20Z",
    "Summary": "In the rapidly evolving digital landscape, where organisations are increasingly vulnerable to cybersecurity threats, Cybersecurity Capability Maturity Models (CCMMs) emerge as pivotal tools in enhancing organisational cybersecurity posture. CCMMs provide a structured framework to guide organisations in assessing their current cybersecurity capabilities, identifying critical gaps, and prioritising improvements. However, the full potential of CCMMs is often not realised due to inherent limitations within the models and challenges encountered during their implementation and adoption processes. These limitations and challenges can significantly hamper the efficacy of CCMMs in improving cybersecurity. As a result, organisations remain vulnerable to cyber threats as they may fail to identify and address critical security gaps, implement necessary improvements or allocate resources effectively. To address these limitations and challenges, conducting a thorough investigation into existing models is essential. Therefore, we conducted a Systematic Literature Review (SLR) analysing 43 publications to identify existing CCMMs, their limitations, and the challenges organisations face when implementing and adopting them. By understanding these barriers, we aim to explore avenues for enhancing the efficacy of CCMMs, ensuring they more effectively meet the cybersecurity needs of organisational entities.",
    "Link": "http://arxiv.org/abs/2408.16140v1",
    "PDF Link": "http://arxiv.org/pdf/2408.16140v1"
  },
  {
    "Title": "A Survey-Based Quantitative Analysis of Stress Factors and Their Impacts\n  Among Cybersecurity Professionals",
    "Authors": "Sunil Arora, John D. Hastings",
    "Published": "2024-09-18T15:18:33Z",
    "Summary": "This study investigates the prevalence and underlying causes of work-related stress and burnout among cybersecurity professionals using a quantitative survey approach guided by the Job Demands-Resources model. Analysis of responses from 50 cybersecurity practitioners reveals an alarming reality: 44% report experiencing severe work-related stress and burnout, while an additional 28% are uncertain about their condition. The demanding nature of cybersecurity roles, unrealistic expectations, and unsupportive organizational cultures emerge as primary factors fueling this crisis. Notably, 66% of respondents perceive cybersecurity jobs as more stressful than other IT positions, with 84% facing additional challenges due to the pandemic and recent high-profile breaches. The study finds that most cybersecurity experts are reluctant to report their struggles to management, perpetuating a cycle of silence and neglect. To address this critical issue, the paper recommends that organizations foster supportive work environments, implement mindfulness programs, and address systemic challenges. By prioritizing the mental health of cybersecurity professionals, organizations can cultivate a more resilient and effective workforce to protect against an ever-evolving threat landscape.",
    "Link": "http://arxiv.org/abs/2409.12047v1",
    "PDF Link": "http://arxiv.org/pdf/2409.12047v1"
  },
  {
    "Title": "Cybersecurity Study Programs: What's in a Name?",
    "Authors": "Jan Vykopal, Valdemar Švábenský, Michael Tuscano Lopez II, Pavel Čeleda",
    "Published": "2024-11-14T07:14:52Z",
    "Summary": "Improving cybersecurity education has become a priority for many countries and organizations worldwide. Computing societies and professional associations have recognized cybersecurity as a distinctive computing discipline and created specialized cybersecurity curricular guidelines. Higher education institutions are introducing new cybersecurity programs, attracting students to this expanding field. In this paper, we examined 101 study programs across 24 countries. Based on their analysis, we argue that top-ranked universities have not yet fully implemented the guidelines and offer programs that have \"cyber\" in their name but lack some essential elements of a cybersecurity program. In particular, most programs do not sufficiently cover non-technical components, such as law, policies, or risk management. Also, most programs teach knowledge and skills but do not expose students to experiential learning outside the traditional classroom (such as internships) to develop their competencies. As a result, graduates of these programs may not meet employer expectations and may require additional training. To help program directors and educators improve their programs and courses, this paper offers examples of effective practices from cybersecurity programs around the world and our teaching practice.",
    "Link": "http://arxiv.org/abs/2411.09240v1",
    "PDF Link": "http://arxiv.org/pdf/2411.09240v1"
  },
  {
    "Title": "Systematic Review of Cybersecurity in Banking: Evolution from\n  Pre-Industry 4.0 to Post-Industry 4.0 in Artificial Intelligence, Blockchain,\n  Policies and Practice",
    "Authors": "Tue Nhi Tran",
    "Published": "2025-02-27T14:17:06Z",
    "Summary": "Throughout the history from pre-industry 4.0 to post-industry 4.0, cybersecurity at banks has undergone significant changes. Pre-industry 4.0 cyber security at banks relied on individual security methods that were highly manual and had low accuracy. When moving to post-industry 4.0, cybersecurity at banks had a major turning point with security methods that combined different technologies such as Artificial Intelligence (AI), Blockchain, IoT, automating necessary processes and significantly increasing the defence layer for banks. However, along with the development of new technologies, the current challenge of cybersecurity at banks lies in scalability, high costs and resources in both money and time for R&D of defence methods along with the threat of high-tech cybercriminals growing and expanding. This report goes from introducing the importance of cybersecurity at banks, analyzing their management, operational and business objectives, evaluating pre-industry 4.0 technologies used for cybersecurity at banks to assessing post-industry 4.0 technologies focusing on Artificial Intelligence and Blockchain, discussing current policies and practices and ending with discussing key advantages and challenges for 4.0 technologies and recommendations for further developing cybersecurity at banks.",
    "Link": "http://arxiv.org/abs/2503.00070v1",
    "PDF Link": "http://arxiv.org/pdf/2503.00070v1"
  },
  {
    "Title": "SecureBERT: A Domain-Specific Language Model for Cybersecurity",
    "Authors": "Ehsan Aghaei, Xi Niu, Waseem Shadid, Ehab Al-Shaer",
    "Published": "2022-04-06T09:17:21Z",
    "Summary": "Natural Language Processing (NLP) has recently gained wide attention in cybersecurity, particularly in Cyber Threat Intelligence (CTI) and cyber automation. Increased connection and automation have revolutionized the world's economic and cultural infrastructures, while they have introduced risks in terms of cyber attacks. CTI is information that helps cybersecurity analysts make intelligent security decisions, that is often delivered in the form of natural language text, which must be transformed to machine readable format through an automated procedure before it can be used for automated security measures.   This paper proposes SecureBERT, a cybersecurity language model capable of capturing text connotations in cybersecurity text (e.g., CTI) and therefore successful in automation for many critical cybersecurity tasks that would otherwise rely on human expertise and time-consuming manual efforts. SecureBERT has been trained using a large corpus of cybersecurity text.To make SecureBERT effective not just in retaining general English understanding, but also when applied to text with cybersecurity implications, we developed a customized tokenizer as well as a method to alter pre-trained weights. The SecureBERT is evaluated using the standard Masked Language Model (MLM) test as well as two additional standard NLP tasks. Our evaluation studies show that SecureBERT\\footnote{\\url{https://github.com/ehsanaghaei/SecureBERT}} outperforms existing similar models, confirming its capability for solving crucial NLP tasks in cybersecurity.",
    "Link": "http://arxiv.org/abs/2204.02685v3",
    "PDF Link": "http://arxiv.org/pdf/2204.02685v3"
  },
  {
    "Title": "Systemization of Knowledge (SoK)- Cross Impact of Transfer Learning in\n  Cybersecurity: Offensive, Defensive and Threat Intelligence Perspectives",
    "Authors": "Sofiya Makar, Ali Dehghantanha, Fattane Zarrinkalam, Gautam Srivastava, Abbas Yazdinejad",
    "Published": "2023-09-12T00:26:38Z",
    "Summary": "Recent literature highlights a significant cross-impact between transfer learning and cybersecurity. Many studies have been conducted on using transfer learning to enhance security, leading to various applications in different cybersecurity tasks. However, previous research is focused on specific areas of cybersecurity. This paper presents a comprehensive survey of transfer learning applications in cybersecurity by covering a wide range of domains, identifying current trends, and shedding light on under-explored areas. The survey highlights the significance of transfer learning in addressing critical issues in cybersecurity, such as improving detection accuracy, reducing training time, handling data imbalance, and enhancing privacy preservation. Additional insights are provided on the common problems solved using transfer learning, such as the lack of labeled data, different data distributions, and privacy concerns. The paper identifies future research directions and challenges that require community attention, including the need for privacy-preserving models, automatic tools for knowledge transfer, metrics for measuring domain relatedness, and enhanced privacy preservation mechanisms. The insights and roadmap presented in this paper will guide researchers in further advancing transfer learning in cybersecurity, fostering the development of robust and efficient cybersecurity systems to counter emerging threats and protect sensitive information. To the best of our knowledge, this paper is the first of its kind to present a comprehensive taxonomy of all areas of cybersecurity that benefited from transfer learning and propose a detailed future roadmap to shape the possible research direction in this area.",
    "Link": "http://arxiv.org/abs/2309.05889v1",
    "PDF Link": "http://arxiv.org/pdf/2309.05889v1"
  },
  {
    "Title": "A Transdisciplinary Approach to Cybersecurity: A Framework for\n  Encouraging Transdisciplinary Thinking",
    "Authors": "Emily Kesler",
    "Published": "2024-05-16T18:12:50Z",
    "Summary": "Classical cybersecurity is often perceived as a rigid science discipline filled with computer scientists and mathematicians. However, due to the rapid pace of technology development and integration, new criminal enterprises, new defense tactics, and the understanding of the human element, cybersecurity is quickly beginning to encompass more than just computers. Cybersecurity experts must broaden their perspectives beyond traditional disciplinary boundaries to provide the best protection possible. They must start to practice transdisciplinary cybersecurity. Taking influence from the Stakeholder Theory in business ethics, this paper presents a framework to encourage transdisciplinary thinking and assist experts in tackling the new challenges of the modern day. The framework uses the simple Think, Plan, Do approach to enable experts to develop their transdisciplinary thinking. The framework is intended to be used as an evaluation tool for existing cybersecurity practices or postures, as a development tool to engage with other disciplines to foster learning and create new methods, and as a guidance tool to encourage new ways of thinking about, perceiving, and executing cybersecurity practices. For each of those intended uses, a use case is presented as an example to showcase how the framework might be used. The ultimate goal of this paper is not the framework but transdisciplinary thinking. By using the tool presented here and developing their own transdisciplinary thinking, cybersecurity experts can be better prepared to face cybersecurity's unique and complex challenges.",
    "Link": "http://arxiv.org/abs/2405.10373v1",
    "PDF Link": "http://arxiv.org/pdf/2405.10373v1"
  },
  {
    "Title": "Navigating the road to automotive cybersecurity compliance",
    "Authors": "Franco Oberti, Fabrizio Abrate, Alessandro Savino, Filippo Parisi, Stefano Di Carlo",
    "Published": "2024-06-29T16:07:48Z",
    "Summary": "The automotive industry has evolved significantly since the introduction of the Ford Model T in 1908. Today's vehicles are not merely mechanical constructs; they are integral components of a complex digital ecosystem, equipped with advanced connectivity features powered by Artificial Intelligence and cloud computing technologies. This evolution has enhanced vehicle safety, efficiency, and the overall driving experience. However, it also introduces new challenges, notably in cybersecurity.   With the increasing integration of digital technologies, vehicles have become more susceptible to cyber-attacks, prompting significant cybersecurity concerns. These concerns include securing sensitive data, protecting vehicles from unauthorized access, and ensuring user privacy. In response, the automotive industry is compelled to adopt robust cybersecurity measures to safeguard both vehicles and data against potential threats.   Legislative frameworks such as UNR155 and UNR156 by the United Nations, along with other international regulations, aim to establish stringent cybersecurity mandates. These regulations require compliance with comprehensive cybersecurity management systems and necessitate regular updates and testing to cope with the evolving nature of cyber threats. The introduction of such regulations highlights the growing recognition of cybersecurity as a critical component of automotive safety and functionality.   The future of automotive cybersecurity lies in the continuous development of advanced protective measures and collaborative efforts among all stakeholders, including manufacturers, policymakers, and cybersecurity professionals. Only through such concerted efforts can the industry hope to address the dual goals of innovation in vehicle functionality and stringent security measures against the backdrop of an increasingly interconnected digital landscape.",
    "Link": "http://arxiv.org/abs/2407.00483v1",
    "PDF Link": "http://arxiv.org/pdf/2407.00483v1"
  },
  {
    "Title": "Assessing the Maturity of Cybersecurity Education in Virginia and the\n  Impact of State Level Investment",
    "Authors": "Patrick Mero, Aaron Pepsin, Chris Kreider",
    "Published": "2025-02-25T18:55:21Z",
    "Summary": "With a global shortage of cybersecurity students with the education and experience necessary to fill more than 3 million jobs, cybersecurity education is an international problem. Significant research within this field has explored this problem in depth, identifying a variety of shortcomings in the cybersecurity educational pipeline including lack of certifications, security clearances, and appropriate educational opportunities within institutions of higher education. Additional research has built on this, exploring specific gaps within what cybersecurity opportunities are provided within institutions of higher education. We build an ordinal scale for assessing this, the cybersecurity education maturity model scale (CEMMs), and provide evidence of reliability and validity. We then calculate the CEMMs score for all public four-year universities in the state of Virginia between 2017 and 2025, with 2017 marking a year in which the state started the Commonwealth Cyber Initiative (CCI). We find that the scale proposed provides a consistent and reliable way to compare the cybersecurity offerings available between universities. When comparing year to year average CEMMs score, we find that public four year universities in Virginia are increasing their program offerings in the area of cybersecurity, with potential to make an impact on the cybersecurity jobs gap.",
    "Link": "http://arxiv.org/abs/2502.18456v1",
    "PDF Link": "http://arxiv.org/pdf/2502.18456v1"
  },
  {
    "Title": "Cybersecurity Awareness",
    "Authors": "Jason R. C. Nurse",
    "Published": "2021-02-28T11:54:58Z",
    "Summary": "Cybersecurity awareness can be viewed as the level of appreciation, understanding or knowledge of cybersecurity or information security aspects. Such aspects include cognizance of cyber risks and threats, but also appropriate protection measures.",
    "Link": "http://arxiv.org/abs/2103.00474v1",
    "PDF Link": "http://arxiv.org/pdf/2103.00474v1"
  },
  {
    "Title": "A systematic literature review on Ransomware attacks",
    "Authors": "Shweta Vasoya, Krishna Bhavsar, Nishtha Patel",
    "Published": "2022-12-08T04:09:40Z",
    "Summary": "In the area of information technology, cybersecurity is critical. Information security is one of todays highest priorities. Cyber attacks, which are on the rise and include Ransomware, are the first thing that springs to mind when we think about cybersecurity. To counteract cybercrime, several governments and companies employ a range of strategies. Despite several cybersecurity measures, ransomware continues to terrify people.",
    "Link": "http://arxiv.org/abs/2212.04063v1",
    "PDF Link": "http://arxiv.org/pdf/2212.04063v1"
  },
  {
    "Title": "Space Cybersecurity Norms",
    "Authors": "Peter Sharfman, Samuel Sanders Visner",
    "Published": "2023-06-12T21:55:20Z",
    "Summary": "This paper addresses: Evolution of the space systems environment, including space system proliferation and space systems as critical infrastructure Cyber threats to, and vulnerabilities of, space systems Alternative approaches to meeting these threats, and the significance of norms Approaches to the development and reinforcement of norms for the cybersecurity of space systems.",
    "Link": "http://arxiv.org/abs/2306.07441v1",
    "PDF Link": "http://arxiv.org/pdf/2306.07441v1"
  },
  {
    "Title": "Considerations for Cloud Security Operations",
    "Authors": "James Cusick",
    "Published": "2016-01-23T17:08:22Z",
    "Summary": "Information Security in Cloud Computing environments is explored. Cloud Computing is presented, security needs are discussed, and mitigation approaches are listed. Topics covered include Information Security, Cloud Computing, Private Cloud, Public Cloud, SaaS, PaaS, IaaS, ISO 27001, OWASP, Secure SDLC.",
    "Link": "http://arxiv.org/abs/1601.06289v1",
    "PDF Link": "http://arxiv.org/pdf/1601.06289v1"
  },
  {
    "Title": "Security of Cloud FPGAs: A Survey",
    "Authors": "Chenglu Jin, Vasudev Gohil, Ramesh Karri, Jeyavijayan Rajendran",
    "Published": "2020-05-11T05:31:15Z",
    "Summary": "Integrating Field Programmable Gate Arrays (FPGAs) with cloud computing instances is a rapidly emerging trend on commercial cloud computing platforms such as Amazon Web Services (AWS), Huawei cloud, and Alibaba cloud. Cloud FPGAs allow cloud users to build hardware accelerators to speed up the computation in the cloud. However, since the cloud FPGA technology is still in its infancy, the security implications of this integration of FPGAs in the cloud are not clear. In this paper, we survey the emerging field of cloud FPGA security, providing a comprehensive overview of the security issues related to cloud FPGAs, and highlighting future challenges in this research area.",
    "Link": "http://arxiv.org/abs/2005.04867v1",
    "PDF Link": "http://arxiv.org/pdf/2005.04867v1"
  },
  {
    "Title": "Interoperability and Standardization of Intercloud Cloud Computing",
    "Authors": "Jingxin K. Wang, Jianrui Ding, Tian Niu",
    "Published": "2012-12-24T19:24:35Z",
    "Summary": "Cloud computing is getting mature, and the interoperability and standardization of the clouds is still waiting to be solved. This paper discussed the interoperability among clouds about message transmission, data transmission and virtual machine transfer. Starting from IEEE Pioneering Cloud Computing Initiative, this paper discussed about standardization of the cloud computing, especially intercloud cloud computing. This paper also discussed the standardization from the market-oriented view.",
    "Link": "http://arxiv.org/abs/1212.5956v1",
    "PDF Link": "http://arxiv.org/pdf/1212.5956v1"
  },
  {
    "Title": "Application of Machine Learning Optimization in Cloud Computing Resource\n  Scheduling and Management",
    "Authors": "Yifan Zhang, Bo Liu, Yulu Gong, Jiaxin Huang, Jingyu Xu, Weixiang Wan",
    "Published": "2024-02-27T05:14:27Z",
    "Summary": "In recent years, cloud computing has been widely used. Cloud computing refers to the centralized computing resources, users through the access to the centralized resources to complete the calculation, the cloud computing center will return the results of the program processing to the user. Cloud computing is not only for individual users, but also for enterprise users. By purchasing a cloud server, users do not have to buy a large number of computers, saving computing costs. According to a report by China Economic News Network, the scale of cloud computing in China has reached 209.1 billion yuan. At present, the more mature cloud service providers in China are Ali Cloud, Baidu Cloud, Huawei Cloud and so on. Therefore, this paper proposes an innovative approach to solve complex problems in cloud computing resource scheduling and management using machine learning optimization techniques. Through in-depth study of challenges such as low resource utilization and unbalanced load in the cloud environment, this study proposes a comprehensive solution, including optimization methods such as deep learning and genetic algorithm, to improve system performance and efficiency, and thus bring new breakthroughs and progress in the field of cloud computing resource management.Rational allocation of resources plays a crucial role in cloud computing. In the resource allocation of cloud computing, the cloud computing center has limited cloud resources, and users arrive in sequence. Each user requests the cloud computing center to use a certain number of cloud resources at a specific time.",
    "Link": "http://arxiv.org/abs/2402.17216v1",
    "PDF Link": "http://arxiv.org/pdf/2402.17216v1"
  },
  {
    "Title": "Evolution of Cloud Storage as Cloud Computing Infrastructure Service",
    "Authors": "Arokia Paul Rajan, Shanmugapriyaa",
    "Published": "2013-08-05T06:11:12Z",
    "Summary": "Enterprises are driving towards less cost, more availability, agility, managed risk - all of which is accelerated towards Cloud Computing. Cloud is not a particular product, but a way of delivering IT services that are consumable on demand, elastic to scale up and down as needed, and follow a pay-for-usage model. Out of the three common types of cloud computing service models, Infrastructure as a Service (IaaS) is a service model that provides servers, computing power, network bandwidth and Storage capacity, as a service to their subscribers. Cloud can relate to many things but without the fundamental storage pieces, which is provided as a service namely Cloud Storage, none of the other applications is possible. This paper introduces Cloud Storage, which covers the key technologies in cloud computing and Cloud Storage, management insights about cloud computing, different types of cloud services, driving forces of cloud computing and cloud storage, advantages and challenges of cloud storage and concludes by pinpointing few challenges to be addressed by the cloud storage providers.",
    "Link": "http://arxiv.org/abs/1308.1303v1",
    "PDF Link": "http://arxiv.org/pdf/1308.1303v1"
  },
  {
    "Title": "A Survey on Cloud Security Issues and Techniques",
    "Authors": "Shubhanjali Sharma, Garima Gupta, P. R. Laxmi",
    "Published": "2014-03-22T08:49:30Z",
    "Summary": "Today, cloud computing is an emerging way of computing in computer science. Cloud computing is a set of resources and services that are offered by the network or internet. Cloud computing extends various computing techniques like grid computing, distributed computing. Today cloud computing is used in both industrial field and academic field. Cloud facilitates its users by providing virtual resources via internet. As the field of cloud computing is spreading the new techniques are developing. This increase in cloud computing environment also increases security challenges for cloud developers. Users of cloud save their data in the cloud hence the lack of security in cloud can lose the users trust. In this paper we will discuss some of the cloud security issues in various aspects like multi-tenancy, elasticity, availability etc. The paper also discuss existing security techniques and approaches for a secure cloud. This paper will enable researchers and professionals to know about different security threats and models and tools proposed.",
    "Link": "http://arxiv.org/abs/1403.5627v1",
    "PDF Link": "http://arxiv.org/pdf/1403.5627v1"
  },
  {
    "Title": "Discussion of various models related to cloud performance",
    "Authors": "Chaitanya Krishna Kande",
    "Published": "2015-05-01T18:10:51Z",
    "Summary": "This paper discusses the various models related to cloud computing. Knowing the metrics related to infrastructure is very critical to enhance the performance of cloud services. Various metrics related to clouds such as pageview response time, admission control and enforcing elasticity to cloud infrastructure are very crucial in analyzing the characteristics of the cloud to enhance the cloud performance.",
    "Link": "http://arxiv.org/abs/1505.00236v1",
    "PDF Link": "http://arxiv.org/pdf/1505.00236v1"
  },
  {
    "Title": "A Comparative Study of Load Balancing Algorithms in Cloud Computing\n  Environment",
    "Authors": "Mayanka Katyal, Atul Mishra",
    "Published": "2014-03-27T05:07:28Z",
    "Summary": "Cloud Computing is a new trend emerging in IT environment with huge requirements of infrastructure and resources. Load Balancing is an important aspect of cloud computing environment. Efficient load balancing scheme ensures efficient resource utilization by provisioning of resources to cloud users on demand basis in pay as you say manner. Load Balancing may even support prioritizing users by applying appropriate scheduling criteria. This paper presents various load balancing schemes in different cloud environment based on requirements specified in Service Level Agreement (SLA).",
    "Link": "http://arxiv.org/abs/1403.6918v1",
    "PDF Link": "http://arxiv.org/pdf/1403.6918v1"
  },
  {
    "Title": "Resource Management in Cloud Computing: Classification and Taxonomy",
    "Authors": "Swapnil M Parikh, Narendra M Patel, Harshadkumar B Prajapati",
    "Published": "2017-02-24T11:39:59Z",
    "Summary": "Cloud Computing is a new era of remote computing / Internet based computing where one can access their personal resources easily from any computer through Internet. Cloud delivers computing as a utility as it is available to the cloud consumers on demand. It is a simple pay-per-use consumer-provider service model. It contains large number of shared resources. So Resource Management is always a major issue in cloud computing like any other computing paradigm. Due to the availability of finite resources it is very challenging for cloud providers to provide all the requested resources. From the cloud providers perspective cloud resources must be allocated in a fair and efficient manner. Research Survey is not available from the perspective of resource management as a process in cloud computing. So this research paper provides a detailed sequential view / steps on resource management in cloud computing. Firstly this research paper classifies various resources in cloud computing. It also gives taxonomy on resource management in cloud computing through which one can do further research. Lastly comparisons on various resource management algorithms has been presented.",
    "Link": "http://arxiv.org/abs/1703.00374v1",
    "PDF Link": "http://arxiv.org/pdf/1703.00374v1"
  },
  {
    "Title": "Securing the Data in Clouds with Hyperelliptic Curve Cryptography",
    "Authors": "Debajyoti Mukhopadhyay, Ashay Shirwadkar, Pratik Gaikar, Tanmay Agrawal",
    "Published": "2014-11-25T08:56:01Z",
    "Summary": "In todays world, Cloud computing has attracted research communities as it provides services in reduced cost due to virtualizing all the necessary resources. Even modern business architecture depends upon Cloud computing .As it is a internet based utility, which provides various services over a network, it is prone to network based attacks. Hence security in clouds is the most important in case of cloud computing. Cloud Security concerns the customer to fully rely on storing data on clouds. That is why Cloud security has attracted attention of the research community. This paper will discuss securing the data in clouds by implementing key agreement, encryption and signature verification/generation with hyperelliptic curve cryptography.",
    "Link": "http://arxiv.org/abs/1411.6771v1",
    "PDF Link": "http://arxiv.org/pdf/1411.6771v1"
  },
  {
    "Title": "A Survey on Cloud Computing Security",
    "Authors": "Hero Modares, Rosli Salleh, Amirhosein Moravejosharieh, Hassan Keshavarz, Majid Talebi Shahgoli",
    "Published": "2012-06-24T07:20:46Z",
    "Summary": "Computation encounter the new approach of cloud computing which maybe keeps the world and possibly can prepare all the human's necessities. In other words, cloud computing is the subsequent regular step in the evolution of on-demand information technology services and products. The Cloud is a metaphor for the Internet and is a concept for the covered complicated infrastructure; it also depends on sketching in computer network diagrams. In this paper we will focus on concept of cloud computing, cloud deployment models, cloud security challenges encryption and data protection, privacy and security and data management and movement from grid to cloud.",
    "Link": "http://arxiv.org/abs/1206.5468v1",
    "PDF Link": "http://arxiv.org/pdf/1206.5468v1"
  },
  {
    "Title": "A Preliminary Study On Emerging Cloud Computing Security Challenges",
    "Authors": "Babin Bhandari, James Zheng",
    "Published": "2018-08-13T10:45:38Z",
    "Summary": "Cloud computing is the internet based provisioning of the computing resources, software, and information on demand. Cloud Computing is referred to as one of most recent emerging paradigms of computing utilities. Since Cloud computing is the dominant infrastructure of the shared services over the internet, it is important to be aware of the security risk and the challenges associated with this emerging computing paradigm. This survey provides a brief introduction to the cloud computing, its major characteristics, and service models. It also explores cloud security threats, lists a few security solutions , and proposes a promsing research direction to deal with the evolving security challenges in Cloud computing.",
    "Link": "http://arxiv.org/abs/1808.04143v1",
    "PDF Link": "http://arxiv.org/pdf/1808.04143v1"
  },
  {
    "Title": "Framework for cloud computing adoption: A road map for Smes to cloud\n  migration",
    "Authors": "Nabeel Khan, Adil Al-Yasiri",
    "Published": "2016-01-07T17:21:41Z",
    "Summary": "Small and Medium size Enterprises (SME) are considered as a backbone of many developing and developed economies of the world; they are the driving force to any major economy across the globe. Through Cloud Computing firms outsource their entire information technology (IT) process while concentrating more on their core business. It allows businesses to cut down heavy cost incurred over IT infrastructure without losing focus on customer needs. However, Cloud industry to an extent has struggled to grow among SMEs due to the reluctance and concerns expressed by them. Throughout the course of this study several interviews were conducted and the literature was reviewed to understand how cloud providers offer services and what challenges SMEs are facing. The study identified issues like cloud knowledge, interoperability, security and contractual concerns to be hindering SMEs adoption of cloud services. From the interviews common practices followed by cloud vendors and what concerns SMEs have were identified as a basis for a cloud framework which will bridge gaps between cloud vendors and SMEs. A stepwise framework for cloud adoption is formulated which identifies and provides recommendation to four most predominant challenges which are hurting cloud industry and taking SMEs away from cloud computing, as well as guide SMEs aiding in successful cloud adoption. Moreover, this framework streamlines the cloud adoption process for SMEs by removing ambiguity in regards to fundamentals associated with their organisation and cloud adoption process.",
    "Link": "http://arxiv.org/abs/1601.01608v1",
    "PDF Link": "http://arxiv.org/pdf/1601.01608v1"
  },
  {
    "Title": "Usage of Cloud Computing Simulators and Future Systems For Computational\n  Research",
    "Authors": "Ramkumar Lakshminarayanan, Rajasekar Ramalingam",
    "Published": "2016-04-30T09:30:14Z",
    "Summary": "Cloud Computing is an Internet based computing, whereby shared resources, software and information, are provided to computers and devices on demand, like the electricity grid. Currently, IaaS (Infrastructure as a Service), PaaS (Platform as a Service) and SaaS (Software as a Service) are used as a business model for Cloud Computing. Nowadays, the adoption and deployment of Cloud Computing is increasing in various domains, forcing researchers to conduct research in the area of Cloud Computing globally. Setting up the research environment is critical for the researchers in the developing countries to evaluate the research outputs. Currently, modeling, simulation technology and access of resources from various university data centers has become a useful and powerful tool in cloud computing research. Several cloud simulators have been specifically developed by various universities to carry out Cloud Computing research, including CloudSim, SPECI, Green Cloud and Future Systems (the Indiana University machines India, Bravo, Delta, Echo and Foxtrot) supports leading edge data science research and a broad range of computing-enabled education as well as integration of ideas from cloud and HPC systems. In this paper, the features, suitability, adaptability and the learning curve of the existing Cloud Computing simulators and Future Systems are reviewed and analyzed.",
    "Link": "http://arxiv.org/abs/1605.00085v1",
    "PDF Link": "http://arxiv.org/pdf/1605.00085v1"
  },
  {
    "Title": "Is Cloud Computing Steganography-proof?",
    "Authors": "Wojciech Mazurczyk, Krzysztof Szczypiorski",
    "Published": "2011-07-20T19:18:36Z",
    "Summary": "The paper focuses on characterisation of information hiding possibilities in Cloud Computing. After general introduction to cloud computing and its security we move to brief description of steganography. In particular we introduce classification of steganographic communication scenarios in cloud computing which is based on location of the steganograms receiver. These scenarios as well as the threats that steganographic methods can cause must be taken into account when designing secure cloud computing services.",
    "Link": "http://arxiv.org/abs/1107.4077v1",
    "PDF Link": "http://arxiv.org/pdf/1107.4077v1"
  },
  {
    "Title": "Surrogate cloud fields with measured cloud properties",
    "Authors": "Victor Venema, Susanne Crewell, Clemens Simmer",
    "Published": "2003-06-09T11:59:36Z",
    "Summary": "This paper describes two new methods to generate 2D and 3D cloud fields based on 1D and 2D ground based profiler meas-urements. These cloud fields share desired statistical properties with real cloud fields. As they, however, are similar but not the same as real clouds, we call them surrogate clouds. One important advantage of the new methods is that the amplitude distribution of cloud liquid water is also exactly determined by the measurement: The surrogate clouds made with the classi-cal methods such as the Fourier method and the Bounded Cascade method are Gaussian and 'log-normal-like', respectively. Our first new method iteratively creates a time series with a measured amplitude distribution and power spectrum. Our sec-ond method uses an evolutionary search algorithm to generate cloud fields with practically arbitrary constraints. These clouds will be used to study the relation between radiation and cloud structure.",
    "Link": "http://arxiv.org/abs/physics/0306067v1",
    "PDF Link": "http://arxiv.org/pdf/physics/0306067v1"
  },
  {
    "Title": "SecLaaS: Secure Logging-as-a-Service for Cloud Forensics",
    "Authors": "Shams Zawoad, Amit Kumar Dutta, Ragib Hasan",
    "Published": "2013-02-25T22:36:06Z",
    "Summary": "Cloud computing has emerged as a popular computing paradigm in recent years. However, today's cloud computing architectures often lack support for computer forensic investigations. Analyzing various logs (e.g., process logs, network logs) plays a vital role in computer forensics. Unfortunately, collecting logs from a cloud is very hard given the black-box nature of clouds and the multi-tenant cloud models, where many users share the same processing and network resources. Researchers have proposed using log API or cloud management console to mitigate the challenges of collecting logs from cloud infrastructure. However, there has been no concrete work, which shows how to provide cloud logs to investigator while preserving users' privacy and integrity of the logs. In this paper, we introduce Secure-Logging-as-a-Service (SecLaaS), which stores virtual machines' logs and provides access to forensic investigators ensuring the confidentiality of the cloud users. Additionally, SeclaaS preserves proofs of past log and thus protects the integrity of the logs from dishonest investigators or cloud providers. Finally, we evaluate the feasibility of the scheme by implementing SecLaaS for network access logs in OpenStack - a popular open source cloud platform.",
    "Link": "http://arxiv.org/abs/1302.6267v1",
    "PDF Link": "http://arxiv.org/pdf/1302.6267v1"
  },
  {
    "Title": "soCloud: A service-oriented component-based PaaS for managing\n  portability, provisioning, elasticity, and high availability across multiple\n  clouds",
    "Authors": "Fawaz Paraiso, Philippe Merle, Lionel Seinturier",
    "Published": "2014-07-08T06:20:48Z",
    "Summary": "Multi-cloud computing is a promising paradigm to support very large scale world wide distributed applications. Multi-cloud computing is the usage of multiple, independent cloud environments, which assumed no priori agreement between cloud providers or third party. However, multi-cloud computing has to face several key challenges such as portability, provisioning, elasticity, and high availability. Developers will not only have to deploy applications to a specific cloud, but will also have to consider application portability from one cloud to another, and to deploy distributed applications spanning multiple clouds. This article presents soCloud a service-oriented component-based Platform as a Service (PaaS) for managing portability, elasticity, provisioning, and high availability across multiple clouds. soCloud is based on the OASIS Service Component Architecture (SCA) standard in order to address portability. soCloud provides services for managing provisioning, elasticity, and high availability across multiple clouds. soCloud has been deployed and evaluated on top of ten existing cloud providers: Windows Azure, DELL KACE, Amazon EC2, CloudBees, OpenShift, dotCloud, Jelastic, Heroku, Appfog, and an Eucalyptus private cloud.",
    "Link": "http://arxiv.org/abs/1407.1963v1",
    "PDF Link": "http://arxiv.org/pdf/1407.1963v1"
  },
  {
    "Title": "Cloud Computing and Grid Computing 360-Degree Compared",
    "Authors": "Ian Foster, Yong Zhao, Ioan Raicu, Shiyong Lu",
    "Published": "2008-12-31T19:13:05Z",
    "Summary": "Cloud Computing has become another buzzword after Web 2.0. However, there are dozens of different definitions for Cloud Computing and there seems to be no consensus on what a Cloud is. On the other hand, Cloud Computing is not a completely new concept; it has intricate connection to the relatively new but thirteen-year established Grid Computing paradigm, and other relevant technologies such as utility computing, cluster computing, and distributed systems in general. This paper strives to compare and contrast Cloud Computing with Grid Computing from various angles and give insights into the essential characteristics of both.",
    "Link": "http://arxiv.org/abs/0901.0131v1",
    "PDF Link": "http://arxiv.org/pdf/0901.0131v1"
  },
  {
    "Title": "Cloud Adoption A Modern Approach",
    "Authors": "Subhadip Kumar",
    "Published": "2023-05-16T20:26:22Z",
    "Summary": "Todays Information Technology world is cloud-centric. Companies are intrigued to migrate their workload private cloud from on-premise Datacenter to Public cloud to take advantage of the latest innovations. It drives the business growth and competitiveness of the organization. At the same time, it is important for Enterprise Architects to understand the drawbacks and challenges to migrate the workload to Cloud. This paper aims to identify the key factors to migrate the workload to the cloud. It also helps an organization to identify the candidate for cloud migration. An impulsive decision to move to the Cloud may be detrimental to an organization. Also, I will discuss one case study to see the benefits and disadvantages of cloud migration. This will help the organization to maximize its ROI.",
    "Link": "http://arxiv.org/abs/2305.18308v1",
    "PDF Link": "http://arxiv.org/pdf/2305.18308v1"
  },
  {
    "Title": "Cloud Security and Security Challenges Revisited",
    "Authors": "Fabian Süß, Marco Freimuth, Andreas Aßmuth, George R. S. Weir, Bob Duncan",
    "Published": "2024-05-18T17:42:02Z",
    "Summary": "In recent years, Cloud Computing has transformed local businesses and created new business models on the Internet- and Cloud services are still flourishing. But after the emphatic hype in the early years, a more realistic perception of Cloud services has emerged. One reason for this surely is that today, Cloud Computing is considered as an established and well-accepted technology and no longer as a technical novelty. But the second reason for this assessment might also be numerous security issues that Cloud Computing in general or specific Cloud services have experienced since then. In this paper, we revisit attacks on Cloud services and Cloud-related attack vectors that have been published in recent years. We then consider successful or proposed solutions to cope with these challenges. Based on these findings, we apply a security metric in order to rank all these Cloud-related security challenges concerning their severity. This should assist security professionals to prioritize their efforts toward addressing these issues.",
    "Link": "http://arxiv.org/abs/2405.11350v1",
    "PDF Link": "http://arxiv.org/pdf/2405.11350v1"
  },
  {
    "Title": "11-Year Warm Cloud Modification Experiment in Maharashtra State, India",
    "Authors": "A. S. R. Murty et. al",
    "Published": "1998-12-28T10:17:51Z",
    "Summary": "A warm cloud modification experiment was carried out in an area of 4800 Sq.Km in the Pune region,India, during the 11-summer monsoon (June-September) seasons (1973-74, 1976, 1979-86). A double-area cross-over design with area randomization was adopted and an instrumented aircraft was used for seeding and cloud physical measurements. Finely pulverised salt (sodium chloride) particles were released into the monsoon clouds (cumulus and stratocumulus) during aircraft penetrations into the clouds at a height of 200-300 m above the cloud-base. The warm cloud responses to salt seeding are found to be critically dependent on the cloud physical characteristics e.g., vertical thickness and liquid water content. Clouds with vertical thickness greater than 1 km, LWC greater than 0.5 gm/cubic m when seeded with salt particles (modal diameter 10 micro m, concentration 1 per litre of cloud air) produced increase in rainfall of 24 per cent significant at 4 per cent level. Shallow clouds (vertical thickness less than 1 km, LWC less than 0.5 gm/cubic m) when seeded showed tendency for dissipation. The cloud physical observations made in not-seeded (control) and seeded (target) clouds have provided some useful evidence to test the applicability of the warm cloud modification hypothesis. The results of the cloud model computations suggested that moderate convergence at the cloud-base is essential for the cloud growth and development of precipitation in the real world. Hygroscopic particle seeding of warm clouds under favourable dynamical conditions (convergence at the cloud-base level) may result in the acceleration of the collision-coalescence process resulting in the enhancement of rainfall.",
    "Link": "http://arxiv.org/abs/physics/9812046v1",
    "PDF Link": "http://arxiv.org/pdf/physics/9812046v1"
  },
  {
    "Title": "Application of Ontologies in Cloud Computing: The State-Of-The-Art",
    "Authors": "Fahim T. Imam",
    "Published": "2016-10-06T05:39:37Z",
    "Summary": "This paper presents a systematic survey on existing literature and seminal works relevant to the application of ontologies in different aspects of Cloud computing. Our hypothesis is that ontologies along with their reasoning capabilities can have significant impact on improving various aspects of the Cloud computing phenomena. Ontologies can promote intelligent decision support mechanisms for various Cloud based services. They can also provide effective interoperability among the Cloud based systems and resources. This survey can promote a comprehensive understanding on the roles and significance of ontologies within the overall domain of Cloud Computing. Also, this project can potentially form the basis of new research area and possibilities for both ontology and Cloud computing communities.",
    "Link": "http://arxiv.org/abs/1610.02333v1",
    "PDF Link": "http://arxiv.org/pdf/1610.02333v1"
  },
  {
    "Title": "A Slow Read attack Using Cloud",
    "Authors": "Darine Ameyed, Fehmi Jaafar, Jaouhar Fattahi",
    "Published": "2017-12-05T21:41:00Z",
    "Summary": "Cloud computing relies on sharing computing resources rather than having local servers or personal devices to handle applications. Nowadays, cloud computing has become one of the fastest growing fields in information technology. However, several new security issues of cloud computing have emerged due to its service delivery models. In this paper, we discuss the case of distributed denial-of-service (DDoS) attack using Cloud resources. First, we show how such attack using a cloud platform could not be detected by previous techniques. Then we present a tricky solution based on the cloud as well.",
    "Link": "http://arxiv.org/abs/1712.01939v1",
    "PDF Link": "http://arxiv.org/pdf/1712.01939v1"
  },
  {
    "Title": "KCES: A Workflow Containerization Scheduling Scheme Under Cloud-Edge\n  Collaboration Framework",
    "Authors": "Chenggang Shan, Runze Gao, Qinghua Han, Zhen Yang, Jinhui Zhang, Yuanqing Xia",
    "Published": "2024-01-02T14:11:24Z",
    "Summary": "As more IoT applications gradually move towards the cloud-edge collaborative mode, the containerized scheduling of workflows extends from the cloud to the edge. However, given the high delay of the communication network, loose coupling of structure, and resource heterogeneity between cloud and edge, workflow containerization scheduling in the cloud-edge scenarios faces the difficulty of resource coordination and application collaboration management. To address these two issues, we propose a KubeEdge-Cloud-Edge-Scheduling scheme named KCES, a workflow containerization scheduling scheme for the KubeEdge cloud-edge framework. The KCES includes a cloud-edge workflow scheduling engine for KubeEdge and workflow scheduling strategies for task horizontal roaming and vertical offloading. Considering the scheduling optimization of cloud-edge workflows, this paper proposes a cloud-edge workflow scheduling model and cloud-edge node model and designs a cloud-edge workflow scheduling engine to maximize cloud-edge resource utilization under the constraint of workflow task delay. A cloud-edge resource hybrid management technology is used to design the cloud-edge resource evaluation and resource allocation algorithms to achieve cloud-edge resource collaboration. Based on the ideas of distributed functional roles and the hierarchical division of computing power, the horizontal roaming among the edges and vertical offloading strategies between the cloud and edges for workflow tasks are designed to realize the cloud-edge application collaboration. Through a customized IoT application workflow instance, experimental results show that KCES is superior to the baseline in total workflow duration, average workflow duration, and resource usage and has the capabilities of horizontal roaming and vertical offloading for workflow tasks.",
    "Link": "http://arxiv.org/abs/2401.01217v1",
    "PDF Link": "http://arxiv.org/pdf/2401.01217v1"
  },
  {
    "Title": "Research Challenges for Enterprise Cloud Computing",
    "Authors": "Ali Khajeh-Hosseini, Ian Sommerville, Ilango Sriram",
    "Published": "2010-01-19T11:39:30Z",
    "Summary": "Cloud computing represents a shift away from computing as a product that is purchased, to computing as a service that is delivered to consumers over the internet from large-scale data centers - or \"clouds\". This paper discusses some of the research challenges for cloud computing from an enterprise or organizational perspective, and puts them in context by reviewing the existing body of literature in cloud computing. Various research challenges relating to the following topics are discussed: the organizational changes brought about by cloud computing; the economic and organizational implications of its utility billing model; the security, legal and privacy issues that cloud computing raises. It is important to highlight these research challenges because cloud computing is not simply about a technological improvement of data centers but a fundamental change in how IT is provisioned and used. This type of research has the potential to influence wider adoption of cloud computing in enterprise, and in the consumer market too.",
    "Link": "http://arxiv.org/abs/1001.3257v1",
    "PDF Link": "http://arxiv.org/pdf/1001.3257v1"
  },
  {
    "Title": "I Have the Proof: Providing Proofs of Past Data Possession in Cloud\n  Forensics",
    "Authors": "Shams Zawoad, Ragib Hasan",
    "Published": "2012-11-19T08:16:59Z",
    "Summary": "Cloud computing has emerged as a popular computing paradigm in recent years. However, today's cloud computing architectures often lack support for computer forensic investigations. A key task of digital forensics is to prove the presence of a particular file in a given storage system. Unfortunately, it is very hard to do so in a cloud given the black-box nature of clouds and the multi-tenant cloud models. In clouds, analyzing the data from a virtual machine instance or data stored in a cloud storage only allows us to investigate the current content of the cloud storage, but not the previous contents. In this paper, we introduce the idea of building proofs of past data possession in the context of a cloud storage service. We present a scheme for creating such proofs and evaluate its performance in a real cloud provider. We also discuss how this proof of past data possession can be used effectively in cloud forensics.",
    "Link": "http://arxiv.org/abs/1211.4328v1",
    "PDF Link": "http://arxiv.org/pdf/1211.4328v1"
  },
  {
    "Title": "A Cloud Computing Survey: Developments and Future Trends in\n  Infrastructure as a Service Computing",
    "Authors": "Jonathan Stuart Ward, Adam Barker",
    "Published": "2013-06-06T12:41:57Z",
    "Summary": "Cloud computing is a recent paradigm based around the notion of delivery of resources via a service model over the Internet. Despite being a new paradigm of computation, cloud computing owes its origins to a number of previous paradigms. The term cloud computing is well defined and no longer merits rigorous taxonomies to furnish a definition. Instead this survey paper considers the past, present and future of cloud computing. As an evolution of previous paradigms, we consider the predecessors to cloud computing and what significance they still hold to cloud services. Additionally we examine the technologies which comprise cloud computing and how the challenges and future developments of these technologies will influence the field. Finally we examine the challenges that limit the growth, application and development of cloud computing and suggest directions required to overcome these challenges in order to further the success of cloud computing.",
    "Link": "http://arxiv.org/abs/1306.1394v1",
    "PDF Link": "http://arxiv.org/pdf/1306.1394v1"
  },
  {
    "Title": "Dynamic Resource Allocation for Virtual Machine Migration Optimization\n  using Machine Learning",
    "Authors": "Yulu Gong, Jiaxin Huang, Bo Liu, Jingyu Xu, Binbin Wu, Yifan Zhang",
    "Published": "2024-03-20T14:13:44Z",
    "Summary": "The paragraph is grammatically correct and logically coherent. It discusses the importance of mobile terminal cloud computing migration technology in meeting the demands of evolving computer and cloud computing technologies. It emphasizes the need for efficient data access and storage, as well as the utilization of cloud computing migration technology to prevent additional time delays. The paragraph also highlights the contributions of cloud computing migration technology to expanding cloud computing services. Additionally, it acknowledges the role of virtualization as a fundamental capability of cloud computing while emphasizing that cloud computing and virtualization are not inherently interconnected. Finally, it introduces machine learning-based virtual machine migration optimization and dynamic resource allocation as a critical research direction in cloud computing, citing the limitations of static rules or manual settings in traditional cloud computing environments. Overall, the paragraph effectively communicates the importance of machine learning technology in addressing resource allocation and virtual machine migration challenges in cloud computing.",
    "Link": "http://arxiv.org/abs/2403.13619v1",
    "PDF Link": "http://arxiv.org/pdf/2403.13619v1"
  },
  {
    "Title": "Compute and Storage Clouds Using Wide Area High Performance Networks",
    "Authors": "Robert L. Grossman, Yunhong Gu, Michael Sabala, Wanzhi Zhang",
    "Published": "2008-08-13T09:48:37Z",
    "Summary": "We describe a cloud based infrastructure that we have developed that is optimized for wide area, high performance networks and designed to support data mining applications. The infrastructure consists of a storage cloud called Sector and a compute cloud called Sphere. We describe two applications that we have built using the cloud and some experimental studies.",
    "Link": "http://arxiv.org/abs/0808.1802v1",
    "PDF Link": "http://arxiv.org/pdf/0808.1802v1"
  },
  {
    "Title": "An Automated Implementation of Hybrid Cloud for Performance Evaluation\n  of Distributed Databases",
    "Authors": "Yaser Mansouri, Victor Prokhorenko, M. Ali Babar",
    "Published": "2020-06-04T13:08:27Z",
    "Summary": "A Hybrid cloud is an integration of resources between private and public clouds. It enables users to horizontally scale their on-premises infrastructure up to public clouds in order to improve performance and cut up-front investment cost. This model of applications deployment is called cloud bursting that allows data-intensive applications especially distributed database systems to have the benefit of both private and public clouds. In this work, we present an automated implementation of a hybrid cloud using (i) a robust and zero-cost Linux-based VPN to make a secure connection between private and public clouds, and (ii) Terraform as a software tool to deploy infrastructure resources based on the requirements of hybrid cloud. We also explore performance evaluation of cloud bursting for six modern and distributed database systems on the hybrid cloud spanning over local OpenStack and Microsoft Azure. Our results reveal that MongoDB and MySQL Cluster work efficient in terms of throughput and operations latency if they burst into a public cloud to supply their resources. In contrast, the performance of Cassandra, Riak, Redis, and Couchdb reduces if they significantly leverage their required resources via cloud bursting.",
    "Link": "http://arxiv.org/abs/2006.02833v1",
    "PDF Link": "http://arxiv.org/pdf/2006.02833v1"
  },
  {
    "Title": "Characterizing User and Provider Reported Cloud Failures",
    "Authors": "Mehmet Berk Cetin, Sacheendra Talluri, Alexandru Iosup",
    "Published": "2021-10-23T15:10:25Z",
    "Summary": "Cloud computing is the backbone of the digital society. Digital banking, media, communication, gaming, and many others depend on cloud services. Unfortunately, cloud services may fail, leading to damaged services, unhappy users, and perhaps millions of dollars lost for companies. Understanding a cloud service failure requires a detailed report on why and how the service failed. Previous work studies how cloud services fail using logs published by cloud operators. However, information is lacking on how users perceive and experience cloud failures. Therefore, we collect and characterize the data for user-reported cloud failures from Down Detector for three cloud service providers over three years. We count and analyze time patterns in the user reports, and derive failures from those user reports and characterize their duration and interarrival time. We characterize provider-reported cloud failures and compare the results with the characterization of user-reported failures. The comparison reveals the information of how users perceive failures and how much of the failures are reported by cloud service providers. Overall, this work provides a characterization of user- and provider-reported cloud failures and compares them with each other.",
    "Link": "http://arxiv.org/abs/2110.12237v2",
    "PDF Link": "http://arxiv.org/pdf/2110.12237v2"
  },
  {
    "Title": "Platforms for Building and Deploying Applications for Cloud Computing",
    "Authors": "Rajkumar Buyya, Karthik Sukumar",
    "Published": "2011-04-22T02:51:54Z",
    "Summary": "Cloud computing is rapidly emerging as a new paradigm for delivering IT services as utlity-oriented services on subscription-basis. The rapid development of applications and their deployment in Cloud computing environments in efficient manner is a complex task. In this article, we give a brief introduction to Cloud computing technology and Platform as a Service, we examine the offerings in this category, and provide the basis for helping readers to understand basic application platform opportunities in Cloud by technologies such as Microsoft Azure, Sales Force, Google App, and Aneka for Cloud computing. We demonstrate that Manjrasoft Aneka is a Cloud Application Platform (CAP) leveraging these concepts and allowing an easy development of Cloud ready applications on a Private/Public/Hybrid Cloud. Aneka CAP offers facilities for quickly developing Cloud applications and a modular platform where additional services can be easily integrated to extend the system capabilities, thus being at pace with the rapidly evolution of Cloud computing.",
    "Link": "http://arxiv.org/abs/1104.4379v1",
    "PDF Link": "http://arxiv.org/pdf/1104.4379v1"
  },
  {
    "Title": "Formal Specification Language Based IaaS Cloud Workload Regression\n  Analysis",
    "Authors": "Sukhpal Singh, Inderveer Chana",
    "Published": "2014-02-13T05:24:37Z",
    "Summary": "Cloud Computing is an emerging area for accessing computing resources. In general, Cloud service providers offer services that can be clustered into three categories: SaaS, PaaS and IaaS. This paper discusses the Cloud workload analysis. The efficient Cloud workload resource mapping technique is proposed. This paper aims to provide a means of understanding and investigating IaaS Cloud workloads and the resources. In this paper, regression analysis is used to analyze the Cloud workloads and identifies the relationship between Cloud workloads and available resources. The effective organization of dynamic nature resources can be done with the help of Cloud workloads. Till Cloud workload is considered a vital talent, the Cloud resources cannot be consumed in an effective style. The proposed technique has been validated by Z Formal specification language. This approach is effective in minimizing the cost and submission burst time of Cloud workloads.",
    "Link": "http://arxiv.org/abs/1402.3034v1",
    "PDF Link": "http://arxiv.org/pdf/1402.3034v1"
  },
  {
    "Title": "An Analysis of the Cloud Computing Security Problem",
    "Authors": "Mohamed Almorsy, John Grundy, Ingo Müller",
    "Published": "2016-09-05T11:31:42Z",
    "Summary": "Cloud computing is a new computational paradigm that offers an innovative business model for organizations to adopt IT without upfront investment. Despite the potential gains achieved from the cloud computing, the model security is still questionable which impacts the cloud model adoption. The security problem becomes more complicated under the cloud model as new dimensions have entered into the problem scope related to the model architecture, multi-tenancy, elasticity, and layers dependency stack. In this paper we introduce a detailed analysis of the cloud security problem. We investigated the problem from the cloud architecture perspective, the cloud offered characteristics perspective, the cloud stakeholders' perspective, and the cloud service delivery models perspective. Based on this analysis we derive a detailed specification of the cloud security problem and key features that should be covered by any proposed security solution.",
    "Link": "http://arxiv.org/abs/1609.01107v1",
    "PDF Link": "http://arxiv.org/pdf/1609.01107v1"
  },
  {
    "Title": "Datacenter Changes vs. Employment Rates for Datacenter Managers In the\n  Cloud Computing Era",
    "Authors": "Timur Mirzoev, Bruce Benson, David Hillhouse, Mickey Lewis",
    "Published": "2014-04-08T14:31:35Z",
    "Summary": "Due to the evolving Cloud Computing paradigm, there is a prevailing concern that in the near future data center managers may be in short supply. Cloud computing, as a whole, is becoming more prevalent into today s computing world. In fact, cloud computing has become so popular that some are now referring to data centers as cloud centers. How does this interest in cloud computing translate into employment rates for data center managers? The popularity of the public and private cloud models are the prevailing force behind answering this question. Therefore, the skill set of the datacenter manager has evolved to harness the on demand self-services, broad network access, resource pooling, rapid elasticity, measured service, and multi tenacity characteristics of cloud computing. Using diverse sources ranging from the Bureau of Labor and Statistics to trade articles, this manuscript takes an in-depth look at these employment rates related to the cloud and the determining factors behind them. Based on the information available, datacenter manager employment rates in the cloud computing era will continue to increase well into 2016.",
    "Link": "http://arxiv.org/abs/1404.2151v1",
    "PDF Link": "http://arxiv.org/pdf/1404.2151v1"
  },
  {
    "Title": "A Novel Application Licensing Framework for Mobile Cloud Environment",
    "Authors": "Atta ur Rehman Khan, Mazliza Othman, Abdul Nasir Khan",
    "Published": "2013-12-30T21:59:30Z",
    "Summary": "Mobile cloud computing is a new technology that enhances smartphone applications capabilities in terms of performance, energy efficiency, and execution support. These features are achieved via computation offloading technique that is supported by specialized mobile cloud application development models. However, the cloud-enabled applications are prone to application piracy issue for which the traditional licensing frameworks are of no use. Therefore, a new licensing framework is required to control application piracy in mobile cloud environment. This paper presents a preliminary design of a novel application licensing framework for mobile cloud environment that restricts execution of applications on unauthenticated smartphones and cloud resources.",
    "Link": "http://arxiv.org/abs/1401.0034v1",
    "PDF Link": "http://arxiv.org/pdf/1401.0034v1"
  },
  {
    "Title": "Towards Constraint-based High Performance Cloud System in the Process of\n  Cloud Computing Adoption in an Organization",
    "Authors": "Mikael Fernandus Simalango, Mun-Young Kang, Sangyoon Oh",
    "Published": "2010-10-24T12:08:12Z",
    "Summary": "Cloud computing is penetrating into various domains and environments, from theoretical computer science to economy, from marketing hype to educational curriculum and from R&D lab to enterprise IT infrastructure. Yet, the currently developing state of cloud computing leaves several issues to address and also affects cloud computing adoption by organizations. In this paper, we explain how the transition into the cloud can occur in an organization and describe the mechanism for transforming legacy infrastructure into a virtual infrastructure-based cloud. We describe the state of the art of infrastructural cloud, which is essential in the decision making on cloud adoption, and highlight the challenges that can limit the scale and speed of the adoption. We then suggest a strategic framework for designing a high performance cloud system. This framework is applicable when transformation cloudbased deployment model collides with some constraints. We give an example of the implementation of the framework in a design of a budget-constrained high availability cloud system.",
    "Link": "http://arxiv.org/abs/1010.4952v1",
    "PDF Link": "http://arxiv.org/pdf/1010.4952v1"
  },
  {
    "Title": "Cloud Infrastructure Service Management - A Review",
    "Authors": "A. Anasuya Threse Innocent",
    "Published": "2012-05-30T09:45:54Z",
    "Summary": "The new era of computing called Cloud Computing allows the user to access the cloud services dynamically over the Internet wherever and whenever needed. Cloud consists of data and resources; and the cloud services include the delivery of software, infrastructure, applications, and storage over the Internet based on user demand through Internet. In short, cloud computing is a business and economic model allowing the users to utilize high-end computing and storage virtually with minimal infrastructure on their end. Cloud has three service models namely, Cloud Software-as-a-Service (SaaS), Cloud Platform-as-a-Service (PaaS), and Cloud Infrastructure-as-a-Service (IaaS). This paper talks in depth of cloud infrastructure service management.",
    "Link": "http://arxiv.org/abs/1206.6016v1",
    "PDF Link": "http://arxiv.org/pdf/1206.6016v1"
  },
  {
    "Title": "Towards a Taxonomy of Performance Evaluation of Commercial Cloud\n  Services",
    "Authors": "Zheng Li, Liam O'Brien, Rainbow Cai, He Zhang",
    "Published": "2013-02-08T07:20:30Z",
    "Summary": "Cloud Computing, as one of the most promising computing paradigms, has become increasingly accepted in industry. Numerous commercial providers have started to supply public Cloud services, and corresponding performance evaluation is then inevitably required for Cloud provider selection or cost-benefit analysis. Unfortunately, inaccurate and confusing evaluation implementations can be often seen in the context of commercial Cloud Computing, which could severely interfere and spoil evaluation-related comprehension and communication. This paper introduces a taxonomy to help profile and standardize the details of performance evaluation of commercial Cloud services. Through a systematic literature review, we constructed the taxonomy along two dimensions by arranging the atomic elements of Cloud-related performance evaluation. As such, this proposed taxonomy can be employed both to analyze existing evaluation practices through decomposition into elements and to design new experiments through composing elements for evaluating performance of commercial Cloud services. Moreover, through smooth expansion, we can continually adapt this taxonomy to the more general area of evaluation of Cloud Computing.",
    "Link": "http://arxiv.org/abs/1302.1957v1",
    "PDF Link": "http://arxiv.org/pdf/1302.1957v1"
  },
  {
    "Title": "A Survey and Comparative Study on Multi-Cloud Architectures: Emerging\n  Issues And Challenges For Cloud Federation",
    "Authors": "Deepika Saxena, Rishabh Gupta, Ashutosh Kumar Singh",
    "Published": "2021-08-29T12:03:41Z",
    "Summary": "Multi-cloud concept has broaden the world of cloud computing and has become a buzzword today. The word Multi-cloud envisions utilization of services from multiple heterogeneous cloud providers via a single architecture at customer premises. Though cloud computing has many issues and offers open research challenges, still the academics and industrial research has paved a pathway for multi-cloud environment. The concept of multi-cloud is in maturing phase, and many research projects are in progress to provide a multi-cloud architecture which is successfully enabled in all the respects like easy configuration, security, management etc. In this paper, concepts, challenges, requirement and future directions for multi-cloud environment are discussed. A survey of existing approaches and solutions provided by different multi-cloud architectures is entailed along with analysis of the pros and cons of different architectures while comparing the same.",
    "Link": "http://arxiv.org/abs/2108.12831v1",
    "PDF Link": "http://arxiv.org/pdf/2108.12831v1"
  },
  {
    "Title": "Molecular Dynamics Simulations on Cloud Computing and Machine Learning\n  Platforms",
    "Authors": "Prateek Sharma, Vikram Jadhao",
    "Published": "2021-11-11T21:20:26Z",
    "Summary": "Scientific computing applications have benefited greatly from high performance computing infrastructure such as supercomputers. However, we are seeing a paradigm shift in the computational structure, design, and requirements of these applications. Increasingly, data-driven and machine learning approaches are being used to support, speed-up, and enhance scientific computing applications, especially molecular dynamics simulations. Concurrently, cloud computing platforms are increasingly appealing for scientific computing, providing \"infinite\" computing powers, easier programming and deployment models, and access to computing accelerators such as TPUs (Tensor Processing Units). This confluence of machine learning (ML) and cloud computing represents exciting opportunities for cloud and systems researchers. ML-assisted molecular dynamics simulations are a new class of workload, and exhibit unique computational patterns. These simulations present new challenges for low-cost and high-performance execution. We argue that transient cloud resources, such as low-cost preemptible cloud VMs, can be a viable platform for this new workload. Finally, we present some low-hanging fruits and long-term challenges in cloud resource management, and the integration of molecular dynamics simulations into ML platforms (such as TensorFlow).",
    "Link": "http://arxiv.org/abs/2111.06466v1",
    "PDF Link": "http://arxiv.org/pdf/2111.06466v1"
  },
  {
    "Title": "Model-Based Cloud Resource Management with TOSCA and OCCI",
    "Authors": "Stéphanie Challita, Fabian Korte, Johannes Erbel, Faiez Zalila, Jens Grabowski, Philippe Merle",
    "Published": "2020-01-22T07:44:29Z",
    "Summary": "With the advent of cloud computing, different cloud providers with heterogeneous cloud services (compute, storage, network, applications, etc.) and their related Application Programming Interfaces (APIs) have emerged. This heterogeneity complicates the implementation of an interoperable cloud system. Several standards have been proposed to address this challenge and provide a unified interface to cloud resources. The Open Cloud Computing Interface (OCCI) thereby focuses on the standardization of a common API for Infrastructure-as-a-Service (IaaS) providers while the Topology and Orchestration Specification for Cloud Applications (TOSCA) focuses on the standardization of a template language to enable the proper definition of the topology of cloud applications and their orchestrations on top of a cloud system. TOSCA thereby does not define how the application topologies are created on the cloud. Therefore, we analyse the conceptual similarities between the two approaches and we study how we can integrate them to obtain a complete standard-based approach to manage both cloud infrastructure and cloud application layers. We propose an automated extensive mapping between the concepts of the two standards and we provide TOSCA Studio, a model-driven tool chain for TOSCA that conforms to OCCI. TOSCA Studio allows to graphically design cloud applications as well as to deploy and manage them at runtime using a fully model-driven cloud orchestrator based on the two standards. Our contribution is validated by successfully designing and deploying three cloud applications: WordPress, Node Cellar and Multi-Tier.",
    "Link": "http://arxiv.org/abs/2001.07900v2",
    "PDF Link": "http://arxiv.org/pdf/2001.07900v2"
  },
  {
    "Title": "Defining Cross-Cloud Systems",
    "Authors": "Yehia Elkhatib",
    "Published": "2016-02-08T19:13:32Z",
    "Summary": "Recent years have seen an increasing number of cross-cloud architectures, i.e. systems that span across cloud provisioning boundaries. However, the cloud computing world still lacks any standards in terms of programming interfaces, which has a knock-on effect on the costs associated with interoperability and severely limits the flexibility and portability of applications and virtual infrastructures. This paper outlines the different types of cross-cloud systems, and the associated design decisions.",
    "Link": "http://arxiv.org/abs/1602.02698v1",
    "PDF Link": "http://arxiv.org/pdf/1602.02698v1"
  },
  {
    "Title": "An Experimental Study of Load Balancing of OpenNebula Open-Source Cloud\n  Computing Platform",
    "Authors": "A B M Moniruzzaman, Kawser Wazed Nafi, Syed Akther Hossain",
    "Published": "2014-06-22T20:40:07Z",
    "Summary": "Cloud Computing is becoming a viable computing solution for services oriented computing. Several open-source cloud solutions are available to these supports. Open-source software stacks offer a huge amount of customizability without huge licensing fees. As a result, open source software are widely used for designing cloud, and private clouds are being built increasingly in the open source way. Numerous contributions have been made by the open-source community related to private-IaaS-cloud. OpenNebula - a cloud platform is one of the popular private cloud management software. However, little has been done to systematically investigate the performance evaluation of this open-source cloud solution in the existing literature. The performance evaluation aids new and existing research, industry and international projects when selecting OpenNebula software to their work. The objective of this paper is to evaluate the load-balancing performance of the OpenNebula cloud management software. For the performance evaluation, the OpenNebula cloud management software is installed and configured as a prototype implementation and tested on the DIU Cloud Lab. In this paper, two set of experiments are conducted to identify the load balancing performance of the OpenNebula cloud management platform- (1) Delete and Add Virtual Machine (VM) from OpenNebula cloud platform; (2) Mapping Physical Hosts to Virtual Machines (VMs) in the OpenNebula cloud platform.",
    "Link": "http://arxiv.org/abs/1406.5759v1",
    "PDF Link": "http://arxiv.org/pdf/1406.5759v1"
  },
  {
    "Title": "SSPU-Net: Self-Supervised Point Cloud Upsampling via Differentiable\n  Rendering",
    "Authors": "Yifan Zhao, Le Hui, Jin Xie",
    "Published": "2021-08-01T13:26:01Z",
    "Summary": "Point clouds obtained from 3D sensors are usually sparse. Existing methods mainly focus on upsampling sparse point clouds in a supervised manner by using dense ground truth point clouds. In this paper, we propose a self-supervised point cloud upsampling network (SSPU-Net) to generate dense point clouds without using ground truth. To achieve this, we exploit the consistency between the input sparse point cloud and generated dense point cloud for the shapes and rendered images. Specifically, we first propose a neighbor expansion unit (NEU) to upsample the sparse point clouds, where the local geometric structures of the sparse point clouds are exploited to learn weights for point interpolation. Then, we develop a differentiable point cloud rendering unit (DRU) as an end-to-end module in our network to render the point cloud into multi-view images. Finally, we formulate a shape-consistent loss and an image-consistent loss to train the network so that the shapes of the sparse and dense point clouds are as consistent as possible. Extensive results on the CAD and scanned datasets demonstrate that our method can achieve impressive results in a self-supervised manner. Code is available at https://github.com/fpthink/SSPU-Net.",
    "Link": "http://arxiv.org/abs/2108.00454v2",
    "PDF Link": "http://arxiv.org/pdf/2108.00454v2"
  },
  {
    "Title": "A Hybrid Cloud ERP Framework For Processing Purchasing Data",
    "Authors": "Xinyu Zhang",
    "Published": "2022-02-22T10:18:00Z",
    "Summary": "Cloud-based enterprise resource planning (cloud ERP) systems have existed in the business market for around ten years. Cloud ERP supports enterprises' daily activities by integrating organizational back-end systems in the cloud environment. One of the critical functions that cloud ERP offers is the purchasing application. The purchasing function of cloud ERP enables enterprises to streamline all the online purchasing transactions in real-time automatically. Even cloud ERP is deployed quite often these days, organizations somehow still lack the knowledge of it; to be specific, there are many issues attached to cloud ERP implementation yet to be solved. Hence, this paper compares four leading cloud ERP platforms in Australia and proposes a hybrid cloud ERP framework to process online purchasing transactions. By adopting a case study approach, a purchasing web-based application is designed and presented in this paper. In general, the proposed hybrid cloud ERP framework and the integrated web-based purchasing application allow user companies to process online purchasing transactions with short operation time and increased business efficiency; in the meantime, the proposed framework also reduces security risks attached to the public cloud.",
    "Link": "http://arxiv.org/abs/2202.10786v1",
    "PDF Link": "http://arxiv.org/pdf/2202.10786v1"
  },
  {
    "Title": "Configuration management in the distributed cloud",
    "Authors": "Tamara Ranković, Ivana Kovačević, Veljko Maksimović, Goran Sladić, Miloš Simić",
    "Published": "2024-10-26T21:07:17Z",
    "Summary": "Owing to their cost-effectiveness and flexibility, cloud services have been the default choice for the deployment of innumerable software systems over the years. However, novel paradigms are beginning to emerge, as the cloud can't meet the requirements of increasingly many latency- and privacy-sensitive applications. The distributed cloud model, being one of the attempts to overcome these challenges, places a distributed cloud layer between device and cloud layers, intending to bring resources closer to data sources. As application code should be kept separate from its configuration, especially in highly dynamic cloud environments, there is a need to incorporate configuration primitives in future distributed cloud platforms. In this paper, we present the design and implementation of a configuration management subsystem for an open-source distributed cloud platform. Our solution spreads across the cloud and distributed cloud layers and supports configuration versioning, selective dissemination to nodes in the distributed cloud layer, and logical isolation via namespaces. Our work serves as a demonstration of the feasibility and usability of the new cloud-extending models and provides valuable insight into one of the possible implementations.",
    "Link": "http://arxiv.org/abs/2410.20276v1",
    "PDF Link": "http://arxiv.org/pdf/2410.20276v1"
  },
  {
    "Title": "A Cost-Effective Strategy for Storing Scientific Datasets with Multiple\n  Service Providers in the Cloud",
    "Authors": "Dong Yuan, Lizhen Cui, Xiao Liu, Erjiang Fu, Yun Yang",
    "Published": "2016-01-26T14:07:45Z",
    "Summary": "Cloud computing provides scientists a platform that can deploy computation and data intensive applications without infrastructure investment. With excessive cloud resources and a decision support system, large generated data sets can be flexibly 1 stored locally in the current cloud, 2 deleted and regenerated whenever reused or 3 transferred to cheaper cloud service for storage. However, due to the pay for use model, the total application cost largely depends on the usage of computation, storage and bandwidth resources, hence cutting the cost of cloud based data storage becomes a big concern for deploying scientific applications in the cloud. In this paper, we propose a novel strategy that can cost effectively store large generated data sets with multiple cloud service providers. The strategy is based on a novel algorithm that finds the trade off among computation, storage and bandwidth costs in the cloud, which are three key factors for the cost of data storage. Both general (random) simulations conducted with popular cloud service providers pricing models and three specific case studies on real world scientific applications show that the proposed storage strategy is highly cost effective and practical for run time utilization in the cloud.",
    "Link": "http://arxiv.org/abs/1601.07028v1",
    "PDF Link": "http://arxiv.org/pdf/1601.07028v1"
  },
  {
    "Title": "A Context Aware and Self Adaptation Strategy for Cloud Service Selection\n  and Configuration in Run Time",
    "Authors": "Asmae Benali, Bouchra El Asri",
    "Published": "2021-04-01T23:49:33Z",
    "Summary": "Day after day, the number of mobile applications deployed on cloud computing continues in increasing because o f smartphone capabilities improvement. Cloud computing has already succeeded in the web based application, for that reason, the demand for context aware services provided by cloud computing increases. To customize a cloud service that takes into account th e consumer requirements, which depend on information change, it brings to light many recent challenges to cloud computing about environment aware, location aware, time aware. The cloud provider, moreover, has to manage personalized applications and the con straints of mobile devices in matters of interaction abilities and communication restrictions. This paper proposes a strategy for selecting automatically an appropriate cloud environment that runs out whole requirements, defines a configuration for the ass ociated cloud environment and able to easily adapt to the change of the environment on either the user or the cloud side or both. This process builds on the principles of dynamic software product lines, Agent oriented software engineering, and the MAPE k m odel to select and configure cloud environments according to the consumer needs and the context change.",
    "Link": "http://arxiv.org/abs/2104.00813v1",
    "PDF Link": "http://arxiv.org/pdf/2104.00813v1"
  },
  {
    "Title": "Vulnerability of Finitely-long Blockchains in Securing Data",
    "Authors": "Yiming Jiang, Jiangfan Zhang",
    "Published": "2023-04-19T20:55:59Z",
    "Summary": "Recently, blockchain has been applied in various fields to secure data exchanges and storage in decentralized systems. In a blockchain application where the task of the application which makes use of the data stored in a blockchain has to be accomplished by a time instant, the employed blockchain is essentially finitely-long. In this paper, we consider a general finitely-long blockchain model which is generalized from most existing works on finitely-long blockchain applications, and take the first step towards characterizing the vulnerability of finitely-long blockchains in securing data against double-spending attacks. For the first time, we develop a general closed-form expression for the probability of success in launching a double-spending attack on a finitely-long blockchain. This probability essentially characterizes the vulnerability of finitely-long blockchains. Then, we prove that the probability of success in launching a double-spending attack on a finitely-long blockchain is no greater than that on an infinitely-long blockchain, which implies that finitely-long blockchains are less vulnerable to double-spending attacks than infinitely-long blockchains. Moreover, we show that unlike infinitely-long blockchains which can be surely paralyzed by a 51% attack, finitely-long blockchains are more resistant to 51% attacks.",
    "Link": "http://arxiv.org/abs/2304.09965v1",
    "PDF Link": "http://arxiv.org/pdf/2304.09965v1"
  },
  {
    "Title": "A Framework for Blockchain Interoperability and Runtime Selection",
    "Authors": "Philipp Frauenthaler, Michael Borkowski, Stefan Schulte",
    "Published": "2019-05-15T13:42:46Z",
    "Summary": "The suitability of a particular blockchain for a given use case depends mainly on the blockchain's functional and non-functional properties. Such properties may vary over time, and thus, a selected blockchain may become unsuitable for a given use case. This uncertainty may hinder the widespread adoption of blockchain technologies in general. To mitigate the impact of volatile blockchain properties, we propose a framework that monitors several blockchains, allows the user to define functional and non-functional requirements, determines the most appropriate blockchain, and enables the switchover to that chain at runtime. Our evaluation using a reference implementation shows that switching to another blockchain can save cost and enable users to benefit from better performance and a higher level of trust.",
    "Link": "http://arxiv.org/abs/1905.07014v1",
    "PDF Link": "http://arxiv.org/pdf/1905.07014v1"
  },
  {
    "Title": "Blockchain Technologies for Smart Energy Systems: Fundamentals,\n  Challenges and Solutions",
    "Authors": "Naveed UL Hassan, Chau Yuen, Dusit Niyato",
    "Published": "2019-09-06T13:55:14Z",
    "Summary": "In this paper, we discuss the integration of blockchain in smart energy systems. We present various blockchain technology solutions, review important blockchain platforms, and several blockchain based smart energy projects in different smart energy domains. The majority of blockchain platforms with embedded combination of blockchain technology solutions are computing- and resource- intensive, and hence not entirely suitable for smart energy applications. We consider the requirements of smart energy systems and accordingly identify appropriate blockchain technology solutions for smart energy applications. Our analysis can help in the development of flexible blockchain platforms for smart energy systems.",
    "Link": "http://arxiv.org/abs/1909.02914v1",
    "PDF Link": "http://arxiv.org/pdf/1909.02914v1"
  },
  {
    "Title": "Testimonium: A Cost-Efficient Blockchain Relay",
    "Authors": "Philipp Frauenthaler, Marten Sigwart, Christof Spanring, Stefan Schulte",
    "Published": "2020-02-26T13:49:47Z",
    "Summary": "Current blockchain technologies provide very limited means of interoperability. In particular, solutions enabling blockchains to verify the existence of data on other blockchains are either very costly or are not fully decentralized. To overcome these limitations, we introduce Testimonium, a novel blockchain relay scheme that applies a validation-on-demand pattern and the on-chain execution of Simplified Payment Verifications to enable the verification of data across blockchains while remaining fully decentralized. Evaluating the scheme for Ethereum-based blockchains shows that Testimonium achieves a cost reduction of up to 92% over existing solutions. As such, the scheme lays a strong foundation for generic blockchain interoperability. For instance, it enables the development of an atomic-commit protocol for distributed transactions across blockchains.",
    "Link": "http://arxiv.org/abs/2002.12837v1",
    "PDF Link": "http://arxiv.org/pdf/2002.12837v1"
  },
  {
    "Title": "Selecting Reliable Blockchain Peers via Hybrid Blockchain Reliability\n  Prediction",
    "Authors": "Peilin Zheng, Zibin Zheng, Liang Chen",
    "Published": "2019-10-31T17:02:07Z",
    "Summary": "Blockchain and blockchain-based decentralized applications are attracting increasing attentions recently. In public blockchain systems, users usually connect to third-party peers or run a peer to join the P2P blockchain network. However, connecting to unreliable blockchain peers will make users waste resources and even lose millions of dollars of cryptocurrencies. In order to select the reliable blockchain peers, it is urgently needed to evaluate and predict the reliability of them. Faced with this problem, we propose H-BRP, Hybrid Blockchain Reliability Prediction model to extract the blockchain reliability factors then make personalized prediction for each user. Large-scale real-world experiments are conducted on 100 blockchain requesters and 200 blockchain peers. The implement and dataset of 2,000,000 test cases are released. The experimental results show that the proposed model obtains better accuracy than other approaches.",
    "Link": "http://arxiv.org/abs/1910.14614v1",
    "PDF Link": "http://arxiv.org/pdf/1910.14614v1"
  },
  {
    "Title": "Managing Blockchain Systems and Applications: A Process Model for\n  Blockchain Configurations",
    "Authors": "Olga Labazova, Erol Kazan, Tobias Dehling, Tuure Tuunanen, Ali Sunyaev",
    "Published": "2021-04-16T14:49:38Z",
    "Summary": "Blockchain is a radical innovation with a unique value proposition that shifts trust from institutions to algorithms. Still, the potential of blockchains remains elusive due to knowledge gaps between computer science research and socio-economic research. Building on information technology governance literature and the theory of coevolution, this study develops a process model for blockchain configurations that captures blockchain capability dimensions and application areas. We demonstrate the applicability of the proposed blockchain configuration process model on four blockchain projects. The proposed blockchain configuration process model assists with the selection and configuration of blockchain systems based on a set of known requirements for a blockchain project. Our findings contribute to research by bridging knowledge gaps between computer science and socio-economic research on blockchain. Specifically, we explore existing blockchain concepts and integrate them in a process model for blockchain configurations.",
    "Link": "http://arxiv.org/abs/2105.02118v1",
    "PDF Link": "http://arxiv.org/pdf/2105.02118v1"
  },
  {
    "Title": "A Logic of Blockchain Updates",
    "Authors": "Kai Brünnler, Dandolo Flumini, Thomas Studer",
    "Published": "2017-07-06T13:03:04Z",
    "Summary": "Blockchains are distributed data structures that are used to achieve consensus in systems for cryptocurrencies (like Bitcoin) or smart contracts (like Ethereum). Although blockchains gained a lot of popularity recently, there is no logic-based model for blockchains available. We introduce BCL, a dynamic logic to reason about blockchain updates, and show that BCL is sound and complete with respect to a simple blockchain model.",
    "Link": "http://arxiv.org/abs/1707.01766v1",
    "PDF Link": "http://arxiv.org/pdf/1707.01766v1"
  },
  {
    "Title": "A Framework for Blockchain-Based Applications",
    "Authors": "Ephraim Feig",
    "Published": "2018-03-02T15:27:39Z",
    "Summary": "Blockchains have recently generated explosive interest from both academia and industry, with many proposed applications. But descriptions of many these proposals are more visionary projections than realizable proposals, and even basic definitions are often missing. We define \"blockchain\" and \"blockchain network\", and then discuss two very different, well known classes of blockchain networks: cryptocurrencies and Git repositories. We identify common primitive elements of both and use them to construct a framework for explicitly articulating what characterizes blockchain networks. The framework consists of a set of questions that every blockchain initiative should address at the very outset. It is intended to help one decide whether or not blockchain is an appropriate approach to a particular application, and if it is, to assist in its initial design stage.",
    "Link": "http://arxiv.org/abs/1803.00892v1",
    "PDF Link": "http://arxiv.org/pdf/1803.00892v1"
  },
  {
    "Title": "Scalable Multi-Chain Coordination via the Hierarchical Longest Chain\n  Rule",
    "Authors": "Yanni Georghiades, Karl Kreder, Jonathan Downing, Alan Orwick, Sriram Vishwanath",
    "Published": "2021-12-21T10:10:51Z",
    "Summary": "This paper introduces BlockReduce, a Proof-of-Work (PoW) based blockchain system which achieves high transaction throughput through a hierarchy of merged mined blockchains, each operating in parallel on a partition the overall application state. Most notably, the full PoW available within the network is applied to all blockchains in BlockReduce, and cross-blockchain state transitions are enabled seamlessly within the core protocol. This paper shows that, given a hierarchy of blockchains and its associated security model, the protocol scales superlinearly in transaction throughput with the number of blockchains operated by the protocol.",
    "Link": "http://arxiv.org/abs/2112.11072v2",
    "PDF Link": "http://arxiv.org/pdf/2112.11072v2"
  },
  {
    "Title": "A Decision Framework for Blockchain Adoption",
    "Authors": "Vittorio Capocasale, Guido Perboli",
    "Published": "2022-10-24T11:50:18Z",
    "Summary": "Blockchain and distributed ledger technologies are gaining the interest of the academy, companies, and institutions. Nonetheless, the path toward blockchain adoption is not straightforward, as blockchain is a complex technology that requires revisiting the standard way of addressing problems and tackling them from a decentralized perspective. Thus, decision-makers adopt blockchain technology for the wrong reasons or prefer it to more suitable ones. This work presents a decision framework for blockchain adoption to help decision-makers decide whether blockchain is applicable, valuable, and preferable to other technologies. In particular, The decision framework is composed of a small set of questions that can be answered from a managerial standpoint and that do not require a deep technical knowledge of blockchain-related topics.",
    "Link": "http://arxiv.org/abs/2210.14888v1",
    "PDF Link": "http://arxiv.org/pdf/2210.14888v1"
  },
  {
    "Title": "ChainSplitter: Towards Blockchain-based Industrial IoT Architecture for\n  Supporting Hierarchical Storage",
    "Authors": "Gang Wang, Zhijie Jerry Shi, Mark Nixon, Song Han",
    "Published": "2019-10-02T01:37:20Z",
    "Summary": "The fast developing Industrial Internet of Things (IIoT) technologies provide a promising opportunity to build large-scale systems to connect numerous heterogeneous devices into the Internet. Most existing IIoT infrastructures are based on a centralized architecture, which is easier for management but cannot effectively support immutable and verifiable services among multiple parties. Blockchain technology provides many desired features for large-scale IIoT infrastructures, such as decentralization, trustworthiness, trackability, and immutability. This paper presents a blockchain-based IIoT architecture to support immutable and verifiable services. However, when applying blockchain technology to the IIoT infrastructure, the required storage space posts a grant challenge to resource-constrained IIoT infrastructures. To address the storage issue, this paper proposes a hierarchical blockchain storage structure, \\textit{ChainSplitter}. Specially, the proposed architecture features a hierarchical storage structure where the majority of the blockchain is stored in the clouds, while the most recent blocks are stored in the overlay network of the individual IIoT networks. The proposed architecture seamlessly binds local IIoT networks, the blockchain overlay network, and the cloud infrastructure together through two connectors, the \\textit{blockchain connector} and the \\textit{cloud connector}, to construct the hierarchical blockchain storage. The blockchain connector in the overlay network builds blocks in blockchain from data generated in IIoT networks, and the cloud connector resolves the blockchain synchronization issues between the overlay network and the clouds. We also provide a case study to show the efficiency of the proposed hierarchical blockchain storage in a practical Industrial IoT case.",
    "Link": "http://arxiv.org/abs/1910.00742v1",
    "PDF Link": "http://arxiv.org/pdf/1910.00742v1"
  },
  {
    "Title": "A Survey of Blockchain Data Management Systems",
    "Authors": "Qian Wei, Bingzhe Li, Wanli Chang, Zhiping Jia, Zhaoyan Shen, Zili Shao",
    "Published": "2021-11-25T07:13:15Z",
    "Summary": "Blockchain has been widely deployed in various sectors, such as finance, education, and public services. Since blockchain runs as an immutable distributed ledger, it has decentralized mechanisms with persistency, anonymity, and auditability, where transactions are jointly performed through cryptocurrency-based consensus algorithms by worldwide distributed nodes. There have been many survey papers reviewing the blockchain technologies from different perspectives, e.g., digital currencies, consensus algorithms, and smart contracts. However, none of them have focused on the blockchain data management systems. To fill in this gap, we have conducted a comprehensive survey on the data management systems, based on three typical types of blockchain, i.e., standard blockchain, hybrid blockchain, and DAG (Directed Acyclic Graph)-based blockchain. We categorize their data management mechanisms into three layers: blockchain architecture, blockchain data structure, and blockchain storage engine, where block architecture indicates how to record transactions on a distributed ledger, blockchain data structure refers to the internal structure of each block, and blockchain storage engine specifies the storage form of data on the blockchain system. For each layer, the works advancing the state-of-the-art are discussed together with technical challenges. Furthermore, we lay out the future research directions for the blockchain data management systems.",
    "Link": "http://arxiv.org/abs/2111.13683v1",
    "PDF Link": "http://arxiv.org/pdf/2111.13683v1"
  },
  {
    "Title": "A Consensus Algorithm Based on Risk Assessment Model for Permissioned\n  Blockchain",
    "Authors": "Xiaohui Zhang, Mingying Xue, Xianghua Miao",
    "Published": "2022-07-15T13:01:00Z",
    "Summary": "Blockchain technology enables stakeholders to conduct trusted data sharing and exchange without a trusted centralized institution. These features make blockchain applications attractive to enhance trustworthiness in very different contexts. Due to unique design concepts and outstanding performance, blockchain has become a popular research topic in industry and academia in recent years. Every participant is anonymous in a permissionless blockchain represented by cryptocurrency applications such as Bitcoin. In this situation, some special incentive mechanisms are applied to permissionless blockchain, such as mined native cryptocurrency to solve the trust issues of permissionless blockchain. In many use cases, permissionless blockchain has bottlenecks in transaction throughput performance, which restricts further application in the real world. A permissioned blockchain can reach a consensus among a group of entities that do not establish an entire trust relationship. Unlike permissionless blockchains, the participants must be identified in permissioned blockchains. By relying on the traditional crash fault-tolerant consensus protocols, permissioned blockchains can achieve high transaction throughput and low latency without sacrificing security. However, how to balance the security and consensus efficiency is still the issue that needs to be solved urgently in permissioned blockchains. As the core module of blockchain technology, the consensus algorithm plays a vital role in the performance of the blockchain system. Thus, this paper proposes a new consensus algorithm for permissioned blockchain, the Risk Assessment-based Consensus protocol (RAC), combined with the decentralized design concept and the risk-node assessment mechanism to address the unbalance issues of performance in speed, scalability, and security.",
    "Link": "http://arxiv.org/abs/2207.07453v1",
    "PDF Link": "http://arxiv.org/pdf/2207.07453v1"
  },
  {
    "Title": "Towards the Blockchain Massive Adoption with Permissionless Storage",
    "Authors": "Jia Kan",
    "Published": "2024-07-25T04:28:52Z",
    "Summary": "Blockchain technology emerged with the advent of Bitcoin and rapidly developed over the past few decades, becoming widely accepted and known by the public. However, in the past decades, the massive adoption of blockchain technology has yet to come. Rather than the scalability issue, the blockchain application is challenged by its expensive usage cost. However, the high cost of blockchain usage is deeply connected with the blockchain consensus and security mechanism. The permissionless blockchain must maintain its high cost for security against the 51% Attack. Chain users indirectly cover the cost as coins are appointed for blockchain usage fees. This conflict prevents the massive adoption of blockchain. Thus, blockchain must be improved to solve those problems: 1. The cost of blockchain usage should be low enough. 2. The blockchain should remain decentralized. 3. The scalability of blockchain must meet the demand.   In my thesis, new approaches are applied to solve the issues above. The key contribution is the discovery of the useful PoW. It extends the Nakamoto PoW with another usage of file data encoding during the same Nakamoto Consensus computation to prove honest data preservation. Based on this theory, a permissionless storage network is proposed as the new security engine for the blockchain. It bridges the high blockchain security cost to the storage users with real demands who are willing to pay for the storage resource. On the other hand, the chain users can benefit from the low transaction fee. Meanwhile, we also provide a scalability solution to shard the blockchain. It enables high TPS and keeps decentralization. The solutions in this thesis provide the answers to all the dependencies of the massive adoption.",
    "Link": "http://arxiv.org/abs/2407.17761v1",
    "PDF Link": "http://arxiv.org/pdf/2407.17761v1"
  },
  {
    "Title": "Blockchain Mutability: Challenges and Proposed Solutions",
    "Authors": "Eugenia Politou, Fran Casino, Efthimios Alepis, Constantinos Patsakis",
    "Published": "2019-07-16T16:23:25Z",
    "Summary": "Blockchain's evolution during the past decade is astonishing: from bitcoin to over 2.000 altcoins, and from decentralised electronic payments to transactions programmable by smart contracts and complex tokens governed by decentralised organisations. While the new generation of blockchain applications is still evolving, blockchain's technical characteristics are also advancing. Yet, immutability, a hitherto indisputable property according to which blockchain data cannot be edited nor deleted, remains the cornerstone of blockchain's security. Nevertheless, blockchain's immutability is being called into question lately in the light of the new erasing requirements imposed by the GDPR's ``\\textit{Right to be Forgotten (RtbF)}'' provision. As the RtbF obliges blockchain data to be editable in order restricted content redactions, modifications or deletions to be applied when requested, blockchains compliance with the regulation is indeed challenging, if not impracticable. Towards resolving this contradiction, various methods and techniques for mutable blockchains have been proposed in an effort to satisfy regulatory erasing requirements while preserving blockchains' security. To this end, this work aims to provide a comprehensive review on the state-of-the-art research approaches, technical workarounds and advanced cryptographic techniques that have been put forward to resolve this conflict and to discuss their potentials, constraints and limitations when applied in the wild to either permissioned or permissionless blockchains.",
    "Link": "http://arxiv.org/abs/1907.07099v1",
    "PDF Link": "http://arxiv.org/pdf/1907.07099v1"
  },
  {
    "Title": "Performance Analysis of the Libra Blockchain: An Experimental Study",
    "Authors": "Jiashuo Zhang, Jianbo Gao, Zhenhao Wu, Wentian Yan, Qize Wu, Qingshan Li, Zhong Chen",
    "Published": "2019-12-11T11:33:36Z",
    "Summary": "Since Bitcoin was first introduced in 2008, many types of cryptocurrencies have been proposed based on blockchain. However, the performance of permissionless blockchains restricts the widespread of cryptocurrency. Recently, Libra was proposed by Facebook based on a permissioned blockchain, i.e. the Libra blockchain. The vision of Libra is to become a global currency supporting financial applications, but it is doubted whether the performance of the Libra blockchain is able to support frequent micropayment scenarios. In this paper, we propose a methodology to evaluate the performance of blockchain platforms and conducted an experimental study on the Libra blockchain. The results show that the Libra blockchain can only process about one thousand transactions per second at most, and the performance drops significantly as the number of validators increases. Although it outperforms permissionless blockchain platforms, the performance of the Libra blockchain is still unsatisfactory compared to other permissioned blockchains like Hyperledger Fabric and needs to make effective improvements in order to support global micropayment in the future.",
    "Link": "http://arxiv.org/abs/1912.05241v1",
    "PDF Link": "http://arxiv.org/pdf/1912.05241v1"
  },
  {
    "Title": "State sharding model on the blockchain",
    "Authors": "Xiangyu Wang, Ting Yang, Yu Wang",
    "Published": "2020-10-30T02:55:19Z",
    "Summary": "Blockchain is an incrementally updated ledger maintained by distributed nodes rather than centralized organizations. The current blockchain technology faces scalability issues, which include two aspects: low transaction throughput and high storage capacity costs. This paper studies the blockchain structure based on state sharding technology, and mainly solves the problem of non-scalability of block chain storage. This paper designs and implements the blockchain state sharding scheme, proposes a specific state sharding data structure and algorithm implementation, and realizes a complete blockchain structure so that the blockchain has the advantages of high throughput, processing a large number of transactions and saving storage costs. Experimental results show that a blockchain network with more than 100,000 nodes can be divided into 1024 shards. A blockchain network with this structure can process 500,000 transactions in about 5 seconds. If the consensus time of the blockchain is about 10 seconds, and the block generation time of the blockchain system of the sharding mechanism is 15 seconds, the transaction throughput can reach 33,000 tx/sec. Experimental results show that the throughput of the proposed protocol increases with the increase of the network node size. This confirms the scalability of the blockchain structure based on sharding technology.",
    "Link": "http://arxiv.org/abs/2010.16034v1",
    "PDF Link": "http://arxiv.org/pdf/2010.16034v1"
  },
  {
    "Title": "Distributed Nonblocking Commit Protocols for Many-Party Cross-Blockchain\n  Transactions",
    "Authors": "Xinying Wang, Olamide Timothy Tawose, Feng Yan, Dongfang Zhao",
    "Published": "2020-01-05T05:58:41Z",
    "Summary": "The interoperability across multiple blockchains would play a critical role in future blockchain-based data management paradigm. Existing techniques either work only for two blockchains or requires a centralized component to govern the cross-blockchain transaction execution, neither of which would meet the scalability requirement. This paper proposes a new distributed commit protocol, namely \\textit{cross-blockchain transaction} (CBT), for conducting transactions across an arbitrary number of blockchains without any centralized component. The key idea of CBT is to extend the two-phase commit protocol with a heartbeat mechanism to ensure the liveness of CBT without introducing additional nodes or blockchains. We have implemented CBT and compared it to the state-of-the-art protocols, demonstrating CBT's low overhead (3.6\\% between two blockchains, less than $1\\%$ among 32 or more blockchains) and high scalability (linear scalability on up to 64-blockchain transactions). In addition, we developed a graphic user interface for users to virtually monitor the status of the cross-blockchain transactions.",
    "Link": "http://arxiv.org/abs/2001.01174v1",
    "PDF Link": "http://arxiv.org/pdf/2001.01174v1"
  },
  {
    "Title": "Novel Architecture to Create and Maintain Personal Blockchains",
    "Authors": "Collin Connors, Dilip Sarkar",
    "Published": "2022-12-12T02:05:59Z",
    "Summary": "Blockchain has been touted as a revolutionary technology. However, despite the excitement, blockchain has not been adopted in many fields. Many are hesitant to adopt blockchain technology due to privacy concerns, barriers to use, or lack of practical use cases. In this work, we outline a potential blockchain use case for tracking financial transactions across multiple financial institutions. We show the downsides of traditional centralized approaches and that blockchain approaches fail to give all the privacy and accessibility required for this use case. Thus we propose a novel blockchain architecture to support our use case. This novel architecture combines the ease of use of public blockchains with the privacy of private blockchains by allowing users to create personal blockchains. We believe this novel personal blockchain architecture will lead to more blockchain adoption, particularly in use cases handling private data.",
    "Link": "http://arxiv.org/abs/2212.14671v1",
    "PDF Link": "http://arxiv.org/pdf/2212.14671v1"
  },
  {
    "Title": "Rateless Coded Blockchain for Dynamic IoT Networks",
    "Authors": "Changlin Yang, Alexei Ashikhmin, Xiaodong Wang, Zibin Zheng",
    "Published": "2023-05-06T02:15:00Z",
    "Summary": "A key constraint that limits the implementation of blockchain in Internet of Things (IoT) is its large storage requirement resulting from the fact that each blockchain node has to store the entire blockchain. This increases the burden on blockchain nodes, and increases the communication overhead for new nodes joining the network since they have to copy the entire blockchain. In order to reduce storage requirements without compromising on system security and integrity, coded blockchains, based on error correcting codes with fixed rates and lengths, have been recently proposed. This approach, however, does not fit well with dynamic IoT networks in which nodes actively leave and join. In such dynamic blockchains, the existing coded blockchain approaches lead to high communication overheads for new joining nodes and may have high decoding failure probability. This paper proposes a rateless coded blockchain with coding parameters adjusted to network conditions. Our goals are to minimize both the storage requirement at each blockchain node and the communication overhead for each new joining node, subject to a target decoding failure probability. We evaluate the proposed scheme in the context of real-world Bitcoin blockchain and show that both storage and communication overhead are reduced by 99.6\\% with a maximum $10^{-12}$ decoding failure probability.",
    "Link": "http://arxiv.org/abs/2305.03895v1",
    "PDF Link": "http://arxiv.org/pdf/2305.03895v1"
  },
  {
    "Title": "PBL: System for Creating and Maintaining Personal Blockchain Ledgers",
    "Authors": "Collin Connors, Dilip Sarkar",
    "Published": "2023-05-08T14:17:27Z",
    "Summary": "Blockchain technology has experienced substantial growth in recent years, yet the diversity of blockchain applications has been limited. Blockchain provides many desirable features for applications, including being append-only, immutable, tamper-evident, tamper-resistant, and fault-tolerant; however, many applications that would benefit from these features cannot incorporate current blockchains. This work presents a novel architecture for creating and maintaining personal blockchain ledgers that address these concerns. Our system utilizes independent modular services, enabling individuals to securely store their data in a personal blockchain ledger. Unlike traditional blockchain, which stores all transactions of multiple users, our novel personal blockchains are designed to allow individuals to maintain their privacy without requiring extensive technical expertise. Using rigorous mathematical methods, we prove that our system produces append-only, immutable, tamper-evident, tamper-resistant ledgers. Our system addresses use cases not addressed by traditional blockchain development platforms. Our system creates a new blockchain paradigm, enabling more individuals and applications to leverage blockchain technology for their needs.",
    "Link": "http://arxiv.org/abs/2305.04723v1",
    "PDF Link": "http://arxiv.org/pdf/2305.04723v1"
  },
  {
    "Title": "Cross-Blockchain Communication Using Oracles With an Off-Chain\n  Aggregation Mechanism Based on zk-SNARKs",
    "Authors": "Michael Sober, Giulia Scaffino, Stefan Schulte",
    "Published": "2024-05-14T07:48:19Z",
    "Summary": "The closed architecture of prevailing blockchain systems renders the usage of this technology mostly infeasible for a wide range of real-world problems. Most blockchains trap users and applications in their isolated space without the possibility of cooperating or switching to other blockchains. Therefore, blockchains need additional mechanisms for seamless communication and arbitrary data exchange between each other and external systems. Unfortunately, current approaches for cross-blockchain communication are resource-intensive or require additional blockchains or tailored solutions depending on the applied consensus mechanisms of the connected blockchains. Therefore, we propose an oracle with an off-chain aggregation mechanism based on ZeroKnowledge Succinct Non-interactive Arguments of Knowledge (zk-SNARKs) to facilitate cross-blockchain communication. The oracle queries data from another blockchain and applies a rollup-like mechanism to move state and computation off-chain. The zkOracle contract only expects the transferred data, an updated state root, and proof of the correct execution of the aggregation mechanism. The proposed solution only requires constant 378 kgas to submit data on the Ethereum blockchain and is primarily independent of the underlying technology of the queried blockchains.",
    "Link": "http://arxiv.org/abs/2405.08395v1",
    "PDF Link": "http://arxiv.org/pdf/2405.08395v1"
  },
  {
    "Title": "Exploring Blockchain Technology through a Modular Lens: A Survey",
    "Authors": "Minghui Xu, Yihao Guo, Chunchi Liu, Qin Hu, Dongxiao Yu, Zehui Xiong, Dusit Niyato, Xiuzhen Cheng",
    "Published": "2023-04-13T16:16:18Z",
    "Summary": "Blockchain has attracted significant attention in recent years due to its potential to revolutionize various industries by providing trustlessness. To comprehensively examine blockchain systems, this article presents both a macro-level overview on the most popular blockchain systems, and a micro-level analysis on a general blockchain framework and its crucial components. The macro-level exploration provides a big picture on the endeavors made by blockchain professionals over the years to enhance the blockchain performance while the micro-level investigation details the blockchain building blocks for deep technology comprehension. More specifically, this article introduces a general modular blockchain analytic framework that decomposes a blockchain system into interacting modules and then examines the major modules to cover the essential blockchain components of network, consensus, and distributed ledger at the micro-level. The framework as well as the modular analysis jointly build a foundation for designing scalable, flexible, and application-adaptive blockchains that can meet diverse requirements. Additionally, this article explores popular technologies that can be integrated with blockchain to expand functionality and highlights major challenges. Such a study provides critical insights to overcome the obstacles in designing novel blockchain systems and facilitates the further development of blockchain as a digital infrastructure to service new applications.",
    "Link": "http://arxiv.org/abs/2304.08283v1",
    "PDF Link": "http://arxiv.org/pdf/2304.08283v1"
  },
  {
    "Title": "A Circuit Approach to Constructing Blockchains on Blockchains",
    "Authors": "Ertem Nusret Tas, David Tse, Yifei Wang",
    "Published": "2024-01-31T22:49:35Z",
    "Summary": "Since the creation of Bitcoin 15 years ago, there has been an explosion in the number of permissionless blockchains. Each of these blockchains provides an open ledger that anyone can read from and write to. In this multi-chain world, an important question emerges: how can we build a more secure overlay blockchain by reading from and writing to a given set of blockchains? Drawing an analogy with switching circuits, we approach the problem by defining two basic compositional operations between blockchains, serial and triangular compositions, and use these operations as building blocks to construct general overlay blockchains. Under the partially synchronous setting, we have the following results: 1) the serial composition, between two blockchains, yields an overlay blockchain that is safe if at least one of the two underlay blockchains is safe and that is live if both of them are live; 2) the triangular composition between three blockchains, akin to parallel composition of switching circuits, yields an overlay blockchain that is safe if all underlay blockchains are safe and that is live if over half of them are live; 3) repeated composition of these two basic operations can yield all possible tradeoffs of safety and liveness for an overlay blockchain built on arbitrary number of underlay chains. The results are also extended to the synchronous setting.",
    "Link": "http://arxiv.org/abs/2402.00220v4",
    "PDF Link": "http://arxiv.org/pdf/2402.00220v4"
  },
  {
    "Title": "A Multi-Layered Security Analysis of Blockchain Systems: From Attack\n  Vectors to Defense and System Hardening",
    "Authors": "Yuhuan Yang, Shipeng Ye, Xiaoqi Li",
    "Published": "2025-04-12T11:23:47Z",
    "Summary": "The application of Bitcoin enables people to understand blockchain technology gradually. Bitcoin is a decentralized currency that does not rely on third-party credit institutions, and the core of Bitcoin's underlying technology is blockchain. With the increasing value of Bitcoin and the vigorous development of decentralization, people's research on blockchain is also increasing day by day. Today's blockchain technology has not only made great achievements in the application of Bitcoin, but has also been preliminarily applied in other fields, such as finance, medical treatment, the Internet of Things, and so on. However, with the initial application of blockchain technology on the Internet, the security of blockchain technology has also been widely concerned by people in the industry. For example, whether currency trading platforms, smart contracts, blockchain consensus mechanisms, and other technologies are vulnerable to attacks, and how we can defend against these attacks digitally and optimize the blockchain system is exactly the subject we want to study. For the security of appeal blockchain, this paper first analyzes the security threats faced by the application digital currency trading platform of the blockchain system, then analyzes the security problems of smart contract closely related to blockchain 2.0, and then analyzes and studies the security threats of blockchain public chain, consensus mechanism, and P2P. Finally, combined with the security problems at all levels of the blockchain system we analyze and study how to optimize the security of the blockchain system.",
    "Link": "http://arxiv.org/abs/2504.09181v1",
    "PDF Link": "http://arxiv.org/pdf/2504.09181v1"
  },
  {
    "Title": "Blockchain: A Graph Primer",
    "Authors": "Cuneyt Gurcan Akcora, Yulia R. Gel, Murat Kantarcioglu",
    "Published": "2017-08-10T16:45:00Z",
    "Summary": "Bitcoin and its underlying technology, blockchain, have gained significant popularity in recent years. Satoshi Nakamoto designed Bitcoin to enable a secure, distributed platform without the need for central authorities, and blockchain has been hailed as a paradigm that will be as impactful as Big Data, Cloud Computing, and Machine Learning.   Blockchain incorporates innovative ideas from various fields, such as public-key encryption and distributed systems. As a result, readers often encounter resources that explain Blockchain technology from a single perspective, leaving them with more questions than answers.   In this primer, we aim to provide a comprehensive view of blockchain. We will begin with a brief history and introduce the building blocks of the blockchain. As graph mining is a major area of blockchain analysis, we will delve into the graph-theoretical aspects of Blockchain technology. We will also discuss the future of blockchain and explain how extensions such as smart contracts and decentralized autonomous organizations will function.   Our goal is to provide a concise but complete description of blockchain technology that is accessible to readers with no prior expertise in the field.",
    "Link": "http://arxiv.org/abs/1708.08749v2",
    "PDF Link": "http://arxiv.org/pdf/1708.08749v2"
  },
  {
    "Title": "Blockchain in the Eyes of Developers",
    "Authors": "He Jiang, Dong Liu, Zhilei Ren, Tao Zhang",
    "Published": "2018-06-19T07:39:47Z",
    "Summary": "The popularity of blockchain technology continues to grow rapidly in both industrial and academic fields. Most studies of blockchain focus on the improvements of security, usability, or efficiency of blockchain protocols, or the applications of blockchain in finance, Internet of Things, or public services. However, few of them could reveal the concerns of front-line developers and the situations of blockchain in practice. In this article, we investigate how developers use and discuss blockchain with a case study of Stack Overflow posts. We find blockchain is a relatively new topic in Stack Overflow but it is rising to popularity. We detect 13 types of questions that developers post in Stack Overflow and identify 45 blockchain relevant entities (e.g., frameworks, libraries, or tools) for building blockchain applications. These findings may help blockchain project communities to know where to improve and help novices to know where to start.",
    "Link": "http://arxiv.org/abs/1806.07080v1",
    "PDF Link": "http://arxiv.org/pdf/1806.07080v1"
  },
  {
    "Title": "Towards Global Asset Management in Blockchain Systems",
    "Authors": "Victor Zakhary, Mohammad Javad Amiri, Sujaya Maiyya, Divyakant Agrawal, Amr El Abbadi",
    "Published": "2019-05-22T20:44:36Z",
    "Summary": "Permissionless blockchains (e.g., Bitcoin, Ethereum, etc) have shown a wide success in implementing global scale peer-to-peer cryptocurrency systems. In such blockchains, new currency units are generated through the mining process and are used in addition to transaction fees to incentivize miners to maintain the blockchain. Although it is clear how currency units are generated and transacted on, it is unclear how to use the infrastructure of permissionless blockchains to manage other assets than the blockchain's currency units (e.g., cars, houses, etc). In this paper, we propose a global asset management system by unifying permissioned and permissionless blockchains. A governmental permissioned blockchain authenticates the registration of end-user assets through smart contract deployments on a permissionless blockchain. Afterwards, end-users can transact on their assets through smart contract function calls (e.g., sell a car, rent a room in a house, etc). In return, end-users get paid in currency units of the same blockchain or other blockchains through atomic cross-chain transactions and governmental offices receive taxes on these transactions in cryptocurrency units.",
    "Link": "http://arxiv.org/abs/1905.09359v1",
    "PDF Link": "http://arxiv.org/pdf/1905.09359v1"
  },
  {
    "Title": "Research on the Security of Blockchain Data: A Survey",
    "Authors": "Liehuang Zhu, Baokun Zheng, Meng Shen, Shui Yu, Feng Gao, Hongyu Li, Kexin Shi, Keke Gai",
    "Published": "2018-12-05T14:09:25Z",
    "Summary": "With the more and more extensive application of blockchain, blockchain security has been widely concerned by the society and deeply studied by scholars. Moreover, the security of blockchain data directly affects the security of various applications of blockchain. In this survey, we perform a comprehensive classification and summary of the security of blockchain data. First, we present classification of blockchain data attacks. Subsequently, we present the attacks and defenses of blockchain data in terms of privacy, availability, integrity and controllability. Data privacy attacks present data leakage or data obtained by attackers through analysis. Data availability attacks present abnormal or incorrect access to blockchain data. Data integrity attacks present blockchain data being tampered. Data controllability attacks present blockchain data accidentally manipulated by smart contract vulnerability. Finally, we present several important open research directions to identify follow-up studies in this area.",
    "Link": "http://arxiv.org/abs/1812.02009v2",
    "PDF Link": "http://arxiv.org/pdf/1812.02009v2"
  },
  {
    "Title": "Segment blockchain: A size reduced storage mechanism for blockchain",
    "Authors": "Yibin Xu, Yangyu Huang",
    "Published": "2020-01-20T08:54:17Z",
    "Summary": "The exponential growth of the blockchain size has become a major contributing factor that hinders the decentralisation of blockchain and its potential implementations in data-heavy applications. In this paper, we propose segment blockchain, an approach that segmentises blockchain and enables nodes to only store a copy of one blockchain segment. We use \\emph{PoW} as a membership threshold to limit the number of nodes taken by an Adversary---the Adversary can only gain at most $n/2$ of nodes in a network of $n$ nodes when it has $50\\%$ of the calculation power in the system (the Nakamoto blockchain security threshold). A segment blockchain system fails when an Adversary stores all copies of a segment, because the Adversary can then leave the system, causing a permanent loss of the segment. We theoretically prove that segment blockchain can sustain a $(AD/n)^m$ failure probability when the Adversary has no more than $AD$ number of nodes and every segment is stored by $m$ number of nodes. The storage requirement is mostly shrunken compared to the traditional design and therefore making the blockchain more suitable for data-heavy applications.",
    "Link": "http://arxiv.org/abs/2001.07023v1",
    "PDF Link": "http://arxiv.org/pdf/2001.07023v1"
  },
  {
    "Title": "Cross-chain Interaction Model In a Fully Verified Way",
    "Authors": "Hong Su",
    "Published": "2021-06-10T02:47:58Z",
    "Summary": "There are different kinds of blockchains, which have been applied in various areas. Blockchains are relatively independent systems that are apt to form isolated data islands. Then cross-chain interaction is proposed to connect different blockchains. However, the current cross-chain methods do not maintain the security of the original blockchain. They either depend on a less secure third-party system or a less secure method. This makes the cross-chain interaction less secure than the original blockchains (the security downgrade issues), or the cross-chain interaction can be done even if the paired blockchain does not exist (the blockchain invisible issue). In this paper, we first propose a system interaction model and use it to analyze the possible security issues. Based on conclusions got from the proposed model, we propose the cross-chain method that verifies the data of the paired blockchain by the consensus algorithm of the paired blockchain (the CIFuV method). With this method, the cross-chain interaction can be as the same security as in the paired blockchain. At last, we evaluate the security issues during the system interaction process, and the possibility to have the CIFuV model on the public blockchains.",
    "Link": "http://arxiv.org/abs/2106.05463v1",
    "PDF Link": "http://arxiv.org/pdf/2106.05463v1"
  },
  {
    "Title": "A Pattern Language for Blockchain Governance",
    "Authors": "Yue Liu, Qinghua Lu, Guangsheng Yu, Hye-Young Paik, Harsha Perera, Liming Zhu",
    "Published": "2022-03-01T07:08:12Z",
    "Summary": "Blockchain technology has been used to build next-generation applications taking advantage of its decentralised nature. Nevertheless, there are some serious concerns about the trustworthiness of blockchain due to the vulnerabilities in on-chain algorithmic mechanisms, and tedious disputes and debates in off-chain communities. Accordingly, blockchain governance has received great attention for improving the trustworthiness of all decisions that direct a blockchain platform. However, there is a lack of systematic knowledge to guide practitioners to perform blockchain governance. We have performed a systematic literature review to understand the state-of-the-art of blockchain governance. We identify the lifecycle stages of a blockchain platform, and present 14 architectural patterns for blockchain governance in this study. This pattern language can provide guidance for the effective use of patterns for blockchain governance in practice, and support the architecture design of governance-driven blockchain systems.",
    "Link": "http://arxiv.org/abs/2203.00268v3",
    "PDF Link": "http://arxiv.org/pdf/2203.00268v3"
  },
  {
    "Title": "Vulnerablity analysis of Azure Blockchain Workbench key management\n  system",
    "Authors": "Dmitry Tanana",
    "Published": "2023-01-27T07:29:37Z",
    "Summary": "With rise of blockchain popularity, more and more people seek to implement blockchain technology into their projects. Most common way is to take existing blockchain stack, such as Azure Blockchain Workbench or Oracle Blockchain Platform. While the blockchain technology is well-protected by its algorithms it is still vulnerable because its privacy relies on regular cryptography. And mistakes or vulnerabilities in key management protocols can affect even the most secure blockchain projects. This article considers question of vulnerabilities within Azure Blockchain Workbench key management system. We describe potential threats for each stage of key management lifecycle based on public reports and then assess how likely are those threats to realize within Azure Blockchain Workbench environment based on the technical documentation for Azure Blockchain Workbench and Azure Key Vault. Finally, we compile results of our assessment into the key management threat table with three distinct degrees of protection: fully protected, partially protected and not protected.",
    "Link": "http://arxiv.org/abs/2301.11569v1",
    "PDF Link": "http://arxiv.org/pdf/2301.11569v1"
  },
  {
    "Title": "Generative AI-enabled Blockchain Networks: Fundamentals, Applications,\n  and Case Study",
    "Authors": "Cong T. Nguyen, Yinqiu Liu, Hongyang Du, Dinh Thai Hoang, Dusit Niyato, Diep N. Nguyen, Shiwen Mao",
    "Published": "2024-01-28T10:46:17Z",
    "Summary": "Generative Artificial Intelligence (GAI) has recently emerged as a promising solution to address critical challenges of blockchain technology, including scalability, security, privacy, and interoperability. In this paper, we first introduce GAI techniques, outline their applications, and discuss existing solutions for integrating GAI into blockchains. Then, we discuss emerging solutions that demonstrate the effectiveness of GAI in addressing various challenges of blockchain, such as detecting unknown blockchain attacks and smart contract vulnerabilities, designing key secret sharing schemes, and enhancing privacy. Moreover, we present a case study to demonstrate that GAI, specifically the generative diffusion model, can be employed to optimize blockchain network performance metrics. Experimental results clearly show that, compared to a baseline traditional AI approach, the proposed generative diffusion model approach can converge faster, achieve higher rewards, and significantly improve the throughput and latency of the blockchain network. Additionally, we highlight future research directions for GAI in blockchain applications, including personalized GAI-enabled blockchains, GAI-blockchain synergy, and privacy and security considerations within blockchain ecosystems.",
    "Link": "http://arxiv.org/abs/2401.15625v1",
    "PDF Link": "http://arxiv.org/pdf/2401.15625v1"
  },
  {
    "Title": "Blockchain for Finance: A Survey",
    "Authors": "Hanjie Wu, Qian Yao, Zhenguang Liu, Butian Huang, Yuan Zhuang, Huayun Tang, Erwu Liu",
    "Published": "2024-02-27T05:25:05Z",
    "Summary": "As an innovative technology for enhancing authenticity, security, and risk management, blockchain is being widely adopted in trade and finance systems. The unique capabilities of blockchain, such as immutability and transparency, enable new business models of distributed data storage, point-to-point transactions, and decentralized autonomous organizations. In this paper, we focus on blockchain-based securities trading, in which blockchain technology plays a vital role in financial services as it ultimately lifts trust and frees the need for third-party verification by using consensus-based verification. We investigate the 12 most popular blockchain platforms and elaborate on 6 platforms that are related to finance, seeking to provide a panorama of securities trading practices. Meanwhile, this survey provides a comprehensive summary of blockchain-based securities trading applications. We gather numerous practical applications of blockchain-based securities trading and categorize them into four distinct categories. For each category, we introduce a typical example and explain how blockchain contributes to solving the key problems faced by FinTech companies and researchers. Finally, we provide interesting observations ranging from mainstream blockchain-based financial institutions to security issues of decentralized finance applications, aiming to picture the current blockchain ecosystem in finance.",
    "Link": "http://arxiv.org/abs/2402.17219v1",
    "PDF Link": "http://arxiv.org/pdf/2402.17219v1"
  },
  {
    "Title": "PoVF: Empowering Decentralized Blockchain Systems with Verifiable\n  Function Consensus",
    "Authors": "Chenxi Xiong, Ting Yang, Yu Wang, Bing Dong",
    "Published": "2025-01-02T08:59:24Z",
    "Summary": "Consensus mechanism is the core technology for blockchain to ensure that transactions are executed in sequence. It also determines the decentralization, security, and efficiency of blockchain. Existing mechanisms all have certain centralization issues and fail to ensure the decentralization of blockchain networks. A decentralized and efficient mechanism is required to improve blockchain systems. This paper proposes a fair consensus mechanism called Proof of Verifiable Functions (PoVF), based on the verifiability and unpredictability of verifiable functions. PoVF provides a sufficiently fair mechanism, ensuring that all nodes in blockchain network have equal opportunity to participate in consensus. In addition, a structure called \"Delay buffer\" is proposed to ensure transactions are executed sequentially. It delay the selection of blocks to avoid blockchain forks caused by broadcasting and transaction execution confusion. According to our security analysis, PoVF is provably secure and has the ability to resist potential adversaries. According to the experiments, PoVF-based blockchain can process up to 4000 transactions per second with nodes configured with only 4-core CPUs. This paper uses the Gini coefficient to measure the decentralization of blockchains, and the PoVF-based blockchain achieves the lowest Gini coefficient of 0.39 among all sampled blockchains. PoVF has been shown to provide sufficient efficiency while ensuring decentralization and security through experiments.",
    "Link": "http://arxiv.org/abs/2501.01146v1",
    "PDF Link": "http://arxiv.org/pdf/2501.01146v1"
  },
  {
    "Title": "An Extended Pattern Collection for Blockchain-based Applications",
    "Authors": "Xiwei Xu, Cesare Pautasso, Sin Kuang Lo, Liming Zhu, Qinghua Lu, Ingo Weber",
    "Published": "2025-02-22T00:15:30Z",
    "Summary": "Blockchain is an emerging technology that enables new forms of decentralized software architectures, where distributed components can reach agreements on shared system states without trusting a central integration point. Blockchain provides a shared infrastructure to execute programs, called smart contracts, and to store data. Since blockchain technologies are at an early stage, there is a lack of a systematically organized knowledge providing a holistic view on designing software systems that use blockchain. We view blockchain as a component of a bigger software system, which requires patterns for using blockchain in the design of the software architecture. In this paper, we collect a list of patterns for blockchain-based applications. The pattern collection is categorized into five categories, including interaction with external world patterns, data management patterns, security patterns, structural patterns of contracts, and user interaction patterns. Some patterns are designed considering the nature of blockchain and how blockchains can be specifically introduced within real-world applications. Others are variants of existing design patterns applied in the context of blockchain-based applications and smart contracts.",
    "Link": "http://arxiv.org/abs/2502.16017v1",
    "PDF Link": "http://arxiv.org/pdf/2502.16017v1"
  },
  {
    "Title": "On the Origins and Variations of Blockchain Technologies",
    "Authors": "Alan T. Sherman, Farid Javani, Haibin Zhang, Enis Golaszewski",
    "Published": "2018-10-15T00:00:33Z",
    "Summary": "We explore the origins of blockchain technologies to better understand the enduring needs they address. We identify the five key elements of a blockchain, show embodiments of these elements, and examine how these elements come together to yield important properties in selected systems. To facilitate comparing the many variations of blockchains, we also describe the four crucial roles of blockchain participants common to all blockchains. Our historical exploration highlights the 1979 work of David Chaum whose vault system embodies many of the elements of blockchains.",
    "Link": "http://arxiv.org/abs/1810.06130v1",
    "PDF Link": "http://arxiv.org/pdf/1810.06130v1"
  },
  {
    "Title": "A Survey of Blockchain Applications in Different Domains",
    "Authors": "Wubing Chen, Zhiying Xu, Shuyu Shi, Yang Zhao, Jun Zhao",
    "Published": "2019-11-06T03:27:27Z",
    "Summary": "Blockchains have received much attention recently since they provide decentralized approaches to the creation and management of value. Many banks, Internet companies, car manufacturers, and even governments worldwide have incorporated or started considering blockchains to improve the security, scalability, and efficiency of their services. In this paper, we survey blockchain applications in different areas. These areas include cryptocurrency, healthcare, advertising, insurance, copyright protection, energy, and societal applications. Our work provides a timely summary for individuals and organizations interested in blockchains. We envision our study to motivate more blockchain applications.",
    "Link": "http://arxiv.org/abs/1911.02013v1",
    "PDF Link": "http://arxiv.org/pdf/1911.02013v1"
  },
  {
    "Title": "DeXTT: Deterministic Cross-Blockchain Token Transfers",
    "Authors": "Michael Borkowski, Marten Sigwart, Philipp Frauenthaler, Taneli Hukkinen, Stefan Schulte",
    "Published": "2019-05-15T14:19:21Z",
    "Summary": "Current blockchain technologies provide very limited interoperability. Restrictions with regards to asset transfers and data exchange between different blockchains reduce usability and comfort for users, and hinder novel developments within the blockchain space.   As a first step towards cross-blockchain interoperability, we propose the DeXTT cross-blockchain transfer protocol, which can be used to transfer a token on any number of blockchains simultaneously in a decentralized manner. We provide a reference implementation using Solidity, and evaluate its performance. We show logarithmic scalability of DeXTT with respect to the number of participating nodes, and analyze cost requirements of the transferred tokens.",
    "Link": "http://arxiv.org/abs/1905.06204v1",
    "PDF Link": "http://arxiv.org/pdf/1905.06204v1"
  },
  {
    "Title": "A Reference Architecture for Blockchain-based Peer-to-Peer IoT\n  Applications",
    "Authors": "Gowri Sankar Ramachandran, Bhaskar Krishnamachari",
    "Published": "2019-05-25T17:38:01Z",
    "Summary": "The advent of Blockchain and Distributed Ledger Technologies enable IoT and smart city application developers to conceive new types of applications and solutions for identity management, trust, and data monetization. However, architecting blockchain-based IoT applications remain challenging due to the heterogeneous nature of blockchain platforms and lack of guidelines on how to interface existing components in the IoT ecosystem with the emerging Blockchain technology. This article explains the characteristics of blockchain and IoT technologies and presents a general reference architecture that can be used to develop many blockchain-based peer-to-peer IoT applications.",
    "Link": "http://arxiv.org/abs/1905.10643v1",
    "PDF Link": "http://arxiv.org/pdf/1905.10643v1"
  },
  {
    "Title": "Pushing Software-Defined Blockchain Components onto Edge Hosts",
    "Authors": "Mayra Samaniego, Ralph Deters",
    "Published": "2019-09-22T04:00:21Z",
    "Summary": "With the advent of blockchain technology, some management tasks of IoT networks can be moved from central systems to distributed validation authorities. Cloud-centric blockchain implementations for IoT have shown satisfactory performance. However, some features of blockchain are not necessary for IoT. For instance, a competitive consensus. This research presents the idea of customizing and encapsulating the features of blockchain into software-defined components to host them on edge devices. Thus, blockchain resources can be provisioned by edge devices (e-miners) working together closer to the things layer in a cooperative manner. This research uses Edison SoC as e-miners to test the software-defined blockchain components.",
    "Link": "http://arxiv.org/abs/1909.09936v1",
    "PDF Link": "http://arxiv.org/pdf/1909.09936v1"
  },
  {
    "Title": "On the Ethereum Blockchain Structure: a Complex Networks Theory\n  Perspective",
    "Authors": "Stefano Ferretti, Gabriele D'Angelo",
    "Published": "2019-08-29T07:55:19Z",
    "Summary": "In this paper, we analyze the Ethereum blockchain using the complex networks modeling framework. Accounts acting on the blockchain are represented as nodes, while the interactions among these accounts, recorded on the blockchain, are treated as links in the network. Using this representation, it is possible to derive interesting mathematical characteristics that improve the understanding of the actual interactions happening in the blockchain. Not only, by looking at the history of the blockchain, it is possible to verify if radical changes in the blockchain evolution happened.",
    "Link": "http://arxiv.org/abs/1908.11808v1",
    "PDF Link": "http://arxiv.org/pdf/1908.11808v1"
  },
  {
    "Title": "A Security Case Study for Blockchain Games",
    "Authors": "Tian Min, Wei Cai",
    "Published": "2019-06-13T08:11:55Z",
    "Summary": "Blockchain gaming is an emerging entertainment paradigm. However, blockchain games are still suffering from security issues, due to the immature blockchain technologies and its unsophisticated developers. In this work, we analyzed the blockchain game architecture and reveal the possible penetration methods of cracking. We scanned more than 600 commercial blockchain games to summarize a security overview from the perspective of the web server and smart contract, respectively. We also conducted three case studies for blockchain games to show detailed vulnerability detection.",
    "Link": "http://arxiv.org/abs/1906.05538v1",
    "PDF Link": "http://arxiv.org/pdf/1906.05538v1"
  },
  {
    "Title": "Workflow Management on the Blockchain --- Implications and\n  Recommendations",
    "Authors": "Joerg Evermann, Henry Kim",
    "Published": "2019-03-31T16:25:10Z",
    "Summary": "Blockchain technology, originally popularized by cryptocurrencies, has been proposed as an infrastructure technology with applications in many areas of business management. Blockchains provide an immutable record of transactions, which makes them useful in situations where business actors may not fully trust each other. The distributed nature of blockchains makes them particularly suitable for inter-organizational e-Business applications. In this paper we examine the use of blockchains for executing inter-organizational workflows. We discuss architectural options and describe prototype implementations of blockchain-based workflow management systems (WfMS), highlighting differences to traditional WfMS. Our main contribution is the identification of potential problems raised by blockchain infrastructure and recommendations to address them.",
    "Link": "http://arxiv.org/abs/1904.01004v2",
    "PDF Link": "http://arxiv.org/pdf/1904.01004v2"
  },
  {
    "Title": "Anonymous State Pinning for Private Blockchains",
    "Authors": "Peter Robinson, John Brainard",
    "Published": "2019-03-07T07:12:00Z",
    "Summary": "Public blockchains such as Ethereum and Bitcoin provide transparency and accountability, and have strong non-repudiation properties, but fall far short of enterprise privacy requirements for business processes. Consequently consortiums are exploring private blockchains to keep their membership and transactions private. However, private blockchains do not provide adequate protection against potential collusion by consortium members to revert the state of the blockchain. To countenance this, the private blockchain state may be \"pinned\" to a tamper resistant public blockchain. Existing solutions offering pinning to the public blockchain would reveal the transaction rate of the private blockchain, and do not provide a mechanism to contest the validity of a pin. Moreover, they require that all transactions and members of the private blockchain be revealed. These challenges are hampering the wider adoption of private blockchain technology. We describe the primary author's `Anonymous State Pinning approach', which overcomes these limitations and present a security proof to demonstrate pins can be challenged without compromising these properties. We perform a gas cost analysis of the implementation to estimate the operating cost of this technology, which shows that pinning a private blockchain at the rate of one pin per hour would cost US$508 per year. A hierarchical pinning approach is proposed which would allow many private blockchains to pin to a management blockchain which would then pin to Ethereum MainNet. This approach saves money, but at the cost of increased finality times.",
    "Link": "http://arxiv.org/abs/1903.02752v1",
    "PDF Link": "http://arxiv.org/pdf/1903.02752v1"
  },
  {
    "Title": "The merits of using Ethereum MainNet as a Coordination Blockchain for\n  Ethereum Private Sidechains",
    "Authors": "Peter Robinson",
    "Published": "2019-06-11T07:49:37Z",
    "Summary": "A Coordination Blockchain is a blockchain with the task of coordinating activities of multiple private blockchains. This paper discusses the pros and cons of using Ethereum MainNet, the public Ethereum blockchain, as a Coordination Blockchain. The requirements Ethereum MainNet needs to fulfil to perform this role are discussed within the context of Ethereum Private Sidechains, a private blockchain technology which allows many blockchains to be operated in parallel, and allows atomic crosschain transactions to execute across blockchains. Ethereum MainNet is a permissionless network which aims to offer strong authenticity, integrity, and non-repudiation properties, that incentivises good behaviour using crypto economics. This paper demonstrates that Ethereum MainNet does deliver these properties. It then provides a comprehensive review of the features of Ethereum Private Sidechains, with a focus on the potential usage of Coordination Blockchains for these features. Finally, the merits of using Ethereum MainNet as a Coordination Blockchain are assessed. For Ethereum Private Sidechains, we found that Ethereum MainNet is best suited to storing long term static data that needs to be widely available, such as the Ethereum Registration Authority information. However, due to Ethereum MainNet's probabilistic finality, it is not well suited to information that needs to be available and acted upon immediately, such as the Sidechain Public Keys and Atomic Crosschain Transaction state information that need to be accessible prior to the first atomic crosschain transaction being issued on a sidechain. Although this paper examined the use of Ethereum MainNet as a Coordination Blockchain within reference to Ethereum Private Sidechains, the discussions and observations of the typical tasks a Coordination blockchain may be expected to perform are applicable more widely to any multi-blockchain system.",
    "Link": "http://arxiv.org/abs/1906.04421v2",
    "PDF Link": "http://arxiv.org/pdf/1906.04421v2"
  },
  {
    "Title": "When Blockchain Meets Smart Grids: A Comprehensive Survey",
    "Authors": "Yihao Guo, Zhiguo Wan, Xiuzhen Cheng",
    "Published": "2021-09-29T01:37:42Z",
    "Summary": "Recent years have witnessed an increasing interest in the blockchain technology, and many blockchain-based applications have been developed to take advantage of its decentralization, transparency, fault tolerance, and strong security. In the field of smart grids, a plethora of proposals have emerged to utilize blockchain for augmenting intelligent energy management, energy trading, security and privacy protection, microgrid management, and energy vehicles. Compared with traditional centralized approaches, blockchain-based solutions are able to exploit the advantages of blockchain to realize better functionality in smart grids. However, the blockchain technology itself has its disadvantages in low processing throughput and weak privacy protection. Therefore, it is of paramount importance to study how to integrate blockchain with smart grids in a more effective way so that the advantages of blockchain can be maximized and its disadvantages can be avoided.   This article surveys the state-of-the-art solutions aiming to integrate the emergent blockchain technology with smart grids. The goal of this survey is to discuss the necessity of applying blockchain in different components of smart grids, identify the challenges encountered by current solutions, and highlight the frameworks and techniques used to integrate blockchain with smart grids. We also present thorough comparison studies among blockchain-based solutions for smart grids from different perspectives, with the aim to provide insights on integrating blockchain with smart grids for different smart grid management tasks. Finally, we list the current projects and initiatives demonstrating the current effort from the practice side. Additionally, we draw attention to open problems that have not yet been tackled by existing solutions, and point out possible future research directions.",
    "Link": "http://arxiv.org/abs/2109.14130v1",
    "PDF Link": "http://arxiv.org/pdf/2109.14130v1"
  },
  {
    "Title": "The Evolution of Embedding Metadata in Blockchain Transactions",
    "Authors": "Tooba Faisal, Nicolas Courtois, Antoaneta Serguieva",
    "Published": "2018-06-18T14:38:02Z",
    "Summary": "The use of blockchains is growing every day, and their utility has greatly expanded from sending and receiving crypto-coins to smart-contracts and decentralized autonomous organizations. Modern blockchains underpin a variety of applications: from designing a global identity to improving satellite connectivity. In our research we look at the ability of blockchains to store metadata in an increasing volume of transactions and with evolving focus of utilization. We further show that basic approaches to improving blockchain privacy also rely on embedding metadata. This paper identifies and classifies real-life blockchain transactions embedding metadata of a number of major protocols running essentially over the bitcoin blockchain. The empirical analysis here presents the evolution of metadata utilization in the recent years, and the discussion suggests steps towards preventing criminal use. Metadata are relevant to any blockchain, and our analysis considers primarily bitcoin as a case study. The paper concludes that simultaneously with both expanding legitimate utilization of embedded metadata and expanding blockchain functionality, the applied research on improving anonymity and security must also attempt to protect against blockchain abuse.",
    "Link": "http://arxiv.org/abs/1806.06738v1",
    "PDF Link": "http://arxiv.org/pdf/1806.06738v1"
  },
  {
    "Title": "Cross-Blockchain Databases for Governments: The Technology for Public\n  Registries and Smart Laws",
    "Authors": "Oleksii Konashevych",
    "Published": "2019-11-28T00:10:09Z",
    "Summary": "There is an ongoing competition among blockchain technologies and the existence of one ultimate blockchain is impossible for many reasons. On the other hand, such variety can create difficulties in adoption, especially for the governments and corporations. The proposed technology ensures a blockchain agnostic approach and aimed to create a unified ecosystem of multiple networks. The cross-blockchain protocol can be used to develop services where end-users decide for themselves their most preferred blockchain. The invention solves problems of duplication of tokens in the result of hardforks, issues with scalability, digital identity and even the \"problem\" of immutability (enforceability). A cross-blockchain DB means a consistent non-conflicting key-value database across a bunch of defined blockchains. It is not a new blockchain, but a protocol for developing databases on existing blockchains. The protocol is also a basis for a \"smart law\" which is a framework for public registries and their governance.",
    "Link": "http://arxiv.org/abs/1912.01713v2",
    "PDF Link": "http://arxiv.org/pdf/1912.01713v2"
  },
  {
    "Title": "A Review on Edge Analytics: Issues, Challenges, Opportunities, Promises,\n  Future Directions, and Applications",
    "Authors": "Sabuzima Nayak, Ripon Patgiri, Lilapati Waikhom, Arif Ahmed",
    "Published": "2021-07-01T21:48:20Z",
    "Summary": "Edge technology aims to bring Cloud resources (specifically, the compute, storage, and network) to the closed proximity of the Edge devices, i.e., smart devices where the data are produced and consumed. Embedding computing and application in Edge devices lead to emerging of two new concepts in Edge technology, namely, Edge computing and Edge analytics. Edge analytics uses some techniques or algorithms to analyze the data generated by the Edge devices. With the emerging of Edge analytics, the Edge devices have become a complete set. Currently, Edge analytics is unable to provide full support for the execution of the analytic techniques. The Edge devices cannot execute advanced and sophisticated analytic algorithms following various constraints such as limited power supply, small memory size, limited resources, etc. This article aims to provide a detailed discussion on Edge analytics. A clear explanation to distinguish between the three concepts of Edge technology, namely, Edge devices, Edge computing, and Edge analytics, along with their issues. Furthermore, the article discusses the implementation of Edge analytics to solve many problems in various areas such as retail, agriculture, industry, and healthcare. In addition, the research papers of the state-of-the-art edge analytics are rigorously reviewed in this article to explore the existing issues, emerging challenges, research opportunities and their directions, and applications.",
    "Link": "http://arxiv.org/abs/2107.06835v1",
    "PDF Link": "http://arxiv.org/pdf/2107.06835v1"
  },
  {
    "Title": "The Cohen-Macaulay type of edge-weighted r-path ideals",
    "Authors": "Shuai Wei",
    "Published": "2023-09-16T00:31:40Z",
    "Summary": "We describe combinatorially the Cohen-Macaulay type of edge-weighted r-path suspensions of edge-weighted graphs for an arbitrary positive integer r. The computation of the Cohen-Macaulay type of edge-weighted suspensions of edge-weighted graphs becomes a special case of r = 1.",
    "Link": "http://arxiv.org/abs/2309.08819v1",
    "PDF Link": "http://arxiv.org/pdf/2309.08819v1"
  },
  {
    "Title": "Democratizing the Edge: A Pervasive Edge Computing Framework",
    "Authors": "Reza Tourani, Srikathyayani Srikanteswara, Satyajayant Misra, Richard Chow, Lily Yang, Xiruo Liu, Yi Zhang",
    "Published": "2020-07-01T17:46:08Z",
    "Summary": "The needs of emerging applications, such as augmented and virtual reality, federated machine learning, and autonomous driving, have motivated edge computing--the push of computation capabilities to the edge. Various edge computing architectures have emerged, including multi-access edge computing and edge-cloud, all with the premise of reducing communication latency and augmenting privacy. However, these architectures rely on static and pre-deployed infrastructure, falling short in harnessing the abundant resources at the network's edge. In this paper, we discuss the design of Pervasive Edge Computing (PEC)--a democratized edge computing framework, which enables end-user devices (e.g., smartphones, IoT devices, and vehicles) to dynamically participate in a large-scale computing ecosystem. Our vision of the democratized edge involves the real-time composition of services using available edge resources like data, software, and compute-hardware from multiple stakeholders. We discuss how the novel Named-Data Networking architecture can facilitate service deployment, discovery, invocation, and migration. We also discuss the economic models critical to the adoption of PEC and the outstanding challenges for its full realization.",
    "Link": "http://arxiv.org/abs/2007.00641v1",
    "PDF Link": "http://arxiv.org/pdf/2007.00641v1"
  },
  {
    "Title": "GENIO: Synergizing Edge Computing with Optical Network Infrastructures",
    "Authors": "Carmine Cesarano, Alessio Foggia, Gianluca Roscigno, Luca Andreani, Roberto Natella",
    "Published": "2025-02-19T12:09:54Z",
    "Summary": "Edge computing has emerged as a paradigm to bring low-latency and bandwidth-intensive applications close to end-users. However, edge computing platforms still face challenges related to resource constraints, connectivity, and security. We present GENIO, a novel platform that integrates edge computing within existing Passive Optical Network (PON) infrastructures. GENIO enhances central offices with computational and storage resources, enabling telecom operators to leverage their existing PON networks as a distributed edge computing infrastructure. Through simulations, we show the feasibility of GENIO in supporting real-world edge scenarios, and its better performance compared to a traditional edge computing architecture.",
    "Link": "http://arxiv.org/abs/2502.13657v1",
    "PDF Link": "http://arxiv.org/pdf/2502.13657v1"
  },
  {
    "Title": "Edge Routing with Ordered Bundles",
    "Authors": "Sergey Bereg, Alexander E. Holroyd, Lev Nachmanson, Sergey Pupyrev",
    "Published": "2012-09-19T12:50:15Z",
    "Summary": "Edge bundling reduces the visual clutter in a drawing of a graph by uniting the edges into bundles. We propose a method of edge bundling drawing each edge of a bundle separately as in metro-maps and call our method ordered bundles. To produce aesthetically looking edge routes it minimizes a cost function on the edges. The cost function depends on the ink, required to draw the edges, the edge lengths, widths and separations. The cost also penalizes for too many edges passing through narrow channels by using the constrained Delaunay triangulation. The method avoids unnecessary edge-node and edge-edge crossings. To draw edges with the minimal number of crossings and separately within the same bundle we develop an efficient algorithm solving a variant of the metro-line crossing minimization problem. In general, the method creates clear and smooth edge routes giving an overview of the global graph structure, while still drawing each edge separately and thus enabling local analysis.",
    "Link": "http://arxiv.org/abs/1209.4227v1",
    "PDF Link": "http://arxiv.org/pdf/1209.4227v1"
  },
  {
    "Title": "Edge-Path Bundling: A Less Ambiguous Edge Bundling Approach",
    "Authors": "Markus Wallinger, Daniel Archambault, David Auber, Martin Nöllenburg, Jaakko Peltonen",
    "Published": "2021-08-11T22:39:45Z",
    "Summary": "Edge bundling techniques cluster edges with similar attributes (i.e. similarity in direction and proximity) together to reduce the visual clutter. All edge bundling techniques to date implicitly or explicitly cluster groups of individual edges, or parts of them, together based on these attributes. These clusters can result in ambiguous connections that do not exist in the data. Confluent drawings of networks do not have these ambiguities, but require the layout to be computed as part of the bundling process. We devise a new bundling method, Edge-Path bundling, to simplify edge clutter while greatly reducing ambiguities compared to previous bundling techniques. Edge-Path bundling takes a layout as input and clusters each edge along a weighted, shortest path to limit its deviation from a straight line. Edge-Path bundling does not incur independent edge ambiguities typically seen in all edge bundling methods, and the level of bundling can be tuned through shortest path distances, Euclidean distances, and combinations of the two. Also, directed edge bundling naturally emerges from the model. Through metric evaluations, we demonstrate the advantages of Edge-Path bundling over other techniques.",
    "Link": "http://arxiv.org/abs/2108.05467v1",
    "PDF Link": "http://arxiv.org/pdf/2108.05467v1"
  },
  {
    "Title": "AI on the Edge: Rethinking AI-based IoT Applications Using Specialized\n  Edge Architectures",
    "Authors": "Qianlin Liang, Prashant Shenoy, David Irwin",
    "Published": "2020-03-27T15:43:01Z",
    "Summary": "Edge computing has emerged as a popular paradigm for supporting mobile and IoT applications with low latency or high bandwidth needs. The attractiveness of edge computing has been further enhanced due to the recent availability of special-purpose hardware to accelerate specific compute tasks, such as deep learning inference, on edge nodes. In this paper, we experimentally compare the benefits and limitations of using specialized edge systems, built using edge accelerators, to more traditional forms of edge and cloud computing. Our experimental study using edge-based AI workloads shows that today's edge accelerators can provide comparable, and in many cases better, performance, when normalized for power or cost, than traditional edge and cloud servers. They also provide latency and bandwidth benefits for split processing, across and within tiers, when using model compression or model splitting, but require dynamic methods to determine the optimal split across tiers. We find that edge accelerators can support varying degrees of concurrency for multi-tenant inference applications, but lack isolation mechanisms necessary for edge cloud multi-tenant hosting.",
    "Link": "http://arxiv.org/abs/2003.12488v1",
    "PDF Link": "http://arxiv.org/pdf/2003.12488v1"
  },
  {
    "Title": "Applications of Auction and Mechanism Design in Edge Computing: A Survey",
    "Authors": "Houming Qiu, Kun Zhu, Nguyen Cong Luong, Changyan Yi, Dusit Niyato, Dong In Kim",
    "Published": "2021-05-08T02:21:09Z",
    "Summary": "Edge computing as a promising technology provides lower latency, more efficient transmission, and faster speed of data processing since the edge servers are closer to the user devices. Each edge server with limited resources can offload latency-sensitive and computation-intensive tasks from nearby user devices. However, edge computing faces challenges such as resource allocation, energy consumption, security and privacy issues, etc. Auction mechanisms can well characterize bidirectional interactions between edge servers and user devices under the above constraints in edge computing. As demonstrated by the existing works, auction and mechanism design approaches are outstanding on achieving optimal allocation strategy while guaranteeing mutual satisfaction among edge servers and user devices, especially for scenarios with scarce resources. In this paper, we introduce a comprehensive survey of recent researches that apply auction approaches in edge computing. Firstly, a brief overview of edge computing including three common edge computing paradigms, i.e., cloudlet, fog computing and mobile edge computing, is presented. Then, we introduce fundamentals and backgrounds of auction schemes commonly used in edge computing systems. After then, a comprehensive survey of applications of auction-based approaches applied for edge computing is provided, which is categorized by different auction approaches. Finally, several open challenges and promising research directions are discussed.",
    "Link": "http://arxiv.org/abs/2105.03559v1",
    "PDF Link": "http://arxiv.org/pdf/2105.03559v1"
  },
  {
    "Title": "EdgeBench: A Workflow-based Benchmark for Edge Computing",
    "Authors": "Qirui Yang, Runyu Jin, Nabil Gandhi, Xiongzi Ge, Hoda Aghaei Khouzani, Ming Zhao",
    "Published": "2020-10-27T03:11:41Z",
    "Summary": "Edge computing has been developed to utilize multiple tiers of resources for privacy, cost and Quality of Service (QoS) reasons. Edge workloads have the characteristics of data-driven and latency-sensitive. Because of this, edge systems have developed to be both heterogeneous and distributed. The unique characteristics of edge workloads and edge systems have motivated EdgeBench, a workflow-based benchmark aims to provide the ability to explore the full design space of edge workloads and edge systems. EdgeBench is both customizable and representative. It allows users to customize the workflow logic of edge workloads, the data storage backends, and the distribution of the individual workflow stages to different computing tiers. To illustrate the usability of EdgeBench, we also implements two representative edge workflows, a video analytics workflow and an IoT hub workflow that represents two distinct but common edge workloads. Both workflows are evaluated using the workflow-level and function-level metrics reported by EdgeBench to illustrate both the performance bottlenecks of the edge systems and the edge workloads.",
    "Link": "http://arxiv.org/abs/2010.14027v1",
    "PDF Link": "http://arxiv.org/pdf/2010.14027v1"
  },
  {
    "Title": "Distributed Training for Deep Learning Models On An Edge Computing\n  Network Using ShieldedReinforcement Learning",
    "Authors": "Tanmoy Sen, Haiying Shen",
    "Published": "2022-06-01T21:32:44Z",
    "Summary": "Edge devices with local computation capability has made distributed deep learning training on edges possible. In such method, the cluster head of a cluster of edges schedules DL training jobs from the edges. Using such centralized scheduling method, the cluster head knows all loads of edges, which can avoid overloading the cluster edges, but the head itself may become overloaded. To handle this problem, we propose a multi-agent RL (MARL) system that enables each edge to schedule its jobs using RL. However, without coordination among edges, action collision may occur, in which multiple edges schedule tasks to the same edge and make it overloaded. For this reason, we propose a system called Shielded ReinfOrcement learning (RL) based DL training on Edges (SROLE). In SROLE, the shield deployed in an edge checks action collisions and provides alternative actions to avoid collisions. As the central shield for entire cluster may become a bottleneck, we further propose a decentralized shielding method, where different shields are responsible for different regions in the cluster and they coordinate to avoid action collisions on the region boundaries. Our emulation and real device experiments show SROLE reduces training time by 59% compared to MARL and centralized RL.",
    "Link": "http://arxiv.org/abs/2206.00774v1",
    "PDF Link": "http://arxiv.org/pdf/2206.00774v1"
  },
  {
    "Title": "KCES: A Workflow Containerization Scheduling Scheme Under Cloud-Edge\n  Collaboration Framework",
    "Authors": "Chenggang Shan, Runze Gao, Qinghua Han, Zhen Yang, Jinhui Zhang, Yuanqing Xia",
    "Published": "2024-01-02T14:11:24Z",
    "Summary": "As more IoT applications gradually move towards the cloud-edge collaborative mode, the containerized scheduling of workflows extends from the cloud to the edge. However, given the high delay of the communication network, loose coupling of structure, and resource heterogeneity between cloud and edge, workflow containerization scheduling in the cloud-edge scenarios faces the difficulty of resource coordination and application collaboration management. To address these two issues, we propose a KubeEdge-Cloud-Edge-Scheduling scheme named KCES, a workflow containerization scheduling scheme for the KubeEdge cloud-edge framework. The KCES includes a cloud-edge workflow scheduling engine for KubeEdge and workflow scheduling strategies for task horizontal roaming and vertical offloading. Considering the scheduling optimization of cloud-edge workflows, this paper proposes a cloud-edge workflow scheduling model and cloud-edge node model and designs a cloud-edge workflow scheduling engine to maximize cloud-edge resource utilization under the constraint of workflow task delay. A cloud-edge resource hybrid management technology is used to design the cloud-edge resource evaluation and resource allocation algorithms to achieve cloud-edge resource collaboration. Based on the ideas of distributed functional roles and the hierarchical division of computing power, the horizontal roaming among the edges and vertical offloading strategies between the cloud and edges for workflow tasks are designed to realize the cloud-edge application collaboration. Through a customized IoT application workflow instance, experimental results show that KCES is superior to the baseline in total workflow duration, average workflow duration, and resource usage and has the capabilities of horizontal roaming and vertical offloading for workflow tasks.",
    "Link": "http://arxiv.org/abs/2401.01217v1",
    "PDF Link": "http://arxiv.org/pdf/2401.01217v1"
  },
  {
    "Title": "Price-Based Distributed Offloading for Mobile-Edge Computing with\n  Computation Capacity Constraints",
    "Authors": "Mengyu Liu, Yuan Liu",
    "Published": "2017-12-02T12:39:50Z",
    "Summary": "Mobile-edge computing (MEC) is a promising technology to enable real-time information transmission and computing by offloading computation tasks from wireless devices to network edge.",
    "Link": "http://arxiv.org/abs/1712.00599v1",
    "PDF Link": "http://arxiv.org/pdf/1712.00599v1"
  },
  {
    "Title": "Communication-Aware Computing for Edge Processing",
    "Authors": "Songze Li, Mohammad Ali Maddah-Ali, A. Salman Avestimehr",
    "Published": "2017-06-22T23:34:16Z",
    "Summary": "We consider a mobile edge computing problem, in which mobile users offload their computation tasks to computing nodes (e.g., base stations) at the network edge. The edge nodes compute the requested functions and communicate the computed results to the users via wireless links. For this problem, we propose a Universal Coded Edge Computing (UCEC) scheme for linear functions to simultaneously minimize the load of computation at the edge nodes, and maximize the physical-layer communication efficiency towards the mobile users. In the proposed UCEC scheme, edge nodes create coded inputs of the users, from which they compute coded output results. Then, the edge nodes utilize the computed coded results to create communication messages that zero-force all the interference signals over the air at each user. Specifically, the proposed scheme is universal since the coded computations performed at the edge nodes are oblivious of the channel states during the communication process from the edge nodes to the users.",
    "Link": "http://arxiv.org/abs/1706.07523v1",
    "PDF Link": "http://arxiv.org/pdf/1706.07523v1"
  },
  {
    "Title": "Towards Edge-assisted Internet of Things: From Security and Efficiency\n  Perspectives",
    "Authors": "Jianbing Ni, Xiaodong Lin, Xuemin, Shen",
    "Published": "2019-02-19T15:14:28Z",
    "Summary": "As we are moving towards the Internet of Things (IoT) era, the number of connected physical devices is increasing at a rapid pace. Mobile edge computing is emerging to handle the sheer volume of produced data and reach the latency demand of computation-intensive IoT applications. Although the advance of mobile edge computing on service latency is studied solidly, security and efficiency on data usage in mobile edge computing have not been clearly identified. In this article, we examine the architecture of mobile edge computing and explore the potentials of utilizing mobile edge computing to enhance data analysis for IoT applications, while achieving data security and computational efficiency. Specifically, we first introduce the overall architecture and several promising edge-assisted IoT applications. We then study the security, privacy and efficiency challenges in data processing for mobile edge computing, and discuss the opportunities to enhance data security and improve computational efficiency with the assistance of edge computing, including secure data aggregation, secure data deduplication and secure computational offloading. Finally, several interesting directions on edge-empowered data analysis are presented for future research.",
    "Link": "http://arxiv.org/abs/1902.07094v1",
    "PDF Link": "http://arxiv.org/pdf/1902.07094v1"
  },
  {
    "Title": "On the Non-Locality of Edge Insertions",
    "Authors": "Florestan Brunck",
    "Published": "2023-11-16T11:18:46Z",
    "Summary": "We challenge the idea that edge insertions are local improvement operations and show that the edge-insertion algorithm must sometimes insert an edge between vertices that are at the farthest combinatorial distance apart, and that this edge must also cross linearly many edges of the triangulation for the algorithm to escape a local optimum and return the optimal triangulation.",
    "Link": "http://arxiv.org/abs/2311.09794v1",
    "PDF Link": "http://arxiv.org/pdf/2311.09794v1"
  },
  {
    "Title": "Cooperative Service Caching and Workload Scheduling in Mobile Edge\n  Computing",
    "Authors": "Xiao Ma, Ao Zhou, Shan Zhang, Shangguang Wang",
    "Published": "2020-02-04T15:06:44Z",
    "Summary": "Mobile edge computing is beneficial to reduce service response time and core network traffic by pushing cloud functionalities to network edge. Equipped with storage and computation capacities, edge nodes can cache services of resource-intensive and delay-sensitive mobile applications and process the corresponding computation tasks without outsourcing to central clouds. However, the heterogeneity of edge resource capacities and inconsistence of edge storage and computation capacities make it difficult to jointly fully utilize the storage and computation capacities when there is no cooperation among edge nodes. To address this issue, we consider cooperation among edge nodes and investigate cooperative service caching and workload scheduling in mobile edge computing. This problem can be formulated as a mixed integer nonlinear programming problem, which has non-polynomial computation complexity. To overcome the challenges of subproblem coupling, computation-communication tradeoff, and edge node heterogeneity, we develop an iterative algorithm called ICE. This algorithm is designed based on Gibbs sampling, which has provably near-optimal results, and the idea of water filling, which has polynomial computation complexity. Simulations are conducted and the results demonstrate that our algorithm can jointly reduce the service response time and the outsourcing traffic compared with the benchmark algorithms.",
    "Link": "http://arxiv.org/abs/2002.01358v1",
    "PDF Link": "http://arxiv.org/pdf/2002.01358v1"
  },
  {
    "Title": "EdgeWorkflowReal: An Edge Computing based Workflow Execution Engine for\n  Smart Systems",
    "Authors": "Xuejun Li, Ran Ding, Xiao Liu, Jia Xu, Yun Yang, John Grundy",
    "Published": "2021-01-30T14:53:09Z",
    "Summary": "Current cloud-based smart systems suffer from weaknesses such as high response latency, limited network bandwidth and the restricted computing power of smart end devices which seriously affect the system's QoS (Quality of Service). Recently, given its advantages of low latency, high bandwidth and location awareness, edge computing has become a promising solution for smart systems. However, the development of edge computing based smart systems is a very challenging job for software developers who do not have the skills for the creation of edge computing environments. The management of edge computing resources and computing tasks is also very challenging. Workflow technology has been widely used in smart systems to automate task and resource management, but there does not yet exist a real-world deployable edge computing based workflow execution engine. To fill this gap, we present EdgeWorkflowReal, an edge computing based workflow execution engine for smart systems. EdgeWorkflowReal supports: 1) automatic creation of a real edge computing environment according to user settings; 2) visualized modelling of edge workflow applications; and 3) automatic deployment, monitoring and performance evaluation of edge workflow applications in a smart system.",
    "Link": "http://arxiv.org/abs/2102.00234v1",
    "PDF Link": "http://arxiv.org/pdf/2102.00234v1"
  },
  {
    "Title": "Delving into Crispness: Guided Label Refinement for Crisp Edge Detection",
    "Authors": "Yunfan Ye, Renjiao Yi, Zhirui Gao, Zhiping Cai, Kai Xu",
    "Published": "2023-06-27T03:12:58Z",
    "Summary": "Learning-based edge detection usually suffers from predicting thick edges. Through extensive quantitative study with a new edge crispness measure, we find that noisy human-labeled edges are the main cause of thick predictions. Based on this observation, we advocate that more attention should be paid on label quality than on model design to achieve crisp edge detection. To this end, we propose an effective Canny-guided refinement of human-labeled edges whose result can be used to train crisp edge detectors. Essentially, it seeks for a subset of over-detected Canny edges that best align human labels. We show that several existing edge detectors can be turned into a crisp edge detector through training on our refined edge maps. Experiments demonstrate that deep models trained with refined edges achieve significant performance boost of crispness from 17.4% to 30.6%. With the PiDiNet backbone, our method improves ODS and OIS by 12.2% and 12.6% on the Multicue dataset, respectively, without relying on non-maximal suppression. We further conduct experiments and show the superiority of our crisp edge detection for optical flow estimation and image segmentation.",
    "Link": "http://arxiv.org/abs/2306.15172v1",
    "PDF Link": "http://arxiv.org/pdf/2306.15172v1"
  },
  {
    "Title": "Watersheds on edge or node weighted graphs \"par l'exemple",
    "Authors": "Fernand Meyer",
    "Published": "2013-03-07T21:15:29Z",
    "Summary": "Watersheds have been defined both for node and edge weighted graphs. We show that they are identical: for each edge (resp.\\ node) weighted graph exists a node (resp. edge) weighted graph with the same minima and catchment basin.",
    "Link": "http://arxiv.org/abs/1303.1829v1",
    "PDF Link": "http://arxiv.org/pdf/1303.1829v1"
  },
  {
    "Title": "SketchSplat: 3D Edge Reconstruction via Differentiable Multi-view Sketch\n  Splatting",
    "Authors": "Haiyang Ying, Matthias Zwicker",
    "Published": "2025-03-18T23:30:03Z",
    "Summary": "Edges are one of the most basic parametric primitives to describe structural information in 3D. In this paper, we study parametric 3D edge reconstruction from calibrated multi-view images. Previous methods usually reconstruct a 3D edge point set from multi-view 2D edge images, and then fit 3D edges to the point set. However, noise in the point set may cause gaps among fitted edges, and the recovered edges may not align with input multi-view images since the edge fitting depends only on the reconstructed 3D point set. To mitigate these problems, we propose SketchSplat, a method to reconstruct accurate, complete, and compact 3D edges via differentiable multi-view sketch splatting. We represent 3D edges as sketches, which are parametric lines and curves defined by attributes including control points, scales, and opacity. During edge reconstruction, we iteratively sample Gaussian points from a set of sketches and rasterize the Gaussians onto 2D edge images. Then the gradient of the image error with respect to the input 2D edge images can be back-propagated to optimize the sketch attributes. Our method bridges 2D edge images and 3D edges in a differentiable manner, which ensures that 3D edges align well with 2D images and leads to accurate and complete results. We also propose a series of adaptive topological operations and apply them along with the sketch optimization. The topological operations help reduce the number of sketches required while ensuring high accuracy, yielding a more compact reconstruction. Finally, we contribute an accurate 2D edge detector that improves the performance of both ours and existing methods. Experiments show that our method achieves state-of-the-art accuracy, completeness, and compactness on a benchmark CAD dataset.",
    "Link": "http://arxiv.org/abs/2503.14786v1",
    "PDF Link": "http://arxiv.org/pdf/2503.14786v1"
  },
  {
    "Title": "A Case for a Programmable Edge Storage Middleware",
    "Authors": "Giulia Frascaria, Animesh Trivedi, Lin Wang",
    "Published": "2021-11-29T17:21:09Z",
    "Summary": "Edge computing is a fast-growing computing paradigm where data is processed at the local site where it is generated, close to the end-devices. This can benefit a set of disruptive applications like autonomous driving, augmented reality, and collaborative machine learning, which produce incredible amounts of data that need to be shared, processed and stored at the edge to meet low latency requirements. However, edge storage poses new challenges due to the scarcity and heterogeneity of edge infrastructures and the diversity of edge applications. In particular, edge applications may impose conflicting constraints and optimizations that are hard to be reconciled on the limited, hard-to-scale edge resources. In this vision paper we argue that a new middleware for constrained edge resources is needed, providing a unified storage service for diverse edge applications. We identify programmability as a critical feature that should be leveraged to optimize the resource sharing while delivering the specialization needed for edge applications. Following this line, we make a case for eBPF and present the design for Griffin - a flexible, lightweight programmable edge storage middleware powered by eBPF.",
    "Link": "http://arxiv.org/abs/2111.14720v1",
    "PDF Link": "http://arxiv.org/pdf/2111.14720v1"
  },
  {
    "Title": "Reliable Transactions in Serverless-Edge Architecture",
    "Authors": "Suyash Gupta, Sajjad Rahnama, Erik Linsenmayer, Faisal Nawab, Mohammad Sadoghi",
    "Published": "2022-01-04T05:06:19Z",
    "Summary": "Modern edge applications demand novel solutions where edge applications do not have to rely on a single cloud provider (which cannot be in the vicinity of every edge device) or dedicated edge servers (which cannot scale as clouds) for processing compute-intensive tasks. A recent computing philosophy, Sky computing, proposes giving each user ability to select between available cloud providers.   In this paper, we present our serverless-edge co-design, which extends the Sky computing vision. In our serverless-edge co-design, we expect edge devices to collaborate and spawn required number of serverless functions. This raises several key challenges: (1) how will this collaboration take place, (2) what if some edge devices are compromised, and (3) what if a selected cloud provider is malicious. Hence, we design ServerlessBFT, the first protocol to guarantee Byzantine fault-tolerant (BFT) transactional flow between edge devices and serverless functions. We present an exhaustive list of attacks and their solutions on our serverless-edge co-design. Further, we extensively benchmark our architecture on a variety of parameters.",
    "Link": "http://arxiv.org/abs/2201.00982v2",
    "PDF Link": "http://arxiv.org/pdf/2201.00982v2"
  },
  {
    "Title": "STEC-IoT: A Security Tactic by Virtualizing Edge Computing on IoT",
    "Authors": "Peiying Zhang, Chunxiao Jiang, Xue Pang, Yi Qian",
    "Published": "2022-02-09T06:20:05Z",
    "Summary": "To a large extent, the deployment of edge computing (EC) can reduce the burden of the explosive growth of the Internet of things. As a powerful hub between the Internet of things and cloud servers, edge devices make the transmission of cloud to things no longer complicated. However, edge nodes are faced with a series of problems, such as large number, a wide range of distribution, and complex environment, the security of edge computing should not be underestimated. Based on this, we propose a tactic to improve the safety of edge computing by virtualizing edge nodes. In detail, first of all, we propose a strategy of edge node partition, virtualize the edge nodes dealing with different types of things into various virtual networks, which are deployed between the edge nodes and the cloud server. Second, considering that different information transmission has different security requirement, we propose a security tactic based on security level measurement. Finally, through simulation experiments, we compare with the existing advanced algorithms which are committed to virtual network security, and prove that the model proposed in this paper has definite progressiveness in enhancing the security of edge computing.",
    "Link": "http://arxiv.org/abs/2202.04300v1",
    "PDF Link": "http://arxiv.org/pdf/2202.04300v1"
  },
  {
    "Title": "Integration of Blockchain and Edge Computing in Internet of Things: A\n  Survey",
    "Authors": "He Xue, Dajiang Chen, Ning Zhang, Hong-Ning Dai, Keping Yu",
    "Published": "2022-05-26T05:14:22Z",
    "Summary": "As an important technology to ensure data security, consistency, traceability, etc., blockchain has been increasingly used in Internet of Things (IoT) applications. The integration of blockchain and edge computing can further improve the resource utilization in terms of network, computing, storage, and security. This paper aims to present a survey on the integration of blockchain and edge computing. In particular, we first give an overview of blockchain and edge computing. We then present a general architecture of an integration of blockchain and edge computing system. We next study how to utilize blockchain to benefit edge computing, as well as how to use edge computing to benefit blockchain. We also discuss the issues brought by the integration of blockchain and edge computing system and solutions from perspectives of resource management, joint optimization, data management, computation offloading and security mechanism. Finally, we analyze and summarize the existing challenges posed by the integration of blockchain and edge computing system and the potential solutions in the future.",
    "Link": "http://arxiv.org/abs/2205.13160v1",
    "PDF Link": "http://arxiv.org/pdf/2205.13160v1"
  },
  {
    "Title": "Edge Intelligence: The Confluence of Edge Computing and Artificial\n  Intelligence",
    "Authors": "Shuiguang Deng, Hailiang Zhao, Weijia Fang, Jianwei Yin, Schahram Dustdar, Albert Y. Zomaya",
    "Published": "2019-09-02T06:37:13Z",
    "Summary": "Along with the rapid developments in communication technologies and the surge in the use of mobile devices, a brand-new computation paradigm, Edge Computing, is surging in popularity. Meanwhile, Artificial Intelligence (AI) applications are thriving with the breakthroughs in deep learning and the many improvements in hardware architectures. Billions of data bytes, generated at the network edge, put massive demands on data processing and structural optimization. Thus, there exists a strong demand to integrate Edge Computing and AI, which gives birth to Edge Intelligence. In this paper, we divide Edge Intelligence into AI for edge (Intelligence-enabled Edge Computing) and AI on edge (Artificial Intelligence on Edge). The former focuses on providing more optimal solutions to key problems in Edge Computing with the help of popular and effective AI technologies while the latter studies how to carry out the entire process of building AI models, i.e., model training and inference, on the edge. This paper provides insights into this new inter-disciplinary field from a broader perspective. It discusses the core concepts and the research road-map, which should provide the necessary background for potential future research initiatives in Edge Intelligence.",
    "Link": "http://arxiv.org/abs/1909.00560v2",
    "PDF Link": "http://arxiv.org/pdf/1909.00560v2"
  },
  {
    "Title": "A Survey on Edge Computing Systems and Tools",
    "Authors": "Fang Liu, Guoming Tang, Youhuizi Li, Zhiping Cai, Xingzhou Zhang, Tongqing Zhou",
    "Published": "2019-11-07T08:16:40Z",
    "Summary": "Driven by the visions of Internet of Things and 5G communications, the edge computing systems integrate computing, storage and network resources at the edge of the network to provide computing infrastructure, enabling developers to quickly develop and deploy edge applications. Nowadays the edge computing systems have received widespread attention in both industry and academia. To explore new research opportunities and assist users in selecting suitable edge computing systems for specific applications, this survey paper provides a comprehensive overview of the existing edge computing systems and introduces representative projects. A comparison of open source tools is presented according to their applicability. Finally, we highlight energy efficiency and deep learning optimization of edge computing systems. Open issues for analyzing and designing an edge computing system are also studied in this survey.",
    "Link": "http://arxiv.org/abs/1911.02794v1",
    "PDF Link": "http://arxiv.org/pdf/1911.02794v1"
  },
  {
    "Title": "Developing an edge computing platform for real-time descriptive\n  analytics",
    "Authors": "Hung Cao, Monica Wachowicz, Sangwhan Cha",
    "Published": "2017-05-23T16:22:06Z",
    "Summary": "The Internet of Mobile Things encompasses stream data being generated by sensors, network communications that pull and push these data streams, as well as running processing and analytics that can effectively leverage actionable information for transportation planning, management, and business advantage. Edge computing emerges as a new paradigm that decentralizes the communication, computation, control and storage resources from the cloud to the edge of the network. This paper proposes an edge computing platform where mobile edge nodes are physical devices deployed on a transit bus where descriptive analytics is used to uncover meaningful patterns from real-time transit data streams. An application experiment is used to evaluate the advantages and disadvantages of our proposed platform to support descriptive analytics at a mobile edge node and generate actionable information to transit managers.",
    "Link": "http://arxiv.org/abs/1705.08449v4",
    "PDF Link": "http://arxiv.org/pdf/1705.08449v4"
  },
  {
    "Title": "Edge-bandwidth of graphs",
    "Authors": "Tao Jiang, Dhruv Mubayi, Aditya Shastri, Douglas B. West",
    "Published": "1999-04-03T03:55:51Z",
    "Summary": "The edge-bandwidth of a graph is the minimum, over all labelings of the edges with distinct integers, of the maximum difference between labels of two incident edges. We prove that edge-bandwidth is at least as large as bandwidth for every graph, with equality for certain caterpillars. We obtain sharp or nearly-sharp bounds on the change in edge-bandwidth under addition, subdivision, or contraction of edges. We compute edge-bandwidth for cliques, bicliques, caterpillars, and some theta graphs.",
    "Link": "http://arxiv.org/abs/math/9904011v1",
    "PDF Link": "http://arxiv.org/pdf/math/9904011v1"
  },
  {
    "Title": "Simultaneous Embeddings with Few Bends and Crossings",
    "Authors": "Fabrizio Frati, Michael Hoffmann, Vincent Kusters",
    "Published": "2015-08-31T17:12:21Z",
    "Summary": "A simultaneous embedding with fixed edges (SEFE) of two planar graphs $R$ and $B$ is a pair of plane drawings of $R$ and $B$ that coincide when restricted to the common vertices and edges of $R$ and $B$. We show that whenever $R$ and $B$ admit a SEFE, they also admit a SEFE in which every edge is a polygonal curve with few bends and every pair of edges has few crossings. Specifically: (1) if $R$ and $B$ are trees then one bend per edge and four crossings per edge pair suffice (and one bend per edge is sometimes necessary), (2) if $R$ is a planar graph and $B$ is a tree then six bends per edge and eight crossings per edge pair suffice, and (3) if $R$ and $B$ are planar graphs then six bends per edge and sixteen crossings per edge pair suffice. Our results improve on a paper by Grilli et al. (GD'14), which proves that nine bends per edge suffice, and on a paper by Chan et al. (GD'14), which proves that twenty-four crossings per edge pair suffice.",
    "Link": "http://arxiv.org/abs/1508.07921v1",
    "PDF Link": "http://arxiv.org/pdf/1508.07921v1"
  },
  {
    "Title": "Edgeformers: Graph-Empowered Transformers for Representation Learning on\n  Textual-Edge Networks",
    "Authors": "Bowen Jin, Yu Zhang, Yu Meng, Jiawei Han",
    "Published": "2023-02-21T23:09:17Z",
    "Summary": "Edges in many real-world social/information networks are associated with rich text information (e.g., user-user communications or user-product reviews). However, mainstream network representation learning models focus on propagating and aggregating node attributes, lacking specific designs to utilize text semantics on edges. While there exist edge-aware graph neural networks, they directly initialize edge attributes as a feature vector, which cannot fully capture the contextualized text semantics of edges. In this paper, we propose Edgeformers, a framework built upon graph-enhanced Transformers, to perform edge and node representation learning by modeling texts on edges in a contextualized way. Specifically, in edge representation learning, we inject network information into each Transformer layer when encoding edge texts; in node representation learning, we aggregate edge representations through an attention mechanism within each node's ego-graph. On five public datasets from three different domains, Edgeformers consistently outperform state-of-the-art baselines in edge classification and link prediction, demonstrating the efficacy in learning edge and node representations, respectively.",
    "Link": "http://arxiv.org/abs/2302.11050v1",
    "PDF Link": "http://arxiv.org/pdf/2302.11050v1"
  },
  {
    "Title": "Generalized cut method for computing the edge-Wiener index",
    "Authors": "Niko Tratnik",
    "Published": "2019-02-08T15:39:41Z",
    "Summary": "The edge-Wiener index of a connected graph $G$ is defined as the Wiener index of the line graph of $G$. In this paper it is shown that the edge-Wiener index of an edge-weighted graph can be computed in terms of the Wiener index, the edge-Wiener index, and the vertex-edge-Wiener index of weighted quotient graphs which are defined by a partition of the edge set that is coarser than $\\Theta^*$-partition. Thus, already known analogous methods for computing the edge-Wiener index of benzenoid systems and phenylenes are greatly generalized. Moreover, reduction theorems are developed for the edge-Wiener index and the vertex-edge-Wiener index since they can be applied in order to compute a corresponding index of a (quotient) graph from the so-called reduced graph. Finally, the obtained results are used to find the closed formula for the edge-Wiener index of an infinite family of graphs.",
    "Link": "http://arxiv.org/abs/1902.03153v2",
    "PDF Link": "http://arxiv.org/pdf/1902.03153v2"
  },
  {
    "Title": "Edge AIBench: Towards Comprehensive End-to-end Edge Computing\n  Benchmarking",
    "Authors": "Tianshu Hao, Yunyou Huang, Xu Wen, Wanling Gao, Fan Zhang, Chen Zheng, Lei Wang, Hainan Ye, Kai Hwang, Zujie Ren, Jianfeng Zhan",
    "Published": "2019-08-06T01:34:51Z",
    "Summary": "In edge computing scenarios, the distribution of data and collaboration of workloads on different layers are serious concerns for performance, privacy, and security issues. So for edge computing benchmarking, we must take an end-to-end view, considering all three layers: client-side devices, edge computing layer, and cloud servers. Unfortunately, the previous work ignores this most important point. This paper presents the BenchCouncil's coordinated e ort on edge AI benchmarks, named Edge AIBench. In total, Edge AIBench models four typical application scenarios: ICU Patient Monitor, Surveillance Camera, Smart Home, and Autonomous Vehicle with the focus on data distribution and workload collaboration on three layers. Edge AIBench is a part of the open-source AIBench project, publicly available from http://www.benchcouncil.org/AIBench/index.html. We also build an edge computing testbed with a federated learning framework to resolve performance, privacy, and security issues.",
    "Link": "http://arxiv.org/abs/1908.01924v1",
    "PDF Link": "http://arxiv.org/pdf/1908.01924v1"
  },
  {
    "Title": "When Edge Meets FaaS: Opportunities and Challenges",
    "Authors": "Runyu Jin, Qirui Yang, Ming Zhao",
    "Published": "2023-06-29T19:20:40Z",
    "Summary": "The proliferation of edge devices and the rapid growth of IoT data have called forth the edge computing paradigm. Function-as-a-service (FaaS) is a promising computing paradigm to realize edge computing. This paper explores the feasibility and advantages of FaaS-based edge computing. It also studies the research challenges that should be addressed in the design of such systems, which are 1) the quick decomposing and recomposing of applications, 2) the trade-off between performance and isolation of sandbox mechanisms, and 3) distributed scheduling. The challenges are illustrated by evaluating existing FaaS-based edge platforms, AWS IoT Greengrass, and OpenFaaS.",
    "Link": "http://arxiv.org/abs/2307.06397v1",
    "PDF Link": "http://arxiv.org/pdf/2307.06397v1"
  },
  {
    "Title": "Edge-Direct Visual Odometry",
    "Authors": "Kevin Christensen, Martial Hebert",
    "Published": "2019-06-11T21:53:49Z",
    "Summary": "In this paper we propose an edge-direct visual odometry algorithm that efficiently utilizes edge pixels to find the relative pose that minimizes the photometric error between images. Prior work on exploiting edge pixels instead treats edges as features and employ various techniques to match edge lines or pixels, which adds unnecessary complexity. Direct methods typically operate on all pixel intensities, which proves to be highly redundant. In contrast our method builds on direct visual odometry methods naturally with minimal added computation. It is not only more efficient than direct dense methods since we iterate with a fraction of the pixels, but also more accurate. We achieve high accuracy and efficiency by extracting edges from only one image, and utilize robust Gauss-Newton to minimize the photometric error of these edge pixels. This simultaneously finds the edge pixels in the reference image, as well as the relative camera pose that minimizes the photometric error. We test various edge detectors, including learned edges, and determine that the optimal edge detector for this method is the Canny edge detection algorithm using automatic thresholding. We highlight key differences between our edge direct method and direct dense methods, in particular how higher levels of image pyramids can lead to significant aliasing effects and result in incorrect solution convergence. We show experimentally that reducing the photometric error of edge pixels also reduces the photometric error of all pixels, and we show through an ablation study the increase in accuracy obtained by optimizing edge pixels only. We evaluate our method on the RGB-D TUM benchmark on which we achieve state-of-the-art performance.",
    "Link": "http://arxiv.org/abs/1906.04838v1",
    "PDF Link": "http://arxiv.org/pdf/1906.04838v1"
  },
  {
    "Title": "KubeEdge-Sedna v0.3: Towards Next-Generation Automatically Customized AI\n  Engineering Scheme",
    "Authors": "Zimu Zheng",
    "Published": "2023-03-08T14:50:52Z",
    "Summary": "The scale of the global edge AI market continues to grow. The current technical challenges that hinder the large-scale replication of edge AI are mainly small samples on the edge and heterogeneity of edge data. In addition, edge AI customers often have requirements for data security compliance and offline autonomy of edge AI services. Based on the lifelong learning method in the academic world, we formally define the problem of edge-cloud collaborative lifelong learning for the first time, and release the industry's first open-source edge-cloud collaborative lifelong learning. Edge-cloud collaborative lifelong learning adapts to data heterogeneity at different edge locations through (1) multi-task transfer learning to achieve accurate prediction of \"thousands of people and thousands of faces\"; (2) incremental processing of unknown tasks, the more systems learn and the smarter systems are with small samples, gradually realize AI engineering and automation; (3) Use the cloud-side knowledge base to remember new situational knowledge to avoid catastrophic forgetting; (4) The edge-cloud collaborative architecture enables data security compliance and edge AI services to be offline autonomy while applying cloud resources. This work hopes to help fundamentally solve the above-mentioned challenges of edge-cloud collaborative machine learning.",
    "Link": "http://arxiv.org/abs/2304.05985v1",
    "PDF Link": "http://arxiv.org/pdf/2304.05985v1"
  },
  {
    "Title": "ENTS: An Edge-native Task Scheduling System for Collaborative Edge\n  Computing",
    "Authors": "Mingjin Zhang, Jiannong Cao, Lei Yang, Liang Zhang, Yuvraj Sahni, Shan Jiang",
    "Published": "2022-10-14T14:13:35Z",
    "Summary": "Collaborative edge computing (CEC) is an emerging paradigm enabling sharing of the coupled data, computation, and networking resources among heterogeneous geo-distributed edge nodes. Recently, there has been a trend to orchestrate and schedule containerized application workloads in CEC, while Kubernetes has become the de-facto standard broadly adopted by the industry and academia. However, Kubernetes is not preferable for CEC because its design is not dedicated to edge computing and neglects the unique features of edge nativeness. More specifically, Kubernetes primarily ensures resource provision of workloads while neglecting the performance requirements of edge-native applications, such as throughput and latency. Furthermore, Kubernetes neglects the inner dependencies of edge-native applications and fails to consider data locality and networking resources, leading to inferior performance. In this work, we design and develop ENTS, the first edge-native task scheduling system, to manage the distributed edge resources and facilitate efficient task scheduling to optimize the performance of edge-native applications. ENTS extends Kubernetes with the unique ability to collaboratively schedule computation and networking resources by comprehensively considering job profile and resource status. We showcase the superior efficacy of ENTS with a case study on data streaming applications. We mathematically formulate a joint task allocation and flow scheduling problem that maximizes the job throughput. We design two novel online scheduling algorithms to optimally decide the task allocation, bandwidth allocation, and flow routing policies. The extensive experiments on a real-world edge video analytics application show that ENTS achieves 43\\%-220\\% higher average job throughput compared with the state-of-the-art.",
    "Link": "http://arxiv.org/abs/2210.07842v1",
    "PDF Link": "http://arxiv.org/pdf/2210.07842v1"
  },
  {
    "Title": "Edge Computing: A Comprehensive Survey of Current Initiatives and a\n  Roadmap for a Sustainable Edge Computing Development",
    "Authors": "Andrea Hamm, Alexander Willner, Ina Schieferdecker",
    "Published": "2019-12-18T11:26:22Z",
    "Summary": "Edge Computing is a new distributed Cloud Computing paradigm in which computing and storage capabilities are pushed to the topological edge of a network. However, various standards and implementations are promoted by different initiatives. Lead by a reference architecture model for Edge Computing, current initiatives are analyzed by explorative content analysis. Providing two main contributions to the field, we present, first, how current initiatives are characterized, and second, a roadmap for sustainable Edge Computing relating three dimensions of sustainable development to four cross-concerns of Edge Computing. Findings show that most initiatives are internationally organized software development projects; important branches are currently telecom and industrial sectors; most addressed is the network virtualization layer. The roadmap reveals numerous chances and risks of Edge Computing related to sustainable development; such as the use of renewable energies, biases, new business models, increase and decrease of energy consumption, responsiveness, monitoring and traceability.",
    "Link": "http://arxiv.org/abs/1912.08530v1",
    "PDF Link": "http://arxiv.org/pdf/1912.08530v1"
  },
  {
    "Title": "Armada: A Robust Latency-Sensitive Edge Cloud in Heterogeneous\n  Edge-Dense Environments",
    "Authors": "Lei Huang, Zhiying Liang, Nikhil Sreekumar, Sumanth Kaushik Vishwanath, Cody Perakslis, Abhishek Chandra, Jon Weissman",
    "Published": "2021-11-23T17:06:07Z",
    "Summary": "Edge computing has enabled a large set of emerging edge applications by exploiting data proximity and offloading latency-sensitive and computation-intensive workloads to nearby edge servers. However, supporting edge application users at scale in wide-area environments poses challenges due to limited point-of-presence edge sites and constrained elasticity. In this paper, we introduce Armada: a densely-distributed edge cloud infrastructure that explores the use of dedicated and volunteer resources to serve geo-distributed users in heterogeneous environments. We describe the lightweight Armada architecture and optimization techniques including performance-aware edge selection, auto-scaling and load balancing on the edge, fault tolerance, and in-situ data access. We evaluate Armada in both real-world volunteer environments and emulated platforms to show how common edge applications, namely real-time object detection and face recognition, can be easily deployed on Armada serving distributed users at scale with low latency.",
    "Link": "http://arxiv.org/abs/2111.12002v1",
    "PDF Link": "http://arxiv.org/pdf/2111.12002v1"
  },
  {
    "Title": "Collaborative Automatic Modulation Classification via Deep Edge\n  Inference for Hierarchical Cognitive Radio Networks",
    "Authors": "Chaowei He, Peihao Dong, Fuhui Zhou, Qihui Wu",
    "Published": "2024-09-12T11:14:25Z",
    "Summary": "In hierarchical cognitive radio networks, edge or cloud servers utilize the data collected by edge devices for modulation classification, which, however, is faced with problems of the transmission overhead, data privacy, and computation load. In this article, an edge learning (EL) based framework jointly mobilizing the edge device and the edge server for intelligent co-inference is proposed to realize the collaborative automatic modulation classification (C-AMC) between them. A spectrum semantic compression neural network (SSCNet) with the lightweight structure is designed for the edge device to compress the collected raw data into a compact semantic message that is then sent to the edge server via the wireless channel. On the edge server side, a modulation classification neural network (MCNet) combining bidirectional long short-term memory (Bi-LSTM) and multi-head attention layers is elaborated to determine the modulation type from the noisy semantic message. By leveraging the computation resources of both the edge device and the edge server, high transmission overhead and risks of data privacy leakage are avoided. The simulation results verify the effectiveness of the proposed C-AMC framework, significantly reducing the model size and computational complexity.",
    "Link": "http://arxiv.org/abs/2409.07946v2",
    "PDF Link": "http://arxiv.org/pdf/2409.07946v2"
  },
  {
    "Title": "EDCSSM: Edge Detection with Convolutional State Space Model",
    "Authors": "Qinghui Hong, Haoyou Jiang, Pingdan Xiao, Sichun Du, Tao Li",
    "Published": "2024-09-03T05:13:25Z",
    "Summary": "Edge detection in images is the foundation of many complex tasks in computer graphics. Due to the feature loss caused by multi-layer convolution and pooling architectures, learning-based edge detection models often produce thick edges and struggle to detect the edges of small objects in images. Inspired by state space models, this paper presents an edge detection algorithm which effectively addresses the aforementioned issues. The presented algorithm obtains state space variables of the image from dual-input channels with minimal down-sampling processes and utilizes these state variables for real-time learning and memorization of image text. Additionally, to achieve precise edges while filtering out false edges, a post-processing algorithm called wind erosion has been designed to handle the binary edge map. To further enhance the processing speed of the algorithm, we have designed parallel computing circuits for the most computationally intensive parts of presented algorithm, significantly improving computational speed and efficiency. Experimental results demonstrate that the proposed algorithm achieves precise thin edge localization and exhibits noise suppression capabilities across various types of images. With the parallel computing circuits, the algorithm to achieve processing speeds exceeds 30 FPS on 5K images.",
    "Link": "http://arxiv.org/abs/2409.01609v1",
    "PDF Link": "http://arxiv.org/pdf/2409.01609v1"
  },
  {
    "Title": "CE-CoLLM: Efficient and Adaptive Large Language Models Through\n  Cloud-Edge Collaboration",
    "Authors": "Hongpeng Jin, Yanzhao Wu",
    "Published": "2024-11-05T06:00:27Z",
    "Summary": "Large Language Models (LLMs) have achieved remarkable success in serving end-users with human-like intelligence. However, LLMs demand high computational resources, making it challenging to deploy them to satisfy various performance objectives, such as meeting the resource constraints on edge devices close to end-users or achieving high accuracy with ample resources. In this paper, we introduce CE-CoLLM, a novel cloud-edge collaboration framework that supports efficient and adaptive LLM inference for end-users at the edge with two modes, (1) low-latency edge standalone inference and (2) highly accurate cloud-edge collaborative inference. First, we show that the inherent high communication costs for transmitting LLM contextual information between the edge and cloud dominate the overall latency, making it inefficient and costly to deploy LLMs using cloud-edge collaboration. Second, we propose several critical techniques to address this challenge, including early-exit mechanism, cloud context manager, and quantization in cloud-edge collaboration to enable not only low-latency standalone edge inference but also efficient and adaptive cloud-edge collaborative inference for LLMs. Third, we perform comprehensive experimental analysis, which demonstrates that CE-CoLLM significantly reduces inference time by up to 13.81% and cloud computation costs by up to 84.55% compared to the popular cloud-based LLM deployment, while maintaining comparable model accuracy. The proposed approach effectively shifts the computational load to the edge, reduces the communication overhead, scales efficiently with multiple edge clients, and provides reliable LLM deployment using cloud-edge collaboration.",
    "Link": "http://arxiv.org/abs/2411.02829v1",
    "PDF Link": "http://arxiv.org/pdf/2411.02829v1"
  },
  {
    "Title": "Smart-Edge-CoCaCo: AI-Enabled Smart Edge with Joint Computation,\n  Caching, and Communication in Heterogeneous IoT",
    "Authors": "Yixue Hao, Yiming Miao, Yuanwen Tian, Long Hu, M. Shamim Hossain, Ghulam Muhammad, Syed Umar Amin",
    "Published": "2019-01-08T02:04:12Z",
    "Summary": "The development of mobile communication technology, hardware, distributed computing, and artificial intelligence (AI) technology has promoted the application of edge computing in the field of heterogeneous Internet of Things (IoT). In order to overcome the defects of the traditional cloud computing model in the era of big data. In this article, we first propose a new AIenabled smart edge with heterogeneous IoT architecture which combines edge computing, caching, and communication. Then, we propose the Smart-Edge-CoCaCo algorithm. To minimize total delay and confirm the computation offloading decision, Smart-Edge-CoCaCo uses joint optimization of the wireless communication model, the collaborative filter caching model in edge cloud, and the computation offloading model. Finally, we built an emotion interaction testbed to perform computational delay experiments in real environments. The experiment results showed that the computation delay of the Smart-Edge-CoCaCo algorithm is lower than that of traditional cloud computing model with the increase of computing task data and the number of concurrent users.",
    "Link": "http://arxiv.org/abs/1901.02126v1",
    "PDF Link": "http://arxiv.org/pdf/1901.02126v1"
  },
  {
    "Title": "EaaS: A Service-Oriented Edge Computing Framework Towards Distributed\n  Intelligence",
    "Authors": "Mingjin Zhang, Jiannong Cao, Yuvraj Sahni, Qianyi Chen, Shan Jiang, Tao Wu",
    "Published": "2022-09-14T13:04:48Z",
    "Summary": "Edge computing has become a popular paradigm where services and applications are deployed at the network edge closer to the data sources. It provides applications with outstanding benefits, including reduced response latency and enhanced privacy protection. For emerging advanced applications, such as autonomous vehicles, industrial IoT, and metaverse, further research is needed. This is because such applications demand ultra-low latency, hyper-connectivity, and dynamic and reliable service provision, while existing approaches are inadequate to address the new challenges. Hence, we envision that the future edge computing is moving towards distributed intelligence, where heterogeneous edge nodes collaborate to provide services in large-scale and geo-distributed edge infrastructure. We thereby propose Edge-as-a-Service (EaaS) to enable distributed intelligence. EaaS jointly manages large-scale cross-node edge resources and facilitates edge autonomy, edge-to-edge collaboration, and resource elasticity. These features enable flexible deployment of services and ubiquitous computation and intelligence. We first give an overview of existing edge computing studies and discuss their limitations to articulate the motivation for proposing EaaS. Then, we describe the details of EaaS, including the physical architecture, proposed software framework, and benefits of EaaS. Various application scenarios, such as real-time video surveillance, smart building, and metaverse, are presented to illustrate the significance and potential of EaaS. Finally, we discuss several challenging issues of EaaS to inspire more research towards this new edge computing framework.",
    "Link": "http://arxiv.org/abs/2209.06613v1",
    "PDF Link": "http://arxiv.org/pdf/2209.06613v1"
  },
  {
    "Title": "How BlockChain Can Help Enhance The Security And Privacy in Edge\n  Computing?",
    "Authors": "Jinyue Song, Tianbo Gu, Prasant Mohapatra",
    "Published": "2021-10-31T07:13:24Z",
    "Summary": "In order to solve security and privacy issues of centralized cloud services, the edge computing network is introduced, where computing and storage resources are distributed to the edge of the network. However, native edge computing is subject to the limited performance of edge devices, which causes challenges in data authorization, data encryption, user privacy, and other fields. Blockchain is currently the hottest technology for distributed networks. It solves the consistent issue of distributed data and is used in many areas, such as cryptocurrency, smart grid, and the Internet of Things.   Our work discussed the security and privacy challenges of edge computing networks. From the perspectives of data authorization, encryption, and user privacy, we analyze the solutions brought by blockchain technology to edge computing networks. In this work, we deeply present the benefits from the integration of the edge computing network and blockchain technology, which effectively controls the data authorization and data encryption of the edge network and enhances the architecture's scalability under the premise of ensuring security and privacy. Finally, we investigate challenges on storage, workload, and latency for future research in this field.",
    "Link": "http://arxiv.org/abs/2111.00416v1",
    "PDF Link": "http://arxiv.org/pdf/2111.00416v1"
  },
  {
    "Title": "Personalizing Federated Learning for Hierarchical Edge Networks with\n  Non-IID Data",
    "Authors": "Seunghyun Lee, Omid Tavallaie, Shuaijun Chen, Kanchana Thilakarathna, Suranga Seneviratne, Adel Nadjaran Toosi, Albert Y. Zomaya",
    "Published": "2025-04-11T11:42:06Z",
    "Summary": "Accommodating edge networks between IoT devices and the cloud server in Hierarchical Federated Learning (HFL) enhances communication efficiency without compromising data privacy. However, devices connected to the same edge often share geographic or contextual similarities, leading to varying edge-level data heterogeneity with different subsets of labels per edge, on top of device-level heterogeneity. This hierarchical non-Independent and Identically Distributed (non-IID) nature, which implies that each edge has its own optimization goal, has been overlooked in HFL research. Therefore, existing edge-accommodated HFL demonstrates inconsistent performance across edges in various hierarchical non-IID scenarios. To ensure robust performance with diverse edge-level non-IID data, we propose a Personalized Hierarchical Edge-enabled Federated Learning (PHE-FL), which personalizes each edge model to perform well on the unique class distributions specific to each edge. We evaluated PHE-FL across 4 scenarios with varying levels of edge-level non-IIDness, with extreme IoT device level non-IIDness. To accurately assess the effectiveness of our personalization approach, we deployed test sets on each edge server instead of the cloud server, and used both balanced and imbalanced test sets. Extensive experiments show that PHE-FL achieves up to 83 percent higher accuracy compared to existing federated learning approaches that incorporate edge networks, given the same number of training rounds. Moreover, PHE-FL exhibits improved stability, as evidenced by reduced accuracy fluctuations relative to the state-of-the-art FedAvg with two-level (edge and cloud) aggregation.",
    "Link": "http://arxiv.org/abs/2504.08872v1",
    "PDF Link": "http://arxiv.org/pdf/2504.08872v1"
  },
  {
    "Title": "Evolution of Landau Levels into Edge States at an Atomically Sharp Edge\n  in Graphene",
    "Authors": "Guohong Li, Adina Luican, Dmitry Abanin, Leonid Levitov, Eva Y. Andrei",
    "Published": "2012-03-25T20:06:28Z",
    "Summary": "The quantum-Hall-effect (QHE) occurs in topologically-ordered states of two-dimensional (2d) electron-systems in which an insulating bulk-state coexists with protected 1d conducting edge-states. Owing to a unique topologically imposed edge-bulk correspondence these edge-states are endowed with universal properties such as fractionally-charged quasiparticles and interference-patterns, which make them indispensable components for QH-based quantum-computation and other applications. The precise edge-bulk correspondence, conjectured theoretically in the limit of sharp edges, is difficult to realize in conventional semiconductor-based electron systems where soft boundaries lead to edge-state reconstruction. Using scanning-tunneling microscopy and spectroscopy to follow the spatial evolution of bulk Landau-levels towards a zigzag edge of graphene supported above a graphite substrate we demonstrate that in this system it is possible to realize atomically sharp edges with no edge-state reconstruction. Our results single out graphene as a system where the edge-state structure can be controlled and the universal properties directly probed.",
    "Link": "http://arxiv.org/abs/1203.5540v1",
    "PDF Link": "http://arxiv.org/pdf/1203.5540v1"
  },
  {
    "Title": "Edge Detection: A Collection of Pixel based Approach for Colored Images",
    "Authors": "B. O Sadiq, S. M Sani, S Garba",
    "Published": "2015-03-19T10:08:37Z",
    "Summary": "The existing traditional edge detection algorithms process a single pixel on an image at a time, thereby calculating a value which shows the edge magnitude of the pixel and the edge orientation. Most of these existing algorithms convert the coloured images into gray scale before detection of edges. However, this process leads to inaccurate precision of recognized edges, thus producing false and broken edges in the image. This paper presents a profile modelling scheme for collection of pixels based on the step and ramp edges, with a view to reducing the false and broken edges present in the image. The collection of pixel scheme generated is used with the Vector Order Statistics to reduce the imprecision of recognized edges when converting from coloured to gray scale images. The Pratt Figure of Merit (PFOM) is used as a quantitative comparison between the existing traditional edge detection algorithm and the developed algorithm as a means of validation. The PFOM value obtained for the developed algorithm is 0.8480, which showed an improvement over the existing traditional edge detection algorithms.",
    "Link": "http://arxiv.org/abs/1503.05689v1",
    "PDF Link": "http://arxiv.org/pdf/1503.05689v1"
  },
  {
    "Title": "Active Canny: Edge Detection and Recovery with Open Active Contour\n  Models",
    "Authors": "Muhammet Bastan, S. Saqib Bukhari, Thomas M. Breuel",
    "Published": "2016-09-12T14:13:26Z",
    "Summary": "We introduce an edge detection and recovery framework based on open active contour models (snakelets). This is motivated by the noisy or broken edges output by standard edge detection algorithms, like Canny. The idea is to utilize the local continuity and smoothness cues provided by strong edges and grow them to recover the missing edges. This way, the strong edges are used to recover weak or missing edges by considering the local edge structures, instead of blindly linking them if gradient magnitudes are above some threshold. We initialize short snakelets on the gradient magnitudes or binary edges automatically and then deform and grow them under the influence of gradient vector flow. The output snakelets are able to recover most of the breaks or weak edges, and they provide a smooth edge representation of the image; they can also be used for higher level analysis, like contour segmentation.",
    "Link": "http://arxiv.org/abs/1609.03415v1",
    "PDF Link": "http://arxiv.org/pdf/1609.03415v1"
  },
  {
    "Title": "Edge Tracing using Gaussian Process Regression",
    "Authors": "Jamie Burke, Stuart King",
    "Published": "2021-11-05T16:43:14Z",
    "Summary": "We introduce a novel edge tracing algorithm using Gaussian process regression. Our edge-based segmentation algorithm models an edge of interest using Gaussian process regression and iteratively searches the image for edge pixels in a recursive Bayesian scheme. This procedure combines local edge information from the image gradient and global structural information from posterior curves, sampled from the model's posterior predictive distribution, to sequentially build and refine an observation set of edge pixels. This accumulation of pixels converges the distribution to the edge of interest. Hyperparameters can be tuned by the user at initialisation and optimised given the refined observation set. This tunable approach does not require any prior training and is not restricted to any particular type of imaging domain. Due to the model's uncertainty quantification, the algorithm is robust to artefacts and occlusions which degrade the quality and continuity of edges in images. Our approach also has the ability to efficiently trace edges in image sequences by using previous-image edge traces as a priori information for consecutive images. Various applications to medical imaging and satellite imaging are used to validate the technique and comparisons are made with two commonly used edge tracing algorithms.",
    "Link": "http://arxiv.org/abs/2111.03605v1",
    "PDF Link": "http://arxiv.org/pdf/2111.03605v1"
  },
  {
    "Title": "Edge Intelligence: Paving the Last Mile of Artificial Intelligence with\n  Edge Computing",
    "Authors": "Zhi Zhou, Xu Chen, En Li, Liekang Zeng, Ke Luo, Junshan Zhang",
    "Published": "2019-05-24T08:19:05Z",
    "Summary": "With the breakthroughs in deep learning, the recent years have witnessed a booming of artificial intelligence (AI) applications and services, spanning from personal assistant to recommendation systems to video/audio surveillance. More recently, with the proliferation of mobile computing and Internet-of-Things (IoT), billions of mobile and IoT devices are connected to the Internet, generating zillions Bytes of data at the network edge. Driving by this trend, there is an urgent need to push the AI frontiers to the network edge so as to fully unleash the potential of the edge big data. To meet this demand, edge computing, an emerging paradigm that pushes computing tasks and services from the network core to the network edge, has been widely recognized as a promising solution. The resulted new inter-discipline, edge AI or edge intelligence, is beginning to receive a tremendous amount of interest. However, research on edge intelligence is still in its infancy stage, and a dedicated venue for exchanging the recent advances of edge intelligence is highly desired by both the computer system and artificial intelligence communities. To this end, we conduct a comprehensive survey of the recent research efforts on edge intelligence. Specifically, we first review the background and motivation for artificial intelligence running at the network edge. We then provide an overview of the overarching architectures, frameworks and emerging key technologies for deep learning model towards training/inference at the network edge. Finally, we discuss future research opportunities on edge intelligence. We believe that this survey will elicit escalating attentions, stimulate fruitful discussions and inspire further research ideas on edge intelligence.",
    "Link": "http://arxiv.org/abs/1905.10083v1",
    "PDF Link": "http://arxiv.org/pdf/1905.10083v1"
  },
  {
    "Title": "Big Data",
    "Authors": "Andreas L Opdahl, Vimala Nunavath",
    "Published": "2020-12-15T16:18:52Z",
    "Summary": "The Internet of Things, crowdsourcing, social media, public authorities, and other sources generate bigger and bigger data sets. Big and open data offers many benefits for emergency management, but also pose new challenges. This chapter will review the sources of big data and their characteristics. We then discuss potential benefits of big data for emergency management along with the technological and societal challenges it poses. We review central technologies for big-data storage and processing in general, before presenting the Spark big-data engine in more detail. Finally, we review ethical and societal threats that big data pose.",
    "Link": "http://arxiv.org/abs/2012.09109v1",
    "PDF Link": "http://arxiv.org/pdf/2012.09109v1"
  },
  {
    "Title": "NNSC-Cobordism of Bartnik Data in High Dimensions",
    "Authors": "Xue Hu, Yuguang Shi",
    "Published": "2020-01-16T01:29:32Z",
    "Summary": "In this short note, we formulate three problems relating to nonnegative scalar curvature (NNSC) fill-ins. Loosely speaking, the first two problems focus on: When are $(n-1)$-dimensional Bartnik data $\\big(\\Sigma_i ^{n-1}, \\gamma_i, H_i\\big)$, $i=1,2$, NNSC-cobordant? (i.e., there is an $n$-dimensional compact Riemannian manifold $\\big(\\Omega^n, g\\big)$ with scalar curvature $R(g)\\geq 0$ and the boundary $\\partial \\Omega=\\Sigma_{1} \\cup \\Sigma_{2}$ such that $\\gamma_i$ is the metric on $\\Sigma_i ^{n-1}$ induced by $g$, and $H_i$ is the mean curvature of $\\Sigma_i$ in $\\big(\\Omega^n, g\\big)$). If $\\big(\\mathbb{S}^{n-1},\\gamma_{\\rm std},0\\big)$ is positive scalar curvature (PSC) cobordant to $\\big(\\Sigma_1 ^{n-1}, \\gamma_1, H_1\\big)$, where $\\big(\\mathbb{S}^{n-1}, \\gamma_{\\rm std}\\big)$ denotes the standard round unit sphere then $\\big(\\Sigma_1 ^{n-1}, \\gamma_1, H_1\\big)$ admits an NNSC fill-in. Just as Gromov's conjecture is connected with positive mass theorem, our problems are connected with Penrose inequality, at least in the case of $n=3$. Our third problem is on $\\Lambda\\big(\\Sigma^{n-1}, \\gamma\\big)$ defined below.",
    "Link": "http://arxiv.org/abs/2001.05607v4",
    "PDF Link": "http://arxiv.org/pdf/2001.05607v4"
  },
  {
    "Title": "Big Data: Opportunities and Privacy Challenges",
    "Authors": "Hervais Simo Fhom",
    "Published": "2015-02-03T12:03:08Z",
    "Summary": "Recent advances in data collection and computational statistics coupled with increases in computer processing power, along with the plunging costs of storage are making technologies to effectively analyze large sets of heterogeneous data ubiquitous. Applying such technologies (often referred to as big data technologies) to an ever growing number and variety of internal and external data sources, businesses and institutions can discover hidden correlations between data items, and extract actionable insights needed for innovation and economic growth. While on one hand big data technologies yield great promises, on the other hand, they raise critical security, privacy, and ethical issues, which if left unaddressed may become significant barriers to the fulfillment of expected opportunities and long-term success of big data. In this paper, we discuss the benefits of big data to individuals and society at large, focusing on seven key use cases: Big data for business optimization and customer analytics, big data and science, big data and health care, big data and finance, big data and the emerging energy distribution systems, big/open data as enablers of openness and efficiency in government, and big data security. In addition to benefits and opportunities, we discuss the security, privacy, and ethical issues at stake.",
    "Link": "http://arxiv.org/abs/1502.00823v1",
    "PDF Link": "http://arxiv.org/pdf/1502.00823v1"
  },
  {
    "Title": "Several Typical Paradigms of Industrial Big Data Application",
    "Authors": "Hu Shaolin, Zhang Qinghua, Su Naiquan, Li Xiwu",
    "Published": "2021-06-29T13:06:35Z",
    "Summary": "Industrial big data is an important part of big data family, which has important application value for industrial production scheduling, risk perception, state identification, safety monitoring and quality control, etc. Due to the particularity of the industrial field, some concepts in the existing big data research field are unable to reflect accurately the characteristics of industrial big data, such as what is industrial big data, how to measure industrial big data, how to apply industrial big data, and so on. In order to overcome the limitation that the existing definition of big data is not suitable for industrial big data, this paper intuitively proposes the concept of big data cloud and the 3M (Multi-source, Multi-dimension, Multi-span in time) definition of cloud-based big data. Based on big data cloud and 3M definition, three typical paradigms of industrial big data applications are built, including the fusion calculation paradigm, the model correction paradigm and the information compensation paradigm. These results are helpful for grasping systematically the methods and approaches of industrial big data applications.",
    "Link": "http://arxiv.org/abs/2106.15377v1",
    "PDF Link": "http://arxiv.org/pdf/2106.15377v1"
  },
  {
    "Title": "Identifying Dwarfs Workloads in Big Data Analytics",
    "Authors": "Wanling Gao, Chunjie Luo, Jianfeng Zhan, Hainan Ye, Xiwen He, Lei Wang, Yuqing Zhu, Xinhui Tian",
    "Published": "2015-05-26T09:38:08Z",
    "Summary": "Big data benchmarking is particularly important and provides applicable yardsticks for evaluating booming big data systems. However, wide coverage and great complexity of big data computing impose big challenges on big data benchmarking. How can we construct a benchmark suite using a minimum set of units of computation to represent diversity of big data analytics workloads? Big data dwarfs are abstractions of extracting frequently appearing operations in big data computing. One dwarf represents one unit of computation, and big data workloads are decomposed into one or more dwarfs. Furthermore, dwarfs workloads rather than vast real workloads are more cost-efficient and representative to evaluate big data systems. In this paper, we extensively investigate six most important or emerging application domains i.e. search engine, social network, e-commerce, multimedia, bioinformatics and astronomy. After analyzing forty representative algorithms, we single out eight dwarfs workloads in big data analytics other than OLAP, which are linear algebra, sampling, logic operations, transform operations, set operations, graph operations, statistic operations and sort.",
    "Link": "http://arxiv.org/abs/1505.06872v1",
    "PDF Link": "http://arxiv.org/pdf/1505.06872v1"
  },
  {
    "Title": "A Taxonomy on Big Data: Survey",
    "Authors": "Ripon Patgiri",
    "Published": "2018-08-25T21:47:23Z",
    "Summary": "The Big Data is the most popular paradigm nowadays and it has almost no untouched area. For instance, science, engineering, economics, business, social science, and government. The Big Data are used to boost up the organization performance using massive amount of dataset. The Data are assets of the organization, and these data gives revenue to the organizations. Therefore, the Big Data is spawning everywhere to enhance the organizations' revenue. Thus, many new technologies emerging based on Big Data. In this paper, we present the taxonomy of Big Data. Besides, we present in-depth insight on the Big Data paradigm.",
    "Link": "http://arxiv.org/abs/1808.08474v3",
    "PDF Link": "http://arxiv.org/pdf/1808.08474v3"
  },
  {
    "Title": "On Big Data Benchmarking",
    "Authors": "Rui Han, Xiaoyi Lu",
    "Published": "2014-02-21T03:03:16Z",
    "Summary": "Big data systems address the challenges of capturing, storing, managing, analyzing, and visualizing big data. Within this context, developing benchmarks to evaluate and compare big data systems has become an active topic for both research and industry communities. To date, most of the state-of-the-art big data benchmarks are designed for specific types of systems. Based on our experience, however, we argue that considering the complexity, diversity, and rapid evolution of big data systems, for the sake of fairness, big data benchmarks must include diversity of data and workloads. Given this motivation, in this paper, we first propose the key requirements and challenges in developing big data benchmarks from the perspectives of generating data with 4V properties (i.e. volume, velocity, variety and veracity) of big data, as well as generating tests with comprehensive workloads for big data systems. We then present the methodology on big data benchmarking designed to address these challenges. Next, the state-of-the-art are summarized and compared, following by our vision for future research directions.",
    "Link": "http://arxiv.org/abs/1402.5194v1",
    "PDF Link": "http://arxiv.org/pdf/1402.5194v1"
  },
  {
    "Title": "Characterizing and Subsetting Big Data Workloads",
    "Authors": "Zhen Jia, Jianfeng Zhan, Lei Wang, Rui Han, Sally A. McKee, Qiang Yang, Chunjie Luo, Jingwei Li",
    "Published": "2014-09-01T10:57:16Z",
    "Summary": "Big data benchmark suites must include a diversity of data and workloads to be useful in fairly evaluating big data systems and architectures. However, using truly comprehensive benchmarks poses great challenges for the architecture community. First, we need to thoroughly understand the behaviors of a variety of workloads. Second, our usual simulation-based research methods become prohibitively expensive for big data. As big data is an emerging field, more and more software stacks are being proposed to facilitate the development of big data applications, which aggravates hese challenges. In this paper, we first use Principle Component Analysis (PCA) to identify the most important characteristics from 45 metrics to characterize big data workloads from BigDataBench, a comprehensive big data benchmark suite. Second, we apply a clustering technique to the principle components obtained from the PCA to investigate the similarity among big data workloads, and we verify the importance of including different software stacks for big data benchmarking. Third, we select seven representative big data workloads by removing redundant ones and release the BigDataBench simulation version, which is publicly available from http://prof.ict.ac.cn/BigDataBench/simulatorversion/.",
    "Link": "http://arxiv.org/abs/1409.0792v1",
    "PDF Link": "http://arxiv.org/pdf/1409.0792v1"
  },
  {
    "Title": "The Implications of Diverse Applications and Scalable Data Sets in\n  Benchmarking Big Data Systems",
    "Authors": "Zhen Jia, Runlin Zhou, Chunge Zhu, Lei Wang, Wanling Gao, Yingjie Shi, Jianfeng Zhan, Lixin Zhang",
    "Published": "2013-07-30T12:34:49Z",
    "Summary": "Now we live in an era of big data, and big data applications are becoming more and more pervasive. How to benchmark data center computer systems running big data applications (in short big data systems) is a hot topic. In this paper, we focus on measuring the performance impacts of diverse applications and scalable volumes of data sets on big data systems. For four typical data analysis applications---an important class of big data applications, we find two major results through experiments: first, the data scale has a significant impact on the performance of big data systems, so we must provide scalable volumes of data sets in big data benchmarks. Second, for the four applications, even all of them use the simple algorithms, the performance trends are different with increasing data scales, and hence we must consider not only variety of data sets but also variety of applications in benchmarking big data systems.",
    "Link": "http://arxiv.org/abs/1307.7943v1",
    "PDF Link": "http://arxiv.org/pdf/1307.7943v1"
  },
  {
    "Title": "A Survey on Blockchain for Big Data: Approaches, Opportunities, and\n  Future Directions",
    "Authors": "Natarajan Deepa, Quoc-Viet Pham, Dinh C. Nguyen, Sweta Bhattacharya, B Prabadevi, Thippa Reddy Gadekallu, Praveen Kumar Reddy Maddikunta, Fang Fang, Pubudu N. Pathirana",
    "Published": "2020-09-02T07:14:45Z",
    "Summary": "Big data has generated strong interest in various scientific and engineering domains over the last few years. Despite many advantages and applications, there are many challenges in big data to be tackled for better quality of service, e.g., big data analytics, big data management, and big data privacy and security. Blockchain with its decentralization and security nature has the great potential to improve big data services and applications. In this article, we provide a comprehensive survey on blockchain for big data, focusing on up-to-date approaches, opportunities, and future directions. First, we present a brief overview of blockchain and big data as well as the motivation behind their integration. Next, we survey various blockchain services for big data, including blockchain for secure big data acquisition, data storage, data analytics, and data privacy preservation. Then, we review the state-of-the-art studies on the use of blockchain for big data applications in different vertical domains such as smart city, smart healthcare, smart transportation, and smart grid. For a better understanding, some representative blockchain-big data projects are also presented and analyzed. Finally, challenges and future directions are discussed to further drive research in this promising area.",
    "Link": "http://arxiv.org/abs/2009.00858v2",
    "PDF Link": "http://arxiv.org/pdf/2009.00858v2"
  },
  {
    "Title": "Big Data Analytics = Machine Learning + Cloud Computing",
    "Authors": "Caesar Wu, Rajkumar Buyya, Kotagiri Ramamohanarao",
    "Published": "2016-01-13T02:18:48Z",
    "Summary": "Big Data can mean different things to different people. The scale and challenges of Big Data are often described using three attributes, namely Volume, Velocity and Variety (3Vs), which only reflect some of the aspects of data. In this chapter we review historical aspects of the term \"Big Data\" and the associated analytics. We augment 3Vs with additional attributes of Big Data to make it more comprehensive and relevant. We show that Big Data is not just 3Vs, but 32 Vs, that is, 9 Vs covering the fundamental motivation behind Big Data, which is to incorporate Business Intelligence (BI) based on different hypothesis or statistical models so that Big Data Analytics (BDA) can enable decision makers to make useful predictions for some crucial decisions or researching results. History of Big Data has demonstrated that the most cost effective way of performing BDA is to employ Machine Learning (ML) on the Cloud Computing (CC) based infrastructure or simply, ML + CC -> BDA. This chapter is devoted to help decision makers by defining BDA as a solution and opportunity to address their business needs.",
    "Link": "http://arxiv.org/abs/1601.03115v1",
    "PDF Link": "http://arxiv.org/pdf/1601.03115v1"
  },
  {
    "Title": "The Anatomy of Big Data Computing",
    "Authors": "Raghavendra Kune, Pramodkumar Konugurthi, Arun Agarwal, Raghavendra Rao Chillarige, Rajkumar Buyya",
    "Published": "2015-09-04T02:34:39Z",
    "Summary": "Advances in information technology and its widespread growth in several areas of business, engineering, medical and scientific studies are resulting in information/data explosion. Knowledge discovery and decision making from such rapidly growing voluminous data is a challenging task in terms of data organization and processing, which is an emerging trend known as Big Data Computing; a new paradigm which combines large scale compute, new data intensive techniques and mathematical models to build data analytics. Big Data computing demands a huge storage and computing for data curation and processing that could be delivered from on-premise or clouds infrastructures. This paper discusses the evolution of Big Data computing, differences between traditional data warehousing and Big Data, taxonomy of Big Data computing and underpinning technologies, integrated platform of Big Data and Clouds known as Big Data Clouds, layered architecture and components of Big Data Cloud and finally discusses open technical challenges and future directions.",
    "Link": "http://arxiv.org/abs/1509.01331v1",
    "PDF Link": "http://arxiv.org/pdf/1509.01331v1"
  },
  {
    "Title": "Mapping Big Data into Knowledge Space with Cognitive\n  Cyber-Infrastructure",
    "Authors": "Hai Zhuge",
    "Published": "2015-07-18T21:38:21Z",
    "Summary": "Big data research has attracted great attention in science, technology, industry and society. It is developing with the evolving scientific paradigm, the fourth industrial revolution, and the transformational innovation of technologies. However, its nature and fundamental challenge have not been recognized, and its own methodology has not been formed. This paper explores and answers the following questions: What is big data? What are the basic methods for representing, managing and analyzing big data? What is the relationship between big data and knowledge? Can we find a mapping from big data into knowledge space? What kind of infrastructure is required to support not only big data management and analysis but also knowledge discovery, sharing and management? What is the relationship between big data and science paradigm? What is the nature and fundamental challenge of big data computing? A multi-dimensional perspective is presented toward a methodology of big data computing.",
    "Link": "http://arxiv.org/abs/1507.06500v1",
    "PDF Link": "http://arxiv.org/pdf/1507.06500v1"
  },
  {
    "Title": "Video Big Data Analytics in the Cloud: Research Issues and Challenges",
    "Authors": "Aftab Alam, Shah Khalid, Muhammad Numan Khan, Tariq Habib Afridi, Irfan Ullah, Young-Koo Lee",
    "Published": "2020-11-05T07:58:19Z",
    "Summary": "On the rise of distributed computing technologies, video big data analytics in the cloud have attracted researchers and practitioners' attention. The current technology and market trends demand an efficient framework for video big data analytics. However, the current work is too limited to provide an architecture on video big data analytics in the cloud, including managing and analyzing video big data, the challenges, and opportunities. This study proposes a service-oriented layered reference architecture for intelligent video big data analytics in the cloud. Finally, we identify and articulate several open research issues and challenges, which have been raised by the deployment of big data technologies in the cloud for video big data analytics. This paper provides the research studies and technologies advancing video analyses in the era of big data and cloud computing. This is the first study that presents the generalized view of the video big data analytics in the cloud to the best of our knowledge.",
    "Link": "http://arxiv.org/abs/2011.02694v1",
    "PDF Link": "http://arxiv.org/pdf/2011.02694v1"
  },
  {
    "Title": "Big Data Security Issues and Challenges in Healthcare",
    "Authors": "Behnam Kiani Kalejahi, Saeed Meshgini, Ayshan Yariyeva, Dawda Ndure, Uzeyir Maharramov, Ali Farzamnia",
    "Published": "2019-12-09T04:51:53Z",
    "Summary": "This paper embodies the usage of Big Data in Healthcare. It is important to note that big data in terms of Architecture and implementation might be or has already or will continue to assist the continuous growth in the field of healthcare. The main important aspects of this study are the general importance of big data in healthcare, the positives big data will help tackle and enhance in this field and not to also forget to mention the tremendous downside big data has on healthcare that is still needed to improve or putting extensive research on. We believe there is still a long way in which institutions and individuals understand the hidden truth about big data. We have highlighted the various ways one could be confidently relied on big data and on the other hand highlighted the weighted importance of big problem big data and expected solutions.",
    "Link": "http://arxiv.org/abs/1912.03848v1",
    "PDF Link": "http://arxiv.org/pdf/1912.03848v1"
  },
  {
    "Title": "Universal Knowledge Discovery from Big Data: Towards a Paradigm Shift\n  from 'Knowledge Discovery' to 'Wisdom Discovery'",
    "Authors": "Bin Shen",
    "Published": "2014-03-28T23:59:34Z",
    "Summary": "Many people hold a vision that big data will provide big insights and have a big impact in the future, and big-data-assisted scientific discovery is seen as an emerging and promising scientific paradigm. However, how to turn big data into deep insights with tremendous value still remains obscure. To meet the challenge, universal knowledge discovery from big data (UKD) is proposed. The new concept focuses on discovering universal knowledge, which exists in the statistical analyses of big data and provides valuable insights into big data. Universal knowledge comes in different forms, e.g., universal patterns, rules, correlations, models and mechanisms. To accelerate big data assisted universal knowledge discovery, a unified research paradigm should be built based on techniques and paradigms from related research domains, especially big data mining and complex systems science. Therefore, I propose an iBEST@SEE methodology. This study lays a solid foundation for the future development of universal knowledge discovery, and offers a pathway to the discovery of \"treasure-trove\" hidden in big data.",
    "Link": "http://arxiv.org/abs/1403.7570v5",
    "PDF Link": "http://arxiv.org/pdf/1403.7570v5"
  },
  {
    "Title": "Journey from Data Mining to Web Mining to Big Data",
    "Authors": "Richa Gupta",
    "Published": "2014-04-16T05:20:28Z",
    "Summary": "This paper describes the journey of big data starting from data mining to web mining to big data. It discusses each of this method in brief and also provides their applications. It states the importance of mining big data today using fast and novel approaches.",
    "Link": "http://arxiv.org/abs/1404.4140v1",
    "PDF Link": "http://arxiv.org/pdf/1404.4140v1"
  },
  {
    "Title": "Storage Solutions for Big Data Systems: A Qualitative Study and\n  Comparison",
    "Authors": "Samiya Khan, Xiufeng Liu, Syed Arshad Ali, Mansaf Alam",
    "Published": "2019-04-25T08:31:50Z",
    "Summary": "Big data systems development is full of challenges in view of the variety of application areas and domains that this technology promises to serve. Typically, fundamental design decisions involved in big data systems design include choosing appropriate storage and computing infrastructures. In this age of heterogeneous systems that integrate different technologies for optimized solution to a specific real world problem, big data system are not an exception to any such rule. As far as the storage aspect of any big data system is concerned, the primary facet in this regard is a storage infrastructure and NoSQL seems to be the right technology that fulfills its requirements. However, every big data application has variable data characteristics and thus, the corresponding data fits into a different data model. This paper presents feature and use case analysis and comparison of the four main data models namely document oriented, key value, graph and wide column. Moreover, a feature analysis of 80 NoSQL solutions has been provided, elaborating on the criteria and points that a developer must consider while making a possible choice. Typically, big data storage needs to communicate with the execution engine and other processing and visualization technologies to create a comprehensive solution. This brings forth second facet of big data storage, big data file formats, into picture. The second half of the research paper compares the advantages, shortcomings and possible use cases of available big data file formats for Hadoop, which is the foundation for most big data computing technologies. Decentralized storage and blockchain are seen as the next generation of big data storage and its challenges and future prospects have also been discussed.",
    "Link": "http://arxiv.org/abs/1904.11498v1",
    "PDF Link": "http://arxiv.org/pdf/1904.11498v1"
  },
  {
    "Title": "BDGS: A Scalable Big Data Generator Suite in Big Data Benchmarking",
    "Authors": "Zijian Ming, Chunjie Luo, Wanling Gao, Rui Han, Qiang Yang, Lei Wang, Jianfeng Zhan",
    "Published": "2014-01-22T02:17:52Z",
    "Summary": "Data generation is a key issue in big data benchmarking that aims to generate application-specific data sets to meet the 4V requirements of big data. Specifically, big data generators need to generate scalable data (Volume) of different types (Variety) under controllable generation rates (Velocity) while keeping the important characteristics of raw data (Veracity). This gives rise to various new challenges about how we design generators efficiently and successfully. To date, most existing techniques can only generate limited types of data and support specific big data systems such as Hadoop. Hence we develop a tool, called Big Data Generator Suite (BDGS), to efficiently generate scalable big data while employing data models derived from real data to preserve data veracity. The effectiveness of BDGS is demonstrated by developing six data generators covering three representative data types (structured, semi-structured and unstructured) and three data sources (text, graph, and table data).",
    "Link": "http://arxiv.org/abs/1401.5465v3",
    "PDF Link": "http://arxiv.org/pdf/1401.5465v3"
  },
  {
    "Title": "BigOP: Generating Comprehensive Big Data Workloads as a Benchmarking\n  Framework",
    "Authors": "Yuqing Zhu, Jianfeng Zhan, Chuliang Weng, Raghunath Nambiar, Jinchao Zhang, Xingzhen Chen, Lei Wang",
    "Published": "2014-01-26T08:41:50Z",
    "Summary": "Big Data is considered proprietary asset of companies, organizations, and even nations. Turning big data into real treasure requires the support of big data systems. A variety of commercial and open source products have been unleashed for big data storage and processing. While big data users are facing the choice of which system best suits their needs, big data system developers are facing the question of how to evaluate their systems with regard to general big data processing needs. System benchmarking is the classic way of meeting the above demands. However, existent big data benchmarks either fail to represent the variety of big data processing requirements, or target only one specific platform, e.g. Hadoop.   In this paper, with our industrial partners, we present BigOP, an end-to-end system benchmarking framework, featuring the abstraction of representative Operation sets, workload Patterns, and prescribed tests. BigOP is part of an open-source big data benchmarking project, BigDataBench. BigOP's abstraction model not only guides the development of BigDataBench, but also enables automatic generation of tests with comprehensive workloads.   We illustrate the feasibility of BigOP by implementing an automatic test generation tool and benchmarking against three widely used big data processing systems, i.e. Hadoop, Spark and MySQL Cluster. Three tests targeting three different application scenarios are prescribed. The tests involve relational data, text data and graph data, as well as all operations and workload patterns. We report results following test specifications.",
    "Link": "http://arxiv.org/abs/1401.6628v2",
    "PDF Link": "http://arxiv.org/pdf/1401.6628v2"
  },
  {
    "Title": "Big data searching using words",
    "Authors": "Santanu Acharjee, Ripunjoy Choudhury",
    "Published": "2024-09-10T13:46:14Z",
    "Summary": "Big data analytics is one of the most promising areas of new research and development in computer science, enterprises, e-commerce, and defense. For many organizations, big data is regarded as one of their most important strategic assets. This explosive growth has made it necessary to develop effective techniques for examining and analyzing big data from a mathematical perspective. Among various methods of analyzing big data, topological data analysis (TDA) is now considered one of the useful tools. However, there is no fundamental concept related to topological structure in big data. In this paper, we introduce some fundamental ideas related to the neighborhood structure of words in data searching, which can be extended to form important topological structures of big data in the future. Additionally, we introduce big data primal in big data searching and discuss the application of neighborhood structures in detecting anomalies in data searching using the Jaccard similarity coefficient.",
    "Link": "http://arxiv.org/abs/2409.15346v2",
    "PDF Link": "http://arxiv.org/pdf/2409.15346v2"
  },
  {
    "Title": "The Streaming k-Mismatch Problem: Tradeoffs between Space and Total Time",
    "Authors": "Shay Golan, Tomasz Kociumaka, Tsvi Kopelowitz, Ely Porat",
    "Published": "2020-04-27T15:41:49Z",
    "Summary": "We revisit the $k$-mismatch problem in the streaming model on a pattern of length $m$ and a streaming text of length $n$, both over a size-$\\sigma$ alphabet. The current state-of-the-art algorithm for the streaming $k$-mismatch problem, by Clifford et al. [SODA 2019], uses $\\tilde O(k)$ space and $\\tilde O\\big(\\sqrt k\\big)$ worst-case time per character. The space complexity is known to be (unconditionally) optimal, and the worst-case time per character matches a conditional lower bound. However, there is a gap between the total time cost of the algorithm, which is $\\tilde O(n\\sqrt k)$, and the fastest known offline algorithm, which costs $\\tilde O\\big(n + \\min\\big(\\frac{nk}{\\sqrt m},\\sigma n\\big)\\big)$ time. Moreover, it is not known whether improvements over the $\\tilde O(n\\sqrt k)$ total time are possible when using more than $O(k)$ space.   We address these gaps by designing a randomized streaming algorithm for the $k$-mismatch problem that, given an integer parameter $k\\le s \\le m$, uses $\\tilde O(s)$ space and costs $\\tilde O\\big(n+\\min\\big(\\frac {nk^2}m,\\frac{nk}{\\sqrt s},\\frac{\\sigma nm}s\\big)\\big)$ total time. For $s=m$, the total runtime becomes $\\tilde O\\big(n + \\min\\big(\\frac{nk}{\\sqrt m},\\sigma n\\big)\\big)$, which matches the time cost of the fastest offline algorithm. Moreover, the worst-case time cost per character is still $\\tilde O\\big(\\sqrt k\\big)$.",
    "Link": "http://arxiv.org/abs/2004.12881v1",
    "PDF Link": "http://arxiv.org/pdf/2004.12881v1"
  },
  {
    "Title": "Towards PTAS for Precedence Constrained Scheduling via Combinatorial\n  Algorithms",
    "Authors": "Shi Li",
    "Published": "2020-04-02T19:16:22Z",
    "Summary": "We study the classic problem of scheduling $n$ precedence constrained unit-size jobs on $m = O(1)$ machines so as to minimize the makespan. In a recent breakthrough, Levey and Rothvoss \\cite{LR16} developed a $(1+\\epsilon)$-approximation for the problem with running time $\\exp\\Big(\\exp\\Big(O\\big(\\frac{m^2}{\\epsilon^2}\\log^2\\log n\\big)\\Big)\\Big)$, via the Sherali-Adams lift of the basic linear programming relaxation for the problem by $\\exp\\Big(O\\big(\\frac{m^2}{\\epsilon^2}\\log^2\\log n\\big)\\Big)$ levels. Garg \\cite{Garg18} recently improved the number of levels to $\\log ^{O(m^2/\\epsilon^2)}n$, and thus the running time to $\\exp\\big(\\log ^{O(m^2/\\epsilon^2)}n\\big)$, which is quasi-polynomial for constant $m$ and $\\epsilon$.   In this paper we present an algorithm that achieves $(1+\\epsilon)$-approximation for the problem with running time $n^{O\\left(\\frac{m^4}{\\epsilon^3}\\log^3\\log n\\right)}$, which is very close to a polynomial for constant $m$ and $\\epsilon$. Unlike the algorithms of Levey-Rothvoss and Garg, which are based on linear-programming hierarchy, our algorithm is purely combinatorial. For this problem, we show that the conditioning operations on the lifted LP solution can be replaced by making guesses about the optimum schedule.",
    "Link": "http://arxiv.org/abs/2004.01231v2",
    "PDF Link": "http://arxiv.org/pdf/2004.01231v2"
  },
  {
    "Title": "Turning Multidimensional Big Data Analytics into Practice: Design and\n  Implementation of ClustCube Big-Data Tools in Real-Life Scenarios",
    "Authors": "Alfredo Cuzzocrea, Abderraouf Hafsaoui, Ismail Benlaredj",
    "Published": "2024-07-26T08:55:00Z",
    "Summary": "Multidimensional Big Data Analytics is an emerging area that marries the capabilities of OLAP with modern Big Data Analytics. Essentially, the idea is engrafting multidimensional models into Big Data analytics processes to gain into expressive power of the overall discovery task. ClustCube is a state-of-the-art model that combines OLAP and Clustering, thus delving into practical and well-understood advantages in the context of real-life applications and systems. In this paper, we show how ClustCube can effectively and efficiently realizing nice tools for supporting Multidimensional Big Data Analytics, and assess these tools in the context of real-life research projects.",
    "Link": "http://arxiv.org/abs/2407.18604v1",
    "PDF Link": "http://arxiv.org/pdf/2407.18604v1"
  },
  {
    "Title": "United Statistical Algorithm, Small and Big Data: Future OF Statistician",
    "Authors": "Emanuel Parzen, Subhadeep Mukhopadhyay",
    "Published": "2013-08-02T23:54:44Z",
    "Summary": "This article provides the role of big idea statisticians in future of Big Data Science. We describe the `United Statistical Algorithms' framework for comprehensive unification of traditional and novel statistical methods for modeling Small Data and Big Data, especially mixed data (discrete, continuous).",
    "Link": "http://arxiv.org/abs/1308.0641v1",
    "PDF Link": "http://arxiv.org/pdf/1308.0641v1"
  },
  {
    "Title": "Big Data in Cloud Computing Review and Opportunities",
    "Authors": "Manoj Muniswamaiah, Tilak Agerwala, Charles Tappert",
    "Published": "2019-12-17T17:23:36Z",
    "Summary": "Big Data is used in decision making process to gain useful insights hidden in the data for business and engineering. At the same time it presents challenges in processing, cloud computing has helped in advancement of big data by providing computational, networking and storage capacity. This paper presents the review, opportunities and challenges of transforming big data using cloud computing resources.",
    "Link": "http://arxiv.org/abs/1912.10821v1",
    "PDF Link": "http://arxiv.org/pdf/1912.10821v1"
  },
  {
    "Title": "Geospatial Big Data Handling with High Performance Computing: Current\n  Approaches and Future Directions",
    "Authors": "Zhenlong Li",
    "Published": "2019-07-29T02:37:43Z",
    "Summary": "Geospatial big data plays a major role in the era of big data, as most data today are inherently spatial, collected with ubiquitous location-aware sensors. Efficiently collecting, managing, storing, and analyzing geospatial data streams enables development of new decision-support systems and provides unprecedented opportunities for business, science, and engineering. However, handling the \"Vs\" (volume, variety, velocity, veracity, and value) of big data is a challenging task. This is especially true for geospatial big data, since the massive datasets must be analyzed in the context of space and time. High performance computing (HPC) provides an essential solution to geospatial big data challenges. This chapter first summarizes four key aspects for handling geospatial big data with HPC and then briefly reviews existing HPC-related platforms and tools for geospatial big data processing. Lastly, future research directions in using HPC for geospatial big data handling are discussed.",
    "Link": "http://arxiv.org/abs/1907.12182v1",
    "PDF Link": "http://arxiv.org/pdf/1907.12182v1"
  },
  {
    "Title": "A Random Sample Partition Data Model for Big Data Analysis",
    "Authors": "Salman Salloum, Yulin He, Joshua Zhexue Huang, Xiaoliang Zhang, Tamer Z. Emara, Chenghao Wei, Heping He",
    "Published": "2017-12-12T06:49:28Z",
    "Summary": "Big data sets must be carefully partitioned into statistically similar data subsets that can be used as representative samples for big data analysis tasks. In this paper, we propose the random sample partition (RSP) data model to represent a big data set as a set of non-overlapping data subsets, called RSP data blocks, where each RSP data block has a probability distribution similar to the whole big data set. Under this data model, efficient block level sampling is used to randomly select RSP data blocks, replacing expensive record level sampling to select sample data from a big distributed data set on a computing cluster. We show how RSP data blocks can be employed to estimate statistics of a big data set and build models which are equivalent to those built from the whole big data set. In this approach, analysis of a big data set becomes analysis of few RSP data blocks which have been generated in advance on the computing cluster. Therefore, the new method for data analysis based on RSP data blocks is scalable to big data.",
    "Link": "http://arxiv.org/abs/1712.04146v2",
    "PDF Link": "http://arxiv.org/pdf/1712.04146v2"
  },
  {
    "Title": "The Promise and Prejudice of Big Data in Intelligence Community",
    "Authors": "Karan Jani",
    "Published": "2016-10-27T06:20:00Z",
    "Summary": "Big data holds critical importance in the current generation of information technology, with applications ranging from financial, industrial, academic to defense sectors. With the exponential rise of open source data from social media and increasing government monitoring, big data is now also linked with national security, and subsequently to the intelligence community. In this study I review the scope of big data sciences in the functioning of intelligence community. The major part of my study focuses on the inherent limitations of big data, which affects the intelligence agencies from gathering of information to anticipating surprises. The limiting factors range from technical to ethical issues connected with big data. My study concludes the need of experts with domain knowledge from intelligence community to efficiently guide big data analysis for timely filling the knowledge gaps. As a case study on limitations of using big data, I narrate some of the ongoing work in nuclear intelligence using simple analytics and argue on why big data analysis in that case would lead to unnecessary complications. For further investigation, I highlight cases of crowdsource forecasting tournaments and predicting unrest from social media.",
    "Link": "http://arxiv.org/abs/1610.08629v1",
    "PDF Link": "http://arxiv.org/pdf/1610.08629v1"
  },
  {
    "Title": "Profit Maximization Auction and Data Management in Big Data Markets",
    "Authors": "Yutao Jiao, Ping Wang, Dusit Niyato, Mohammad Abu Alsheikh, Shaohan Feng",
    "Published": "2017-04-05T03:49:10Z",
    "Summary": "A big data service is any data-originated resource that is offered over the Internet. The performance of a big data service depends on the data bought from the data collectors. However, the problem of optimal pricing and data allocation in big data services is not well-studied. In this paper, we propose an auction-based big data market model. We first define the data cost and utility based on the impact of data size on the performance of big data analytics, e.g., machine learning algorithms. The big data services are considered as digital goods and uniquely characterized with \"unlimited supply\" compared to conventional goods which are limited. We therefore propose a Bayesian profit maximization auction which is truthful, rational, and computationally efficient. The optimal service price and data size are obtained by solving the profit maximization auction. Finally, experimental results on a real-world taxi trip dataset show that our big data market model and auction mechanism effectively solve the profit maximization problem of the service provider.",
    "Link": "http://arxiv.org/abs/1704.01260v2",
    "PDF Link": "http://arxiv.org/pdf/1704.01260v2"
  },
  {
    "Title": "Contextualization of Big Data Quality: A framework for comparison",
    "Authors": "Mostafa Mirzaie, Behshid Behkamal, Samad Paydar",
    "Published": "2019-10-26T07:24:14Z",
    "Summary": "With the advent of big data applications and the increasing amount of data being produced in these applications, the importance of efficient methods for big data analysis has become highly evident. However, the success of any such method will be hindered should the data lacks the required quality. Big data quality assessment is therefore a major requirement for any organization or business that use big data analytics for its decision making. On the other hand, using contextual information is advantageous in many analysis tasks in various domains, e.g. user behavior analysis in the social networks. However, the big data quality assessment has benefited less from this potential. There is a vast variety of data sources in the big data domain that can be utilized to improve the quality evaluation of big data. Including contextual information provided by these sources into the big data quality assessment process is an emerging trend towards more advanced techniques aimed at enhancing the performance and accuracy of quality assessment. This paper presents a context classification framework for big data quality, categorizing the context features into four primary dimensions: 1) context category, 2) data source type that contextual features come from, 3) discovery and extraction method of context, and 4) the quality factors affected by the contextual data. The proposed model introduces new context features and dimensions that need to be taken into consideration in quality assessment of big data. The initial evaluation demonstrates that the model is more understandable, more comprehensive, richer, and more useful compared to existing models.",
    "Link": "http://arxiv.org/abs/1911.01274v1",
    "PDF Link": "http://arxiv.org/pdf/1911.01274v1"
  },
  {
    "Title": "Big Graph Mining: Frameworks and Techniques",
    "Authors": "Sabeur Aridhi, Engelbert Mephu Nguifo",
    "Published": "2016-02-09T16:53:08Z",
    "Summary": "Big graph mining is an important research area and it has attracted considerable attention. It allows to process, analyze, and extract meaningful information from large amounts of graph data. Big graph mining has been highly motivated not only by the tremendously increasing size of graphs but also by its huge number of applications. Such applications include bioinformatics, chemoinformatics and social networks. One of the most challenging tasks in big graph mining is pattern mining in big graphs. This task consists on using data mining algorithms to discover interesting, unexpected and useful patterns in large amounts of graph data. It aims also to provide deeper understanding of graph data. In this context, several graph processing frameworks and scaling data mining/pattern mining techniques have been proposed to deal with very big graphs. This paper gives an overview of existing data mining and graph processing frameworks that deal with very big graphs. Then it presents a survey of current researches in the field of data mining / pattern mining in big graphs and discusses the main research issues related to this field. It also gives a categorization of both distributed data mining and machine learning techniques, graph processing frameworks and large scale pattern mining approaches.",
    "Link": "http://arxiv.org/abs/1602.03072v1",
    "PDF Link": "http://arxiv.org/pdf/1602.03072v1"
  },
  {
    "Title": "Big Data: Understanding Big Data",
    "Authors": "Kevin Taylor-Sakyi",
    "Published": "2016-01-15T19:10:43Z",
    "Summary": "Steve Jobs, one of the greatest visionaries of our time was quoted in 1996 saying \"a lot of times, people do not know what they want until you show it to them\" [38] indicating he advocated products to be developed based on human intuition rather than research. With the advancements of mobile devices, social networks and the Internet of Things, enormous amounts of complex data, both structured and unstructured are being captured in hope to allow organizations to make better business decisions as data is now vital for an organizations success. These enormous amounts of data are referred to as Big Data, which enables a competitive advantage over rivals when processed and analyzed appropriately. However Big Data Analytics has a few concerns including Management of Data-lifecycle, Privacy & Security, and Data Representation. This paper reviews the fundamental concept of Big Data, the Data Storage domain, the MapReduce programming paradigm used in processing these large datasets, and focuses on two case studies showing the effectiveness of Big Data Analytics and presents how it could be of greater good in the future if handled appropriately.",
    "Link": "http://arxiv.org/abs/1601.04602v1",
    "PDF Link": "http://arxiv.org/pdf/1601.04602v1"
  },
  {
    "Title": "Hiding Information in Big Data based on Deep Learning",
    "Authors": "Dingju Zhu",
    "Published": "2019-12-31T03:23:54Z",
    "Summary": "The current approach of information hiding based on deep learning model can not directly use the original data as carriers, which means the approach can not make use of the existing data in big data to hiding information. We proposed a novel method of information hiding in big data based on deep learning. Our method uses the existing data in big data as carriers and uses deep learning models to hide and extract secret messages in big data. The data amount of big data is unlimited and thus the data amount of secret messages hided in big data can also be unlimited. Before opponents want to extract secret messages from carriers, they need to find the carriers, however finding out the carriers from big data is just like finding out a box from the sea. Deep learning models are well known as deep black boxes in which the process from the input to the output is very complex, and thus the deep learning model for information hiding is almost impossible for opponents to reconstruct. The results also show that our method can hide secret messages safely, conveniently, quickly and with no limitation on the data amount.",
    "Link": "http://arxiv.org/abs/1912.13156v2",
    "PDF Link": "http://arxiv.org/pdf/1912.13156v2"
  },
  {
    "Title": "A Survey on Sampling and Profiling over Big Data (Technical Report)",
    "Authors": "Zhicheng Liu, Aoqian Zhang",
    "Published": "2020-05-08T02:54:07Z",
    "Summary": "Due to the development of internet technology and computer science, data is exploding at an exponential rate. Big data brings us new opportunities and challenges. On the one hand, we can analyze and mine big data to discover hidden information and get more potential value. On the other hand, the 5V characteristic of big data, especially Volume which means large amount of data, brings challenges to storage and processing. For some traditional data mining algorithms, machine learning algorithms and data profiling tasks, it is very difficult to handle such a large amount of data. The large amount of data is highly demanding hardware resources and time consuming. Sampling methods can effectively reduce the amount of data and help speed up data processing. Hence, sampling technology has been widely studied and used in big data context, e.g., methods for determining sample size, combining sampling with big data processing frameworks. Data profiling is the activity that finds metadata of data set and has many use cases, e.g., performing data profiling tasks on relational data, graph data, and time series data for anomaly detection and data repair. However, data profiling is computationally expensive, especially for large data sets. Therefore, this paper focuses on researching sampling and profiling in big data context and investigates the application of sampling in different categories of data profiling tasks. From the experimental results of these studies, the results got from the sampled data are close to or even exceed the results of the full amount of data. Therefore, sampling technology plays an important role in the era of big data, and we also have reason to believe that sampling technology will become an indispensable step in big data processing in the future.",
    "Link": "http://arxiv.org/abs/2005.05079v1",
    "PDF Link": "http://arxiv.org/pdf/2005.05079v1"
  },
  {
    "Title": "Leveraging cloud based big data analytics in knowledge management for\n  enhanced decision making in organizations",
    "Authors": "Mohammad Shorfuzzaman",
    "Published": "2017-02-15T06:29:34Z",
    "Summary": "In recent past, big data opportunities have gained much momentum to enhance knowledge management in organizations. However, big data due to its various properties like high volume, variety, and velocity can no longer be effectively stored and analyzed with traditional data management techniques to generate values for knowledge development. Hence, new technologies and architectures are required to store and analyze this big data through advanced data analytics and in turn generate vital real-time knowledge for effective decision making by organizations. More specifically, it is necessary to have a single infrastructure which provides common functionality of knowledge management, and flexible enough to handle different types of big data and big data analysis tasks. Cloud computing infrastructures capable of storing and processing large volume of data can be used for efficient big data processing because it minimizes the initial cost for the large-scale computing infrastructure demanded by big data analytics. This paper aims to explore the impact of big data analytics on knowledge management and proposes a cloud-based conceptual framework that can analyze big data in real time to facilitate enhanced decision making intended for competitive advantage. Thus, this framework will pave the way for organizations to explore the relationship between big data analytics and knowledge management which are mostly deemed as two distinct entities.",
    "Link": "http://arxiv.org/abs/1702.04474v1",
    "PDF Link": "http://arxiv.org/pdf/1702.04474v1"
  },
  {
    "Title": "Proceedings of the 3rd Italian Conference on Big Data and Data Science\n  (ITADATA2024)",
    "Authors": "Nicola Bena, Claudia Diamantini, Michela Natilli, Luigi Romano, Giovanni Stilo, Valentina Pansanella, Claudio A. Ardagna, Anna Monreale, Roberto Trasarti",
    "Published": "2025-03-19T06:48:18Z",
    "Summary": "Proceedings of the 3rd Italian Conference on Big Data and Data Science (ITADATA2024), held in Pisa, Italy, September 17-19, 2024.   The Italian Conference on Big Data and Data Science (ITADATA2024) is the annual event supported by the CINI Big Data National Laboratory and ISTI CNR that aims to put together Italian researchers and professionals from academia, industry, government, and public administration working in the field of big data and data science, as well as related fields (e.g., security and privacy, HPC, Cloud).   ITADATA2024 covered research on all theoretical and practical aspects of Big Data and data science including data governance, data processing, data analysis, data reporting, data protection, as well as experimental studies and lessons learned. In particular, ITADATA2024 focused on   - Data spaces   - Data processing life cycle   - Machine learning and Large Language Models   - Applications of big data and data science in healthcare, finance, industry 5.0, and beyond   - Data science for social network analysis",
    "Link": "http://arxiv.org/abs/2503.14937v1",
    "PDF Link": "http://arxiv.org/pdf/2503.14937v1"
  },
  {
    "Title": "Big Data Computing Using Cloud-Based Technologies, Challenges and Future\n  Perspectives",
    "Authors": "Samiya Khan, Kashish Ara Shakil, Mansaf Alam",
    "Published": "2017-11-24T07:26:51Z",
    "Summary": "The excessive amounts of data generated by devices and Internet-based sources at a regular basis constitute, big data. This data can be processed and analyzed to develop useful applications for specific domains. Several mathematical and data analytics techniques have found use in this sphere. This has given rise to the development of computing models and tools for big data computing. However, the storage and processing requirements are overwhelming for traditional systems and technologies. Therefore, there is a need for infrastructures that can adjust the storage and processing capability in accordance with the changing data dimensions. Cloud Computing serves as a potential solution to this problem. However, big data computing in the cloud has its own set of challenges and research issues. This chapter surveys the big data concept, discusses the mathematical and data analytics techniques that can be used for big data and gives taxonomy of the existing tools, frameworks and platforms available for different big data computing models. Besides this, it also evaluates the viability of cloud-based big data computing, examines existing challenges and opportunities, and provides future research directions in this field.",
    "Link": "http://arxiv.org/abs/1712.05233v1",
    "PDF Link": "http://arxiv.org/pdf/1712.05233v1"
  },
  {
    "Title": "Sharp Frequency Bounds for Sample-Based Queries",
    "Authors": "Eric Bax, John Donald",
    "Published": "2022-08-14T00:38:14Z",
    "Summary": "A data sketch algorithm scans a big data set, collecting a small amount of data -- the sketch, which can be used to statistically infer properties of the big data set. Some data sketch algorithms take a fixed-size random sample of a big data set, and use that sample to infer frequencies of items that meet various criteria in the big data set. This paper shows how to statistically infer probably approximately correct (PAC) bounds for those frequencies, efficiently, and precisely enough that the frequency bounds are either sharp or off by only one, which is the best possible result without exact computation.",
    "Link": "http://arxiv.org/abs/2208.06753v1",
    "PDF Link": "http://arxiv.org/pdf/2208.06753v1"
  },
  {
    "Title": "A systematic data characteristic understanding framework towards\n  physical-sensor big data challenges",
    "Authors": "Zhipeng Ma, Bo Nørregaard Jørgensen, Zheng Grace Ma",
    "Published": "2025-01-22T08:49:44Z",
    "Summary": "Big data present new opportunities for modern society while posing challenges for data scientists. Recent advancements in sensor networks and the widespread adoption of IoT have led to the collection of physical-sensor data on an enormous scale. However, significant challenges arise in high-quality big data analytics. To uncover big data challenges and enhance data quality, it is essential to quantitatively unveil data characteristics. Furthermore, the existing studies lack analysis of the specific time-related characteristics. Enhancing the efficiency and precision of data analytics through the big data lifecycle requires a comprehensive understanding of data characteristics to address the hidden big data challenges. To fill in the research gap, this paper proposes a systematic data characteristic framework based on a 6Vs model. The framework aims to unveil the data characteristics in terms of data volume, variety, velocity, veracity, value, and variability through a set of statistical indicators. This model improves the objectivity of data characteristic understanding by relying solely on data-driven indicators. The indicators related to time-related characteristics in physical-sensor data are also included. Furthermore, the big data challenges are linked to each dimension of the 6Vs model to gain a quantitative understanding of the data challenges. Finally, a pipeline is developed to implement the proposed framework, and two case studies are conducted to illustrate the process of understanding the physical-sensor data characteristics and making recommendations for data preprocessing to address the big data challenges. The proposed framework is able to analyze the characteristics of all physical-sensor data, therefore, identifying potential challenges in subsequent analytics, and providing recommendations for data preprocessing.",
    "Link": "http://arxiv.org/abs/2501.12720v1",
    "PDF Link": "http://arxiv.org/pdf/2501.12720v1"
  },
  {
    "Title": "Video Big Data Analytics in the Cloud: A Reference Architecture, Survey,\n  Opportunities, and Open Research Issues",
    "Authors": "Aftab Alam, Irfan Ullah, Young-Koo Lee",
    "Published": "2020-11-16T09:21:53Z",
    "Summary": "The proliferation of multimedia devices over the Internet of Things (IoT) generates an unprecedented amount of data. Consequently, the world has stepped into the era of big data. Recently, on the rise of distributed computing technologies, video big data analytics in the cloud has attracted the attention of researchers and practitioners. The current technology and market trends demand an efficient framework for video big data analytics. However, the current work is too limited to provide a complete survey of recent research work on video big data analytics in the cloud, including the management and analysis of a large amount of video data, the challenges, opportunities, and promising research directions. To serve this purpose, we present this study, which conducts a broad overview of the state-of-the-art literature on video big data analytics in the cloud. It also aims to bridge the gap among large-scale video analytics challenges, big data solutions, and cloud computing. In this study, we clarify the basic nomenclatures that govern the video analytics domain and the characteristics of video big data while establishing its relationship with cloud computing. We propose a service-oriented layered reference architecture for intelligent video big data analytics in the cloud. Then, a comprehensive and keen review has been conducted to examine cutting-edge research trends in video big data analytics. Finally, we identify and articulate several open research issues and challenges, which have been raised by the deployment of big data technologies in the cloud for video big data analytics. To the best of our knowledge, this is the first study that presents the generalized view of the video big data analytics in the cloud. This paper provides the research studies and technologies advancing video analyses in the era of big data and cloud computing.",
    "Link": "http://arxiv.org/abs/2011.07807v1",
    "PDF Link": "http://arxiv.org/pdf/2011.07807v1"
  },
  {
    "Title": "Big Data Generated by Connected and Automated Vehicles for Safety\n  Monitoring, Assessment and Improvement, Final Report (Year 3)",
    "Authors": "Asad J. Khattak, Iman Mahdinia, Sevin Mohammadi, Amin Mohammadnazar, Behram Wali",
    "Published": "2021-01-09T20:00:26Z",
    "Summary": "This report focuses on safety aspects of connected and automated vehicles (CAVs). The fundamental question to be answered is how can CAVs improve road users' safety? Using advanced data mining and thematic text analytics tools, the goal is to systematically synthesize studies related to Big Data for safety monitoring and improvement. Within this domain, the report systematically compares Big Data initiatives related to transportation initiatives nationally and internationally and provides insights regarding the evolution of Big Data science applications related to CAVs and new challenges. The objectives addressed are: 1-Creating a database of Big Data efforts by acquiring reports, white papers, and journal publications; 2-Applying text analytics tools to extract key concepts, and spot patterns and trends in Big Data initiatives; 3-Understanding the evolution of CAV Big Data in the context of safety by quantifying granular taxonomies and modeling entity relations among contents in CAV Big Data research initiatives, and 4-Developing a foundation for exploring new approaches to tracking and analyzing CAV Big Data and related innovations. The study synthesizes and derives high-quality information from innovative research activities undertaken by various research entities through Big Data initiatives. The results can provide a conceptual foundation for developing new approaches for guiding and tracking the safety implications of Big Data and related innovations.",
    "Link": "http://arxiv.org/abs/2101.06106v1",
    "PDF Link": "http://arxiv.org/pdf/2101.06106v1"
  },
  {
    "Title": "Big Data Systems Meet Machine Learning Challenges: Towards Big Data\n  Science as a Service",
    "Authors": "Radwa Elshawi, Sherif Sakr",
    "Published": "2017-09-21T18:50:32Z",
    "Summary": "Recently, we have been witnessing huge advancements in the scale of data we routinely generate and collect in pretty much everything we do, as well as our ability to exploit modern technologies to process, analyze and understand this data. The intersection of these trends is what is called, nowadays, as Big Data Science. Cloud computing represents a practical and cost-effective solution for supporting Big Data storage, processing and for sophisticated analytics applications. We analyze in details the building blocks of the software stack for supporting big data science as a commodity service for data scientists. We provide various insights about the latest ongoing developments and open challenges in this domain.",
    "Link": "http://arxiv.org/abs/1709.07493v1",
    "PDF Link": "http://arxiv.org/pdf/1709.07493v1"
  },
  {
    "Title": "Big Data Model \"Entity and Features",
    "Authors": "Nataliya Shakhovska, Uyrii Bolubash, Oleh Veres",
    "Published": "2019-05-03T16:13:52Z",
    "Summary": "The article deals with the problem which led to Big Data. Big Data information technology is the set of methods and means of processing different types of structured and unstructured dynamic large amounts of data for their analysis and use of decision support. Features of NoSQL databases and categories are described. The developed Big Data Model \"Entity and Features\" allows determining the distance between the sources of data on the availability of information about a particular entity. The information structure of Big Data has been devised. It became a basis for further research and for concentrating on a problem of development of diverse data without their preliminary integration.",
    "Link": "http://arxiv.org/abs/1905.01306v1",
    "PDF Link": "http://arxiv.org/pdf/1905.01306v1"
  },
  {
    "Title": "Big Data Analytics Using Cloud and Crowd",
    "Authors": "Mohammad Allahbakhsh, Saeed Arbabi, Hamid-Reza Motahari-Nezhad, Boualem Benatallah",
    "Published": "2016-04-16T13:46:36Z",
    "Summary": "The increasing application of social and human-enabled systems in people's daily life from one side and from the other side the fast growth of mobile and smart phones technologies have resulted in generating tremendous amount of data, also referred to as big data, and a need for analyzing these data, i.e., big data analytics. Recently a trend has emerged to incorporate human computing power into big data analytics to solve some shortcomings of existing big data analytics such as dealing with semi or unstructured data. Including crowd into big data analytics creates some new challenges such as security, privacy and availability issues.   In this paper study hybrid human-machine big data analytics and propose a framework to study these systems from crowd involvement point of view. We identify some open issues in the area and propose a set of research directions for the future of big data analytics area.",
    "Link": "http://arxiv.org/abs/1604.04749v1",
    "PDF Link": "http://arxiv.org/pdf/1604.04749v1"
  },
  {
    "Title": "Big Data, Big Decisions Choosing the Right Database",
    "Authors": "Mohamed Hassan",
    "Published": "2024-05-03T22:37:44Z",
    "Summary": "In the burgeoning era of big data, selecting the optimal database solution has become a critical decision for organizations across every industry. Big data demands a powerful database solution. Traditionally, SQL Database, Database ruled, offering a structured approach familiar to many organizations. However, big data's complexity and unstructured nature challenge SQL Database's limitations. Enter NoSQL Database: flexible and scalable, making them ideal for big data's ever-changing nature. We'll explore the key differences between SQL and NoSQL Database. Performance-wise, SQL Database shines for structured queries. Its standardized language (SQL) ensures data consistency and complex analysis. But for big data's unstructured formats, this rigidity becomes a hurdle. NoSQL offers a welcome contrast. Its flexible schema allows for diverse data formats and evolving structures, perfect for undefined or frequently changing data models. Additionally, NoSQL boasts superior horizontal scalability, distributing data across multiple servers for cost-effective growth. Understanding these key differentiators empowers organizations to choose the optimal database for their big data needs.",
    "Link": "http://arxiv.org/abs/2405.02506v2",
    "PDF Link": "http://arxiv.org/pdf/2405.02506v2"
  },
  {
    "Title": "Greening Big Data Networks: The Impact of Veracity",
    "Authors": "Ali M. Al-Salim, Taisir E. H. El-Gorashi, Ahmed Q. Lawey, Jaafar M. H. Elmirghani",
    "Published": "2018-12-26T13:33:45Z",
    "Summary": "The continuous increase in big data applications, in number and types, creates new challenges that should be tackled by the green ICT community. Big data is mainly characterized by 4 Vs volume, variety, velocity, and veracity. Each V poses a number of challenges that have implications on the energy efficiency of the underlying networks carrying the big data. Addressing the veracity of the data is a more serious challenge to data scientists, since they need to distinguish between the meaningful data and the dirty data. In this article, we investigate the impact of big data veracity on greening IP by developing a Mixed Integer Linear Programming, MILP, model that encapsulates the distinctive features of veracity. In our analyses, the big data network was greened by cleansing the raw big data before processing and then progressively processing the cleansed big data at strategic locations, dubbed processing nodes, PNs. The PNs are built into the network along the path from the sources to the centralized datacenters. At each PN, the cleansed data was processed and smaller volume of useful information was extracted progressively, thereby, reducing the network power consumption. Furthermore, a backup for the cleansed data was stored in an optimally selected Backup Node, BN. We evaluated the network power saving that can be achieved by a green big data network compared to the classical non-progressive approach. We obtained up to 52 percent network power savings, on average, in the green big data approach compared to the classical approach.",
    "Link": "http://arxiv.org/abs/1812.10307v1",
    "PDF Link": "http://arxiv.org/pdf/1812.10307v1"
  },
  {
    "Title": "Federated Learning for Big Data: A Survey on Opportunities,\n  Applications, and Future Directions",
    "Authors": "Thippa Reddy Gadekallu, Quoc-Viet Pham, Thien Huynh-The, Sweta Bhattacharya, Praveen Kumar Reddy Maddikunta, Madhusanka Liyanage",
    "Published": "2021-10-08T14:36:43Z",
    "Summary": "Big data has remarkably evolved over the last few years to realize an enormous volume of data generated from newly emerging services and applications and a massive number of Internet-of-Things (IoT) devices. The potential of big data can be realized via analytic and learning techniques, in which the data from various sources is transferred to a central cloud for central storage, processing, and training. However, this conventional approach faces critical issues in terms of data privacy as the data may include sensitive data such as personal information, governments, banking accounts. To overcome this challenge, federated learning (FL) appeared to be a promising learning technique. However, a gap exists in the literature that a comprehensive survey on FL for big data services and applications is yet to be conducted. In this article, we present a survey on the use of FL for big data services and applications, aiming to provide general readers with an overview of FL, big data, and the motivations behind the use of FL for big data. In particular, we extensively review the use of FL for key big data services, including big data acquisition, big data storage, big data analytics, and big data privacy preservation. Subsequently, we review the potential of FL for big data applications, such as smart city, smart healthcare, smart transportation, smart grid, and social media. Further, we summarize a number of important projects on FL-big data and discuss key challenges of this interesting topic along with several promising solutions and directions.",
    "Link": "http://arxiv.org/abs/2110.04160v2",
    "PDF Link": "http://arxiv.org/pdf/2110.04160v2"
  },
  {
    "Title": "Big Data: Overview",
    "Authors": "Richa Gupta, Sunny Gupta, Anuradha Singhal",
    "Published": "2014-04-16T04:58:57Z",
    "Summary": "Big data is data that exceeds the processing capacity of traditional databases. The data is too big to be processed by a single machine. New and innovative methods are required to process and store such large volumes of data. This paper provides an overview on big data, its importance in our live and some technologies to handle big data.",
    "Link": "http://arxiv.org/abs/1404.4136v1",
    "PDF Link": "http://arxiv.org/pdf/1404.4136v1"
  },
  {
    "Title": "Data Partitioning View of Mining Big Data",
    "Authors": "Shichao Zhang",
    "Published": "2016-11-29T16:05:56Z",
    "Summary": "There are two main approximations of mining big data in memory. One is to partition a big dataset to several subsets, so as to mine each subset in memory. By this way, global patterns can be obtained by synthesizing all local patterns discovered from these subsets. Another is the statistical sampling method. This indicates that data partitioning should be an important strategy for mining big data. This paper recalls our work on mining big data with a data partitioning and shows some interesting findings among the local patterns discovered from subsets of a dataset.",
    "Link": "http://arxiv.org/abs/1611.09691v1",
    "PDF Link": "http://arxiv.org/pdf/1611.09691v1"
  },
  {
    "Title": "What is the next innovation after the internet of things?",
    "Authors": "Hung Cao",
    "Published": "2017-08-23T19:32:55Z",
    "Summary": "The world had witnessed several generations of the Internet. Starting with the Fixed Internet, then the Mobile Internet, scientists now focus on many types of research related to the \"Thing\" Internet (or Internet of Things). The question is \"what is the next Internet generation after the Thing Internet?\" This paper envisions about the Tactile Internet which could be the next Internet generation in the near future. The paper will introduce what is the tactile internet, why it could be the next future Internet, as well as the impact and its application in the future society. Furthermore, some challenges and the requirements are presented to guide further research in this near future field.",
    "Link": "http://arxiv.org/abs/1708.07160v1",
    "PDF Link": "http://arxiv.org/pdf/1708.07160v1"
  },
  {
    "Title": "In Things We Trust? Towards trustability in the Internet of Things",
    "Authors": "Jaap-Henk Hoepman",
    "Published": "2011-09-12T21:46:37Z",
    "Summary": "This essay discusses the main privacy, security and trustability issues with the Internet of Things.",
    "Link": "http://arxiv.org/abs/1109.2637v1",
    "PDF Link": "http://arxiv.org/pdf/1109.2637v1"
  },
  {
    "Title": "Privacy in the Internet of Things: Threats and Challenges",
    "Authors": "Jan Henrik Ziegeldorf, Oscar Garcia Morchon, Klaus Wehrle",
    "Published": "2015-05-28T13:22:07Z",
    "Summary": "The Internet of Things paradigm envisions the pervasive interconnection and cooperation of smart things over the current and future Internet infrastructure. The Internet of Things is, thus, the evolution of the Internet to cover the real-world, enabling many new services that will improve people's everyday lives, spawn new businesses and make buildings, cities and transport smarter. Smart things allow indeed for ubiquitous data collection or tracking, but these useful features are also examples of privacy threats that are already now limiting the success of the Internet of Things vision when not implemented correctly. These threats involve new challenges such as the pervasive privacy-aware management of personal data or methods to control or avoid ubiquitous tracking and profiling. This paper analyzes the privacy issues in the Internet of Things in detail. To this end, we first discuss the evolving features and trends in the Internet of Things with the goal of scrutinizing their privacy implications. Second, we classify and examine privacy threats in this new setting, pointing out the challenges that need to be overcome to ensure that the Internet of Things becomes a reality.",
    "Link": "http://arxiv.org/abs/1505.07683v1",
    "PDF Link": "http://arxiv.org/pdf/1505.07683v1"
  },
  {
    "Title": "Sensing as a Service (S2aaS): Buying and Selling IoT Data",
    "Authors": "Charith Perera",
    "Published": "2017-02-08T11:32:51Z",
    "Summary": "The Internet of Things (IoT) [1] envisions the creation of an environment where everyday objects (e.g. microwaves, fridges, cars, coffee machines, etc.) are connected to the internet and make users' lives more convenient. It will also lead users to consume resources more efficiently.",
    "Link": "http://arxiv.org/abs/1702.02380v1",
    "PDF Link": "http://arxiv.org/pdf/1702.02380v1"
  },
  {
    "Title": "Rentable Internet of Things Infrastructure for Sensing as a Service\n  (S2aaS)",
    "Authors": "Charith Perera",
    "Published": "2018-07-13T16:50:11Z",
    "Summary": "Sensing as a Service (S2aaS) model [1] [2] is inspired by the traditional Everything as a service (XaaS) approaches [3]. It aims to better utilize the existing Internet of Things (IoT) infrastructure. S2aaS vision aims to create 'rentable infrastructure' where interested parties can gather IoT data by paying a fee for the infrastructure owners.",
    "Link": "http://arxiv.org/abs/1807.09680v1",
    "PDF Link": "http://arxiv.org/pdf/1807.09680v1"
  },
  {
    "Title": "User Empowerment in the Internet of Things",
    "Authors": "Dejan Munjin, Jean-Henry Morin",
    "Published": "2011-07-19T16:09:07Z",
    "Summary": "This paper focuses on the characteristics of two big triggers that facilitated wide user adoption of the Internet: Web 2.0 and online social networks. We detect brakes for reproduction of these events in Internet of things. To support our hypothesis we first compare the difference between the ways of use of the Internet with the future scenarios of Internet of things. We detect barriers that could slow down apparition of this kind of social events during user adoption of Internet of Things and we propose a conceptual framework to solve these problems.",
    "Link": "http://arxiv.org/abs/1107.3759v1",
    "PDF Link": "http://arxiv.org/pdf/1107.3759v1"
  },
  {
    "Title": "Review of internet of things of security threats and Challenges",
    "Authors": "Fehim Köylü, Ahmed O. Ali, Mohamud M. Hassan, Muhiadin M. Sabriye, Abdirisak Ali Osman, Ali Ammar Hilal, Qazwan Abdullah",
    "Published": "2021-07-20T06:18:37Z",
    "Summary": "The Internet of Things has received a lot of research attention. It is considered part of the Internet of the future and is made up of billions of intelligent communication. The future of the Internet will consist of heterogeneously connected devices that expand the world boundaries with physical entities and virtual components. It provides new functionality for related things. This study systematically examines the definition, architecture, essential technologies, and applications of the Internet of Things. We will introduce various definitions of the Internet of Things. Then, it will be discussed new techniques for implementing the Internet of Things and several open issues related to the Internet of Things applications will be investigated. Finally, the key challenges that need to be addressed by the research community and possible solutions to address them are investigated.",
    "Link": "http://arxiv.org/abs/2107.10733v1",
    "PDF Link": "http://arxiv.org/pdf/2107.10733v1"
  },
  {
    "Title": "Challenges and Opportunities in Securing the Industrial Internet of\n  Things",
    "Authors": "Martin Serror, Sacha Hack, Martin Henze, Marko Schuba, Klaus Wehrle",
    "Published": "2021-11-23T08:13:58Z",
    "Summary": "Given the tremendous success of the Internet of Things in interconnecting consumer devices, we observe a natural trend to likewise interconnect devices in industrial settings, referred to as Industrial Internet of Things or Industry 4.0. While this coupling of industrial components provides many benefits, it also introduces serious security challenges. Although sharing many similarities with the consumer Internet of Things, securing the Industrial Internet of Things introduces its own challenges but also opportunities, mainly resulting from a longer lifetime of components and a larger scale of networks. In this paper, we identify the unique security goals and challenges of the Industrial Internet of Things, which, unlike consumer deployments, mainly follow from safety and productivity requirements. To address these security goals and challenges, we provide a comprehensive survey of research efforts to secure the Industrial Internet of Things, discuss their applicability, and analyze their security benefits.",
    "Link": "http://arxiv.org/abs/2111.11714v1",
    "PDF Link": "http://arxiv.org/pdf/2111.11714v1"
  },
  {
    "Title": "A Study on Internet of Things based Applications",
    "Authors": "Deeksha Jain, P. Venkata Krishna, V. Saritha",
    "Published": "2012-06-18T11:20:56Z",
    "Summary": "This paper gives a detail analysis of various applications based on Internet of Thing (IoT)s. This explains about how internet of things evolved from mobile computing and ubiquitous computing. It emphasises the fact that objects are connected over the internet rather than people. The properties of Internet of Things (IOT) are product information, electronic tag, standard expressed and uploading information. It utilises the Radio Frequency Identification (RFID) technology and wireless sensor networks (WSN). IOT applications are used in domains such as healthcare, supply chain management, defence and agriculture. Lastly the paper focuses on issues involved in IOT. Though it is a boon, IOT faces certain crucial issues like privacy and security.",
    "Link": "http://arxiv.org/abs/1206.3891v1",
    "PDF Link": "http://arxiv.org/pdf/1206.3891v1"
  },
  {
    "Title": "Human Resource Development and the Internet of Things",
    "Authors": "Robert M Yawson, Daniel Woldeab, Emmanuel Osafo",
    "Published": "2021-06-22T01:11:29Z",
    "Summary": "The Internet of Things (IoT) is affecting national innovation ecosystems and the approach of organizations to innovation and how they create and capture value in everyday business activities. The Internet of Things (IoT), is disruptive, and it will change the manner in which human resources are developed and managed, calling for a new and adaptive human resource development approach. The Classical Internet communication form is human-human. The prospect of IoT is that every object will have a unique way of identification and can be addressed so that every object can be connected. The communication forms will expand from human-human to human-human, human-thing, and thing-thing. This will bring a new challenge to how Human Resource Development (HRD) is practiced. This paper provides an overview of the Internet of Things and conceptualizes the role of HRD in the age of the Internet of Things. Keywords:",
    "Link": "http://arxiv.org/abs/2107.04003v1",
    "PDF Link": "http://arxiv.org/pdf/2107.04003v1"
  },
  {
    "Title": "6G Internet of Things: A Comprehensive Survey",
    "Authors": "Dinh C. Nguyen, Ming Ding, Pubudu N. Pathirana, Aruna Seneviratne, Jun Li, Dusit Niyato, Octavia Dobre, H. Vincent Poor",
    "Published": "2021-08-11T00:19:05Z",
    "Summary": "The sixth generation (6G) wireless communication networks are envisioned to revolutionize customer services and applications via the Internet of Things (IoT) towards a future of fully intelligent and autonomous systems. In this article, we explore the emerging opportunities brought by 6G technologies in IoT networks and applications, by conducting a holistic survey on the convergence of 6G and IoT. We first shed light on some of the most fundamental 6G technologies that are expected to empower future IoT networks, including edge intelligence, reconfigurable intelligent surfaces, space-air-ground-underwater communications, Terahertz communications, massive ultra-reliable and low-latency communications, and blockchain. Particularly, compared to the other related survey papers, we provide an in-depth discussion of the roles of 6G in a wide range of prospective IoT applications via five key domains, namely Healthcare Internet of Things, Vehicular Internet of Things and Autonomous Driving, Unmanned Aerial Vehicles, Satellite Internet of Things, and Industrial Internet of Things. Finally, we highlight interesting research challenges and point out potential directions to spur further research in this promising area.",
    "Link": "http://arxiv.org/abs/2108.04973v1",
    "PDF Link": "http://arxiv.org/pdf/2108.04973v1"
  },
  {
    "Title": "On Web-based Domain-Specific Language for Internet of Things",
    "Authors": "Manfred Sneps-Sneppe, Dmitry Namiot",
    "Published": "2015-05-25T18:48:20Z",
    "Summary": "This paper discusses the challenges of the Internet of Things programming. Sensing and data gathering from the various sources are often the key elements of applications for Smart Cities. So, the effective programming models for them are very important. In this article, we discuss system software models and solutions, rather than network related aspects. In our paper, we present the web-based domain-specific language for Internet of Things applications. Our goal is to present the modern models for data processing in Internet of Things and Smart Cities applications. In our view, the use of this kind of tools should seriously reduce the time to develop new applications.",
    "Link": "http://arxiv.org/abs/1505.06713v1",
    "PDF Link": "http://arxiv.org/pdf/1505.06713v1"
  },
  {
    "Title": "Internet of Nano, Bio-Nano, Biodegradable and Ingestible Things: A\n  Survey",
    "Authors": "Seyda Senturk, Ibrahim Kok, Fatmana Senturk",
    "Published": "2022-02-24T23:05:54Z",
    "Summary": "In recent years, advances in biotechnology, nanotechnology and materials science have led to development of revolutionizing applications in Internet of Things (IoT). In particular, the interconnection of nanomaterials, nanoimplants and nanobiosensors with existing IoT networks have inspired the concepts of Internet of Nano Things (IoNT), Internet of Bio-Nano Things (IoBNT), Internet of Biodegradable Things (IoBDT) and Internet of Ingestible Things (IoIT). To date, although there are several survey papers that addressed these concepts separately, there is no current survey covering all studies in IoNT, IoBNT, IoBDT and IoIT. Therefore, in this paper, we provide a complete overview of all recent work in these four areas. Furthermore, we emphasize the research challenges, potential applications, and open research areas.",
    "Link": "http://arxiv.org/abs/2202.12409v1",
    "PDF Link": "http://arxiv.org/pdf/2202.12409v1"
  },
  {
    "Title": "The Internet of Things: Perspectives on Security from RFID and WSN",
    "Authors": "Ayush Shah, Ambar Pal, H. B. Acharya",
    "Published": "2016-04-03T19:20:16Z",
    "Summary": "A massive current research effort focuses on combining pre-existing 'Intranets' of Things into one Internet of Things. However, this unification is not a panacea; it will expose new attack surfaces and vectors, just as it enables new applications. We therefore urgently need a model of security in the Internet of Things. In this regard, we note that IoT descends directly from pre-existing research (in embedded Internet and pervasive intelligence), so there exist several bodies of related work: security in RFID, sensor networks, cyber-physical systems, and so on. In this paper, we survey the existing literature on RFID and WSN security, as a step to compiling all known attacks and defenses relevant to the Internet of Things.",
    "Link": "http://arxiv.org/abs/1604.00389v1",
    "PDF Link": "http://arxiv.org/pdf/1604.00389v1"
  },
  {
    "Title": "Turing Test for the Internet of Things",
    "Authors": "Neil Rubens",
    "Published": "2014-12-11T09:49:07Z",
    "Summary": "How smart is your kettle? How smart are things in your kitchen, your house, your neighborhood, on the internet? With the advent of Internet of Things, and the move of making devices `smart' by utilizing AI, a natural question arrises, how can we evaluate the progress. The standard way of evaluating AI is through the Turing Test. While Turing Test was designed for AI; the device that it was tailored to was a computer. Applying the test to variety of devices that constitute Internet of Things poses a number of challenges which could be addressed through a number of adaptations.",
    "Link": "http://arxiv.org/abs/1412.3802v1",
    "PDF Link": "http://arxiv.org/pdf/1412.3802v1"
  },
  {
    "Title": "Privacy Preservation Technologies in Internet of Things",
    "Authors": "Jaydip Sen",
    "Published": "2010-12-10T04:05:49Z",
    "Summary": "Since the beginning of the Internet thirty years ago, we have witnessed a number of changes in the application of communication technologies. Today, the Internet can be described to a large extent as a ubiquitous infrastructure that is always accessible. After the era of connecting places and connecting people, the Internet of the future will also connect things. The idea behind the resulting Internet of Things is to seamlessly gather and use information about objects of the real world during their entire lifecycle. In this paper, we consider different approaches to technological protection of user data privacy in the world of Internet of Things. In particular,we consider what kind of security problems are being faced and what level of protection can be provided by applying approaches based on secure multi-party computations.",
    "Link": "http://arxiv.org/abs/1012.2177v3",
    "PDF Link": "http://arxiv.org/pdf/1012.2177v3"
  },
  {
    "Title": "User-driven Privacy Enforcement for Cloud-based Services in the Internet\n  of Things",
    "Authors": "Martin Henze, Lars Hermerschmidt, Daniel Kerpen, Roger Häußling, Bernhard Rumpe, Klaus Wehrle",
    "Published": "2014-12-09T14:02:52Z",
    "Summary": "Internet of Things devices are envisioned to penetrate essentially all aspects of life, including homes and urbanspaces, in use cases such as health care, assisted living, and smart cities. One often proposed solution for dealing with the massive amount of data collected by these devices and offering services on top of them is the federation of the Internet of Things and cloud computing. However, user acceptance of such systems is a critical factor that hinders the adoption of this promising approach due to severe privacy concerns. We present UPECSI, an approach for user-driven privacy enforcement for cloud-based services in the Internet of Things to address this critical factor. UPECSI enables enforcement of all privacy requirements of the user once her sensitive data leaves the border of her network, provides a novel approach for the integration of privacy functionality into the development process of cloud-based services, and offers the user an adaptable and transparent configuration of her privacy requirements. Hence, UPECSI demonstrates an approach for realizing user-accepted cloud services in the Internet of Things.",
    "Link": "http://arxiv.org/abs/1412.3325v1",
    "PDF Link": "http://arxiv.org/pdf/1412.3325v1"
  },
  {
    "Title": "Low power communication signal enhancement method of Internet of things\n  based on nonlocal mean denoising",
    "Authors": "Mingchuan Tian, Jizheng Liu",
    "Published": "2022-05-14T10:06:21Z",
    "Summary": "In order to improve the transmission effect of low-power communication signal of Internet of things and compress the enhancement time of low-power communication signal, this paper designs a low-power communication signal enhancement method of Internet of things based on nonlocal mean denoising. Firstly, the residual of one-dimensional communication layer is pre processed by convolution core to obtain the residual of one-dimensional communication layer; Then, according to the two classification recognition method, the noise reduction signal feature recognition of the low-power communication signal of the Internet of things is realized, the non local mean noise reduction algorithm is used to remove the low-power communication signal of the Internet of things, and the weight value between similar blocks is calculated according to the European distance method. Finally, the low-power communication signal enhancement of the Internet of things is realized by the non local mean value denoising method. The experimental results show that the communication signal enhancement time overhead of this method is low, which is always less than 2.6s. The lowest bit error rate after signal enhancement is about 1%, and the signal-to-noise ratio is up to 18 dB, which shows that this method can achieve signal enhancement.",
    "Link": "http://arxiv.org/abs/2205.10323v1",
    "PDF Link": "http://arxiv.org/pdf/2205.10323v1"
  },
  {
    "Title": "Cells in the Internet of Things",
    "Authors": "Ayush Shah, H. B. Acharya, Ambar Pal",
    "Published": "2015-10-27T11:09:17Z",
    "Summary": "The Internet of Things combines various earlier areas of research. As a result, research on the subject is still organized around these pre-existing areas: distributed computing with services and objects, networks (usually combining 6lowpan with Zigbee etc. for the last-hop), artificial intelligence and semantic web, and human-computer interaction. We are yet to create a unified model that covers all these perspectives - domain, device, service, agent, etc. In this paper, we propose the concept of cells as units of structure and context in the Internet of things. This allows us to have a unified vocabulary to refer to single entities (whether dumb motes, intelligent spimes, or virtual services), intranets of things, and finally the complete Internet of things. The question that naturally follows, is what criteria we choose to demarcate boundaries; we suggest various possible answers to this question. We also mention how this concept ties into the existing visions and protocols, and suggest how it may be used as the foundation of a formal model.",
    "Link": "http://arxiv.org/abs/1510.07861v1",
    "PDF Link": "http://arxiv.org/pdf/1510.07861v1"
  },
  {
    "Title": "Economic viability and Future Impact of Internet of Things in India: An\n  Inevitable wave",
    "Authors": "Sharul Agrawal, Himanshu S Mazumdar",
    "Published": "2015-12-02T10:51:21Z",
    "Summary": "The Internet of things , sometimes referred as Internet of objects can be stated as an environment in which any physical things or objects are assiThis paper studies the evolution of internet usage and classifies the impact areas where internet will go beyond personal communication or knowledge interface but it will provide communication and knowledge base support to numerous gadgets and systems around us",
    "Link": "http://arxiv.org/abs/1601.04363v2",
    "PDF Link": "http://arxiv.org/pdf/1601.04363v2"
  },
  {
    "Title": "Wireless Sensors Networks for Internet of Things",
    "Authors": "Nacer Khalil, Mohamed Riduan Abid, Driss Benhaddou, Michael Gerndt",
    "Published": "2016-06-27T18:46:36Z",
    "Summary": "The Internet is smoothly migrating from an Internet of people towards an Internet of Things (IoT). By 2020, it is expected to have 50 billion things connected to the Internet. However, such a migration induces a strong level of complexity when handling interoperability between the heterogeneous Inter- net things, e.g., RFIDs (Radio Frequency Identification), mobile handheld devices, and wireless sensors. In this context, a couple of standards have been already set, e.g., IPv6, 6LoWPAN (IPv6 over Low power Wireless Personal Area Networks), and M2M (Machine to Machine communications). In this paper, we focus on the integration of wireless sensor networks into IoT, and shed further light on the subtleties of such integration. We present a real-world test bed deployment where wireless sensors are used to control electrical appliances in a smart building. Encountered problems are highlighted and suitable solutions are presented.",
    "Link": "http://arxiv.org/abs/1606.08407v1",
    "PDF Link": "http://arxiv.org/pdf/1606.08407v1"
  },
  {
    "Title": "A Middleware for the Internet of Things",
    "Authors": "Mahmoud Elkhodr, Seyed Shahrestani, Hon Cheung",
    "Published": "2016-04-17T03:36:42Z",
    "Summary": "The Internet of Things (IoT) connects everyday objects including a vast array of sensors, actuators, and smart devices, referred to as things to the Internet, in an intelligent and pervasive fashion. This connectivity gives rise to the possibility of using the tracking capabilities of things to impinge on the location privacy of users. Most of the existing management and location privacy protection solutions do not consider the low-cost and low-power requirements of things, or, they do not account for the heterogeneity, scalability, or autonomy of communications supported in the IoT. Moreover, these traditional solutions do not consider the case where a user wishes to control the granularity of the disclosed information based on the context of their use (e.g. based on the time or the current location of the user). To fill this gap, a middleware, referred to as the Internet of Things Management Platform (IoT-MP) is proposed in this paper.",
    "Link": "http://arxiv.org/abs/1604.04823v1",
    "PDF Link": "http://arxiv.org/pdf/1604.04823v1"
  },
  {
    "Title": "A Review on Security and Privacy of Internet of Medical Things",
    "Authors": "Mohan Krishna Kagita, Navod Thilakarathne, Thippa Reddy Gadekallu, Praveen Kumar Reddy Maddikunta",
    "Published": "2020-09-11T12:31:40Z",
    "Summary": "The Internet of Medical Things (IoMT) are increasing the accuracy, reliability, and the production capability of electronic devices by playing a very important part in the industry of healthcare. The available medical resources and services related to healthcare are working to get an interconnection with each other by the digital healthcare system by the contribution of the researchers. Sensors, wearable devices, medical devices, and clinical devices are all connected to form an ecosystem of the Internet of Medical Things. The different applications of healthcare are enabled by the Internet of Medical Things to reduce the healthcare costs, to attend the medical responses on time and it also helps in increasing the quality of the medical treatment. The healthcare industry is transformed by the Internet of Medical Things as it delivers targeted and personalized medical care and it also seamlessly enables the communication of medical data. Devices used in the medical field and their application are connected to the system of healthcare of Information technology with the help of the digital world.",
    "Link": "http://arxiv.org/abs/2009.05394v1",
    "PDF Link": "http://arxiv.org/pdf/2009.05394v1"
  },
  {
    "Title": "Social Internet of Things: Architectural Approaches and Challenges",
    "Authors": "Juan Ochoa-Zambrano, Juan Garbajosa",
    "Published": "2020-02-11T17:41:22Z",
    "Summary": "Social Internet of Things (SIoT) takes a step forward over the traditional Internet of Things (IoT), introducing a new paradigm that combines the concepts of social networks with the IoT, to obtain the benefits of both worlds, as in the case of the Social Internet of Vehicles. With the emergence of the Social Internet of Things, new challenges also arise that need to be analyzed in depth. In this article, the key challenges around the software architecture of the various SIoT system described in the literature are analyzed. One of the conclusions is that SIoT is still at an early stage of development, and therefore, SIoT systems architecture will be concerned by this fact. Challenging quality attributes specific for SIoT include scalability, navigability and trust",
    "Link": "http://arxiv.org/abs/2002.04566v1",
    "PDF Link": "http://arxiv.org/pdf/2002.04566v1"
  },
  {
    "Title": "Can Blockchain Protect Internet-of-Things?",
    "Authors": "Hiroshi Watanabe",
    "Published": "2018-07-17T11:34:44Z",
    "Summary": "In the Internet-of-Things, the number of connected devices is expected to be extremely huge, i.e., more than a couple of ten billion. It is however well-known that the security for the Internet-of-Things is still open problem. In particular, it is difficult to certify the identification of connected devices and to prevent the illegal spoofing. It is because the conventional security technologies have advanced for mainly protecting logical network and not for physical network like the Internet-of-Things. In order to protect the Internet-of-Things with advanced security technologies, we propose a new concept (datachain layer) which is a well-designed combination of physical chip identification and blockchain. With a proposed solution of the physical chip identification, the physical addresses of connected devices are uniquely connected to the logical addresses to be protected by blockchain.",
    "Link": "http://arxiv.org/abs/1807.06357v2",
    "PDF Link": "http://arxiv.org/pdf/1807.06357v2"
  },
  {
    "Title": "Threat Analysis of Industrial Internet of Things Devices",
    "Authors": "Simon Liebl, Leah Lathrop, Ulrich Raithel, Matthias Söllner, Andreas Aßmuth",
    "Published": "2024-05-25T17:45:12Z",
    "Summary": "As part of the Internet of Things, industrial devices are now also connected to cloud services. However, the connection to the Internet increases the risks for Industrial Control Systems. Therefore, a threat analysis is essential for these devices. In this paper, we examine Industrial Internet of Things devices, identify and rank different sources of threats and describe common threats and vulnerabilities. Finally, we recommend a procedure to carry out a threat analysis on these devices.",
    "Link": "http://arxiv.org/abs/2405.16314v1",
    "PDF Link": "http://arxiv.org/pdf/2405.16314v1"
  },
  {
    "Title": "From Internet of Things to Internet of Data Apps",
    "Authors": "Silvery Fu, Sylvia Ratnasamy",
    "Published": "2023-09-08T18:26:05Z",
    "Summary": "We introduce the Internet of Data Apps (IoDA), representing the next natural progression of the Internet, Big Data, AI, and the Internet of Things. Despite advancements in these fields, the full potential of universal data access - the capability to seamlessly consume and contribute data via data applications - remains stifled by organizational and technological silos. To address these constraints, we propose the designs of an IoDA layer borrowing inspirations from the standard Internet protocols. This layer facilitates the interconnection of data applications across different devices and domains. This short paper serves as an invitation to dialogue over this proposal.",
    "Link": "http://arxiv.org/abs/2309.04546v1",
    "PDF Link": "http://arxiv.org/pdf/2309.04546v1"
  },
  {
    "Title": "When Distributed Ledger Technology meets Internet of Things -- Benefits\n  and Challenges",
    "Authors": "Pavlos Charalampidis, Alexandros Fragkiadakis",
    "Published": "2020-08-28T10:26:59Z",
    "Summary": "There is a growing interest from both the academia and industry to employ distributed ledger technology in the Internet-of-Things domain for addressing security-related and performance challenges. Distributed ledger technology enables non-trusted entities to communicate and reach consensus in a fully distributed manner through a cryptographically secure and immutable ledger. However, significant challenges arise mainly related to transaction processing speed and user privacy. This work explores the interplay between Internet-of-Things and distributed ledger technology, analysing the fundamental characteristics of this technology and discussing the related benefits and challenges.",
    "Link": "http://arxiv.org/abs/2008.12569v1",
    "PDF Link": "http://arxiv.org/pdf/2008.12569v1"
  },
  {
    "Title": "Wearable Internet of Things for Personalized Healthcare Study of Trends\n  and Latent Research",
    "Authors": "Samiya Khan, Mansaf Alam",
    "Published": "2020-04-20T14:04:08Z",
    "Summary": "In this age of heterogeneous systems, diverse technologies are integrated to create application-specific solutions. The recent upsurge in acceptance of technologies such as cloud computing and ubiquitous Internet has cleared the path for Internet of Things (IoT). Moreover, the increasing Internet penetration with the rising use of mobile devices has inspired an era of technology that allows interfacing of physical objects and connecting them to Internet for developing applications serving a wide range of purposes. Recent developments in the area of wearable devices has led to the creation of another segment in IoT, which can be conveniently referred to as Wearable Internet of Things (WIoT). Research in this area promises to personalize healthcare in previously unimaginable ways by allowing individual tracking of wellness and health information. This chapter shall cover the different facets of Wearable Internet of Things (WIoT) and ways in which it is a key driving technology behind the concept of personalized healthcare. It shall discuss the theoretical aspects of WIoT, focusing on functionality, design and applicability. Moreover, it shall also elaborate on the role of wearable sensors, big data and cloud computing as enabling technologies for WIoT.",
    "Link": "http://arxiv.org/abs/2005.06958v1",
    "PDF Link": "http://arxiv.org/pdf/2005.06958v1"
  },
  {
    "Title": "Context-aware Dynamic Discovery and Configuration of 'Things' in Smart\n  Environments",
    "Authors": "Charith Perera, Prem Jayaraman, Arkady Zaslavsky, Peter Christen, Dimitrios Georgakopoulos",
    "Published": "2013-11-09T04:01:30Z",
    "Summary": "The Internet of Things (IoT) is a dynamic global information network consisting of Internet-connected objects, such as RFIDs, sensors, actuators, as well as other instruments and smart appliances that are becoming an integral component of the future Internet. Currently, such Internet-connected objects or `things' outnumber both people and computers connected to the Internet and their population is expected to grow to 50 billion in the next 5 to 10 years. To be able to develop IoT applications, such `things' must become dynamically integrated into emerging information networks supported by architecturally scalable and economically feasible Internet service delivery models, such as cloud computing. Achieving such integration through discovery and configuration of `things' is a challenging task. Towards this end, we propose a Context-Aware Dynamic Discovery of {Things} (CADDOT) model. We have developed a tool SmartLink, that is capable of discovering sensors deployed in a particular location despite their heterogeneity. SmartLink helps to establish the direct communication between sensor hardware and cloud-based IoT middleware platforms. We address the challenge of heterogeneity using a plug in architecture. Our prototype tool is developed on an Android platform. Further, we employ the Global Sensor Network (GSN) as the IoT middleware for the proof of concept validation. The significance of the proposed solution is validated using a test-bed that comprises 52 Arduino-based Libelium sensors.",
    "Link": "http://arxiv.org/abs/1311.2134v1",
    "PDF Link": "http://arxiv.org/pdf/1311.2134v1"
  },
  {
    "Title": "Achieving Ethical Algorithmic Behaviour in the Internet-of-Things: a\n  Review",
    "Authors": "Seng W. Loke",
    "Published": "2019-10-22T21:34:19Z",
    "Summary": "The Internet-of-Things is emerging as a vast inter-connected space of devices and things surrounding people, many of which are increasingly capable of autonomous action, from automatically sending data to cloud servers for analysis, changing the behaviour of smart objects, to changing the physical environment. A wide range of ethical concerns has arisen in their usage and development in recent years. Such concerns are exacerbated by the increasing autonomy given to connected things. This paper reviews, via examples, the landscape of ethical issues, and some recent approaches to address these issues, concerning connected things behaving autonomously, as part of the Internet-of-Things. We consider ethical issues in relation to device operations and accompanying algorithms. Examples of concerns include unsecured consumer devices, data collection with health related Internet-of-Things, hackable vehicles and behaviour of autonomous vehicles in dilemma situations, accountability with Internet-of-Things systems, algorithmic bias, uncontrolled cooperation among things, and automation affecting user choice and control. Current ideas towards addressing a range of ethical concerns are reviewed and compared, including programming ethical behaviour, whitebox algorithms, blackbox validation, algorithmic social contracts, enveloping IoT systems, and guidelines and code of ethics for IoT developers - a suggestion from the analysis is that a multi-pronged approach could be useful, based on the context of operation and deployment.",
    "Link": "http://arxiv.org/abs/1910.10241v1",
    "PDF Link": "http://arxiv.org/pdf/1910.10241v1"
  },
  {
    "Title": "A Review on Internet of Things (IoT), Internet of Everything (IoE) and\n  Internet of Nano Things (IoNT)",
    "Authors": "Mahdi H. Miraz, Maaruf Ali, Peter S. Excell, Rich Picking",
    "Published": "2017-09-07T14:45:26Z",
    "Summary": "The current prominence and future promises of the Internet of Things (IoT), Internet of Everything (IoE) and Internet of Nano Things (IoNT) are extensively reviewed and a summary survey report is presented. The analysis clearly distinguishes between IoT and IoE which are wrongly considered to be the same by many people. Upon examining the current advancement in the fields of IoT, IoE and IoNT, the paper presents scenarios for the possible future expansion of their applications.",
    "Link": "http://arxiv.org/abs/1709.10470v1",
    "PDF Link": "http://arxiv.org/pdf/1709.10470v1"
  },
  {
    "Title": "Towards Robotic Things in Society",
    "Authors": "Seng W. Loke",
    "Published": "2019-10-22T22:17:17Z",
    "Summary": "Emerging are so-called smart things embedded with computational, sensing, networking and actuation capabilities, from smart bins to smart park benches, as well as the proliferation of autonomous vehicles and robots in an increasingly wide range of applications. This is not only an increased in automation affecting and hopefully improving daily life, but also calls for thinking about what a society saturated with such robotic things (i.e., smart things and robots) might look like. This paper discusses five aspects of a vision of Internet connected robotic things (or Internet of Robotic Things (IoRT)) occupying and operating in public spaces, from streets, parks to shopping malls. We discuss, highlighting issues, with the notion of an entourage of drones and robots accompanying people in public places, the idea of creating environments or envelopes suitable for robot function, the idea of societies of robotic things, and governance for robotic things in public spaces.",
    "Link": "http://arxiv.org/abs/1910.10253v1",
    "PDF Link": "http://arxiv.org/pdf/1910.10253v1"
  },
  {
    "Title": "IoT Applications in Urban Sustainability",
    "Authors": "Samiya Khan, Mohammad Moazum Wani, Mansaf Alam",
    "Published": "2020-07-23T12:50:33Z",
    "Summary": "Internet of Things is one of the driving technologies behind the concept of Smart Cities and is capable of playing a significant role in facilitating urban sustainable development. This chapter explores the relationship between three core concepts namely Smart Cities, Internet of Things and Sustainability; thereby identifying the challenges and opportunities that exist in the synergistic use of Internet of Things for sustainability, in the Smart Cities context. Moreover, this chapter also presents some of the existing use cases that apply Internet of Things for urban sustainable development, also presenting the vision for these applications as they continue to evolve in and adapt to the real world scenario. It is because of the interdisciplinary nature of these applications that a clear comprehension of the associated challenges becomes quintessential. Study of challenges and opportunities in this area shall facilitate collaboration between different sectors of urban planning and optimize the utilization of Internet of Things for sustainability.",
    "Link": "http://arxiv.org/abs/2008.10656v1",
    "PDF Link": "http://arxiv.org/pdf/2008.10656v1"
  },
  {
    "Title": "Comment on Chen et al.'s Authentication Protocol for Internet of Health\n  Things",
    "Authors": "Iman Jafarian, Siavash Khorsandi",
    "Published": "2024-06-24T17:16:29Z",
    "Summary": "The Internet of Medical Things has revolutionized the healthcare industry, enabling the seamless integration of connected medical devices and wearable sensors to enhance patient care and optimize healthcare services. However, the rapid adoption of the Internet of Medical Things also introduces significant security challenges that must be effectively addressed to preserve patient privacy, protect sensitive medical data, and ensure the overall reliability and safety of Internet of Medical Things systems. In this context, a key agreement protocol is used to securely establish shared cryptographic keys between interconnected medical devices and the central system, ensuring confidential and authenticated communication. Recently Chen et al. proposed a lightweight authentication and key agreement protocol for the Internet of health things. In this article, we provide a descriptive analysis of their proposed scheme and prove that Chen et al.'s scheme is vulnerable to Known session-specific temporary information attacks and stolen verifier attacks.",
    "Link": "http://arxiv.org/abs/2406.16804v1",
    "PDF Link": "http://arxiv.org/pdf/2406.16804v1"
  },
  {
    "Title": "Addressing Security and Privacy Challenges in Internet of Things",
    "Authors": "Arsalan Mosenia",
    "Published": "2018-07-18T01:01:45Z",
    "Summary": "Internet of Things (IoT), also referred to as the Internet of Objects, is envisioned as a holistic and transformative approach for providing numerous services. The rapid development of various communication protocols and miniaturization of transceivers along with recent advances in sensing technologies offer the opportunity to transform isolated devices into communicating smart things. Smart things, that can sense, store, and even process electrical, thermal, optical, chemical, and other signals to extract user-/environment-related information, have enabled services only limited by human imagination.   Despite picturesque promises of IoT-enabled systems, the integration of smart things into the standard Internet introduces several security challenges because the majority of Internet technologies, communication protocols, and sensors were not designed to support IoT. Several recent research studies have demonstrated that launching security/privacy attacks against IoT-enabled systems, in particular wearable medical sensor (WMS)-based systems, may lead to catastrophic situations and life-threatening conditions. Therefore, security threats and privacy concerns in the IoT domain need to be proactively studied and aggressively addressed. In this thesis, we tackle several domain-specific security/privacy challenges associated with IoT-enabled systems.",
    "Link": "http://arxiv.org/abs/1807.06724v1",
    "PDF Link": "http://arxiv.org/pdf/1807.06724v1"
  },
  {
    "Title": "Sense-Deliberate-Act Cognitive Agents for Sense-Compute-Control\n  Applications in the Internet of Things & Services",
    "Authors": "Armin Moin",
    "Published": "2020-09-22T15:52:21Z",
    "Summary": "In this paper, we advocate Agent-Oriented Software Engi-neering (AOSE) through employing Belief-Desire-Intention (BDI) intel-ligent agents for developing Sense-Compute-Control (SCC) applications in the Internet of Things and Services (IoTS). We argue that not only the agent paradigm, in general, but also cognitive BDI agents with sense-deliberate-act cycle, in particular, fit very well to the nature of SCC applications in the IoTS. However, considering the highly constrained heterogeneous devices that are prevalent in the IoTS, existing BDI agent frameworks, even those especially created for Wireless Sensor Networks (WSNs), do not work. We elaborate on the challenges and propose pos-sible approaches to address them.",
    "Link": "http://arxiv.org/abs/2009.10638v1",
    "PDF Link": "http://arxiv.org/pdf/2009.10638v1"
  },
  {
    "Title": "A Conceptual Paper on SERVQUAL-Framework for Assessing Quality of\n  Internet of Things (IoT) Services",
    "Authors": "Sheikh Muhammad Hizam, Waqas Ahmed",
    "Published": "2020-01-06T12:13:53Z",
    "Summary": "Service quality possesses the vital prominence in usability of innovative products and services. As technological innovation has made the life synchronized and effective, Internet of Things (IoT) is matter of discussion everywhere. From users' perspective, IoT services are always embraced by various system characteristics of security and performance. A service quality model can better present the preference of such technology customers. the study intends to project theoretical model of service quality for internet of things (IoT). Based on the existing models of service quality and the literature in internet of things, a framework is proposed to conceptualize and measure service quality for internet of things.This study established the IoT-Servqual model with four dimensions (i.e., Privacy, Functionality, Efficiency, and Tangibility) of multiple service quality models. These dimensions are essential and inclined towards the users' leaning of IoT Services. This paper contributes to research on internet of things services by development of a comprehensive framework for customers' quality apprehension. This model will previse the expression of information secrecy of users related with internet of things (IoT). This research will advance understanding of service quality in modern day technology and assist firms to devise the fruitful service structure.",
    "Link": "http://arxiv.org/abs/2001.01840v1",
    "PDF Link": "http://arxiv.org/pdf/2001.01840v1"
  },
  {
    "Title": "Challenges and Characteristics of Intelligent Autonomy for Internet of\n  Battle Things in Highly Adversarial Environments",
    "Authors": "Alexander Kott",
    "Published": "2018-03-20T22:15:14Z",
    "Summary": "Numerous, artificially intelligent, networked things will populate the battlefield of the future, operating in close collaboration with human warfighters, and fighting as teams in highly adversarial environments. This paper explores the characteristics, capabilities and intelligence required of such a network of intelligent things and humans - Internet of Battle Things (IOBT). It will experience unique challenges that are not yet well addressed by the current generation of AI and machine learning.",
    "Link": "http://arxiv.org/abs/1803.11256v2",
    "PDF Link": "http://arxiv.org/pdf/1803.11256v2"
  },
  {
    "Title": "Intelligent Autonomous Things on the Battlefield",
    "Authors": "Alexander Kott, Ethan Stump",
    "Published": "2019-02-26T17:59:55Z",
    "Summary": "Numerous, artificially intelligent, networked things will populate the battlefield of the future, operating in close collaboration with human warfighters, and fighting as teams in highly adversarial environments. This chapter explores the characteristics, capabilities and intelli-gence required of such a network of intelligent things and humans - Internet of Battle Things (IOBT). The IOBT will experience unique challenges that are not yet well addressed by the current generation of AI and machine learning.",
    "Link": "http://arxiv.org/abs/1902.10086v1",
    "PDF Link": "http://arxiv.org/pdf/1902.10086v1"
  },
  {
    "Title": "Semantic Reasoning for Context-aware Internet of Things Applications",
    "Authors": "Altti Ilari Maarala, Xiang Su, Jukka Riekki",
    "Published": "2016-04-28T08:17:56Z",
    "Summary": "Advances in ICT are bringing into reality the vision of a large number of uniquely identifiable, interconnected objects and things that gather information from diverse physical environments and deliver the information to a variety of innovative applications and services. These sensing objects and things form the Internet of Things (IoT) that can improve energy and cost efficiency and automation in many different industry fields such as transportation and logistics, health care and manufacturing, and facilitate our everyday lives as well. IoT applications rely on real-time context data and allow sending information for driving the behaviors of users in intelligent environments.",
    "Link": "http://arxiv.org/abs/1604.08340v1",
    "PDF Link": "http://arxiv.org/pdf/1604.08340v1"
  },
  {
    "Title": "HMIoT: A New Healthcare Model Based on Internet of Things",
    "Authors": "Mohsen Yaghoubi Suraki, Morteza Yaghoubi Suraki, Leila SourakiAzad",
    "Published": "2015-07-28T20:18:18Z",
    "Summary": "In recent century, with developing of equipment, using of the internet and things connected to the internet is growing. Therefore, the need for informing in the process of expanding the scope of its application is very necessary and important. These days, using intelligent and autonomous devices in our daily lives has become commonplace and the Internet is the most important part of the relationship between these tools and even at close distances also. Things connected to the Internet that are currently in use and can be inclusive of all the sciences as a step to develop and coordinate of them. In this paper we investigate application and using of Internet of things from the perspective of various sciences. We show that how this phenomenon can influence on future health of people.",
    "Link": "http://arxiv.org/abs/1507.08569v1",
    "PDF Link": "http://arxiv.org/pdf/1507.08569v1"
  },
  {
    "Title": "Internet of Nano-Things, Things and Everything: Future Growth Trends",
    "Authors": "Mahdi H. Miraz, Maaruf Ali, Peter S. Excell, Richard Picking",
    "Published": "2018-08-28T02:53:17Z",
    "Summary": "The current statuses and future promises of the Internet of Things (IoT), Internet of Everything (IoE) and Internet of Nano-Things (IoNT) are extensively reviewed and a summarized survey is presented. The analysis clearly distinguishes between IoT and IoE, which are wrongly considered to be the same by many commentators. After evaluating the current trends of advancement in the fields of IoT, IoE and IoNT, this paper identifies the 21 most significant current and future challenges as well as scenarios for the possible future expansion of their applications. Despite possible negative aspects of these developments, there are grounds for general optimism about the coming technologies. Certainly, many tedious tasks can be taken over by IoT devices. However, the dangers of criminal and other nefarious activities, plus those of hardware and software errors, pose major challenges that are a priority for further research. Major specific priority issues for research are identified.",
    "Link": "http://arxiv.org/abs/1808.09869v1",
    "PDF Link": "http://arxiv.org/pdf/1808.09869v1"
  },
  {
    "Title": "Survey of Security and Privacy Issues of Internet of Things",
    "Authors": "Tuhin Borgohain, Uday Kumar, Sugata Sanyal",
    "Published": "2015-01-09T17:58:18Z",
    "Summary": "This paper is a general survey of all the security issues existing in the Internet of Things (IoT) along with an analysis of the privacy issues that an end-user may face as a consequence of the spread of IoT. The majority of the survey is focused on the security loopholes arising out of the information exchange technologies used in Internet of Things. No countermeasure to the security drawbacks has been analyzed in the paper.",
    "Link": "http://arxiv.org/abs/1501.02211v1",
    "PDF Link": "http://arxiv.org/pdf/1501.02211v1"
  },
  {
    "Title": "Internet of Things: Concept, Building blocks, Applications and\n  Challenges",
    "Authors": "Riad Abdmeziem, Djamel Tandjaoui",
    "Published": "2014-01-02T15:37:15Z",
    "Summary": "Internet of things (IoT) constitutes one of the most important technology that has the potential to affect deeply our way of life, after mobile phones and Internet. The basic idea is that every objet that is around us will be part of the network (Internet), interacting to reach a common goal. In another word, the Internet of Things concept aims to link the physical world to the digital one. Technology advances along with popular demand will foster the wide spread deployement of IoT's services, it would radically transform our corporations, communities, and personal spheres. In this survey, we aim to provide the reader with a broad overview of the Internet of things concept, its building blocks, its applications along with its challenges.",
    "Link": "http://arxiv.org/abs/1401.6877v1",
    "PDF Link": "http://arxiv.org/pdf/1401.6877v1"
  },
  {
    "Title": "Development of Internet of Things, Augmented Reality and 5G technologies\n  (review)",
    "Authors": "A. S. Smirnov, A. V. Tumialis, K. S. Golokhvast",
    "Published": "2019-02-21T12:47:51Z",
    "Summary": "Just as the emergence of personal computers and smartphones has changed the life of modern society, the Internet of Things, augmented reality and ultra-fast and reliable telecommunications networks of the new generation, by combining the physical objects of the real world with the ever-increasing computing power and intelligence of cyberspace, will make the next big revolution in all spheres of human activity. Keywords: Internet of Things, 5G, augmented reality.",
    "Link": "http://arxiv.org/abs/1902.08008v1",
    "PDF Link": "http://arxiv.org/pdf/1902.08008v1"
  },
  {
    "Title": "Improving the quality of healthcare through Internet of Things",
    "Authors": "Cornel Turcu, Cristina Turcu",
    "Published": "2019-03-12T21:20:48Z",
    "Summary": "This paper attempts to outline how the adoption of Internet of Things (IoT) in healthcare can create real economic value and improve the patient experience. Thus, getting the maximum benefits requires understanding both the IoT paradigm and the enabling technologies, and how IoT can be applied in the field of healthcare. We will mention some open challenging issues to be addressed by the research community, and not only. Besides the real barriers in adopting the Internet of Things, there are some advantages regard collecting and processing patient data, and monitoring the daily health states of individuals, just to name a few. These aspects could revolutionize the healthcare industry.",
    "Link": "http://arxiv.org/abs/1903.05221v1",
    "PDF Link": "http://arxiv.org/pdf/1903.05221v1"
  },
  {
    "Title": "A Novel Method for Developing Robotics via Artificial Intelligence and\n  Internet of Things",
    "Authors": "Aadhityan A",
    "Published": "2014-05-12T21:27:42Z",
    "Summary": "This paper describe about a new methodology for developing and improving the robotics field via artificial intelligence and internet of things. Now a day, we can say Artificial Intelligence take the world into robotics. Almost all industries use robots for lot of works. They are use co-operative robots to make different kind of works. But there was some problem to make robot for multi tasks. So there was a necessary new methodology to made multi tasking robots. It will be done only by artificial intelligence and internet of things.",
    "Link": "http://arxiv.org/abs/1405.3939v1",
    "PDF Link": "http://arxiv.org/pdf/1405.3939v1"
  },
  {
    "Title": "Towards a Practical Architecture for India Centric Internet of Things",
    "Authors": "Prasant Misra, Yogesh Simmhan, Jay Warrior",
    "Published": "2014-07-02T01:05:33Z",
    "Summary": "An effective architecture for the Internet of Things (IoT), particularly for an emerging nation like India with limited technology penetration at the national scale, should be based on tangible technology advances in the present, practical application scenarios of social and entrepreneurial value, and ubiquitous capabilities that make the realization of IoT affordable and sustainable. Humans, data, communication and devices play key roles in the IoT ecosystem that we perceive. In a push towards this sustainable and practical IoT Architecture for India, we synthesize ten design paradigms to consider.",
    "Link": "http://arxiv.org/abs/1407.0434v2",
    "PDF Link": "http://arxiv.org/pdf/1407.0434v2"
  },
  {
    "Title": "Unveiling Contextual Similarity of Things via Mining Human-Thing\n  Interactions in the Internet of Things",
    "Authors": "Lina Yao, Quan Z. Sheng, Anne H. H. Ngu, Xue Li, Boualem Benatallah",
    "Published": "2015-12-24T13:47:27Z",
    "Summary": "With recent advances in radio-frequency identification (RFID), wireless sensor networks, and Web services, physical things are becoming an integral part of the emerging ubiquitous Web. Finding correlations of ubiquitous things is a crucial prerequisite for many important applications such as things search, discovery, classification, recommendation, and composition. This article presents DisCor-T, a novel graph-based method for discovering underlying connections of things via mining the rich content embodied in human-thing interactions in terms of user, temporal and spatial information. We model these various information using two graphs, namely spatio-temporal graph and social graph. Then, random walk with restart (RWR) is applied to find proximities among things, and a relational graph of things (RGT) indicating implicit correlations of things is learned. The correlation analysis lays a solid foundation contributing to improved effectiveness in things management. To demonstrate the utility, we develop a flexible feature-based classification framework on top of RGT and perform a systematic case study. Our evaluation exhibits the strength and feasibility of the proposed approach.",
    "Link": "http://arxiv.org/abs/1512.08493v3",
    "PDF Link": "http://arxiv.org/pdf/1512.08493v3"
  },
  {
    "Title": "The Use of Agricultural Robots in Orchard Management",
    "Authors": "Qin Zhang, Manoj Karkee, Amy Tabb",
    "Published": "2019-07-30T17:56:17Z",
    "Summary": "Book chapter that summarizes recent research on agricultural robotics in orchard management, including Robotic pruning, Robotic thinning, Robotic spraying, Robotic harvesting, Robotic fruit transportation, and future trends.",
    "Link": "http://arxiv.org/abs/1907.13114v1",
    "PDF Link": "http://arxiv.org/pdf/1907.13114v1"
  },
  {
    "Title": "Robotics in Snow and Ice",
    "Authors": "François Pomerleau",
    "Published": "2022-08-10T01:02:57Z",
    "Summary": "Definition: The terms \"robotics in snow and ice\" refers to robotic systems being studied, developed, and used in areas where water can be found in its solid state. This specialized branch of field robotics investigates the impact of extreme conditions related to cold environments on autonomous vehicles.",
    "Link": "http://arxiv.org/abs/2208.05095v1",
    "PDF Link": "http://arxiv.org/pdf/2208.05095v1"
  },
  {
    "Title": "Robot Accident Investigation: a case study in Responsible Robotics",
    "Authors": "Alan F. T. Winfield, Katie Winkle, Helena Webb, Ulrik Lyngs, Marina Jirotka, Carl Macrae",
    "Published": "2020-05-15T11:31:54Z",
    "Summary": "Robot accidents are inevitable. Although rare, they have been happening since assembly-line robots were first introduced in the 1960s. But a new generation of social robots are now becoming commonplace. Often with sophisticated embedded artificial intelligence (AI) social robots might be deployed as care robots to assist elderly or disabled people to live independently. Smart robot toys offer a compelling interactive play experience for children and increasingly capable autonomous vehicles (AVs) the promise of hands-free personal transport and fully autonomous taxis. Unlike industrial robots which are deployed in safety cages, social robots are designed to operate in human environments and interact closely with humans; the likelihood of robot accidents is therefore much greater for social robots than industrial robots. This paper sets out a draft framework for social robot accident investigation; a framework which proposes both the technology and processes that would allow social robot accidents to be investigated with no less rigour than we expect of air or rail accident investigations. The paper also places accident investigation within the practice of responsible robotics, and makes the case that social robotics without accident investigation would be no less irresponsible than aviation without air accident investigation.",
    "Link": "http://arxiv.org/abs/2005.07474v1",
    "PDF Link": "http://arxiv.org/pdf/2005.07474v1"
  },
  {
    "Title": "Pattern Formation for Asynchronous Robots without Agreement in Chirality",
    "Authors": "Sruti Gan Chaudhuri, Swapnil Ghike, Shrainik Jain, Krishnendu Mukhopadhyaya",
    "Published": "2014-03-11T16:12:58Z",
    "Summary": "This paper presents a deterministic algorithm for forming a given asymmetric pattern in finite time by a set of autonomous, homogeneous, oblivious mobile robots under the CORDA model. The robots are represented as points on the 2D plane. There is no explicit communication between the robots. The robots coordinate among themselves by observing the positions of the other robots on the plane. Initially all the robots are assumed to be stationary. The robots have local coordinate systems defined by Sense of Direction (SoD), orientation or chirality and scale. Initially the robots are in asymmetric configuration. We show that these robots can form any given asymmetric pattern in finite time.",
    "Link": "http://arxiv.org/abs/1403.2625v1",
    "PDF Link": "http://arxiv.org/pdf/1403.2625v1"
  },
  {
    "Title": "Formation of General Position by Asynchronous Mobile Robots",
    "Authors": "S. Bhagat, S. Gan Chaudhuri, K. Mukhopadhyaya",
    "Published": "2014-08-09T07:43:54Z",
    "Summary": "The traditional distributed model of autonomous, homogeneous, mobile point robots usually assumes that the robots do not create any visual obstruction for the other robots, i.e., the robots are see through. In this paper, we consider a slightly more realistic model, by incorporating the notion of obstructed visibility (i.e., robots are not see through) for other robots. Under the new model of visibility, a robot may not have the full view of its surroundings. Many of the existing algorithms demand that each robot should have the complete knowledge of the positions of other robots. Since, vision is the only mean of their communication, it is required that the robots are in general position (i.e., no three robots are collinear). We consider asynchronous robots. They also do not have common chirality (or any agreement on a global coordinate system). In this paper, we present a distributed algorithm for obtaining a general position for the robots in finite time from any arbitrary configuration. The algorithm also assures collision free motion for each robot. This algorithm may also be used as a preprocessing module for many other subsequent tasks performed by the robots.",
    "Link": "http://arxiv.org/abs/1408.2072v1",
    "PDF Link": "http://arxiv.org/pdf/1408.2072v1"
  },
  {
    "Title": "A review of cuspidal serial and parallel manipulators",
    "Authors": "Philippe Wenger, Damien Chablat",
    "Published": "2022-10-11T07:19:04Z",
    "Summary": "Cuspidal robots can move from one inverse or direct kinematic solution to another without ever passing through a singularity. These robots have remained unknown because almost all industrial robots do not have this feature. However, in fact, industrial robots are the exceptions. Some robots appeared recently in the industrial market can be shown to be cuspidal but, surprisingly, almost nobody knows it and robot users meet difficulties in planning trajectories with these robots. This paper proposes a review on the fundamental and application aspects of cuspidal robots. It addresses the important issues raised by these robots for the design and planning of trajectories. The identification of all cuspidal robots is still an open issue. This paper recalls in details the case of serial robots with three joints but it also addresses robots with more complex architectures such as 6-revolute-jointed robot and parallel robots. We hope that this paper will help disseminate more widely knowledge on cuspidal robots.",
    "Link": "http://arxiv.org/abs/2210.05204v1",
    "PDF Link": "http://arxiv.org/pdf/2210.05204v1"
  },
  {
    "Title": "Optimal Dispersion of Silent Robots in a Ring",
    "Authors": "Bibhuti Das, Barun Gorain, Kaushik Mondal, Krishnendu Mukhopadhyaya, Supantha Pandit",
    "Published": "2024-08-10T08:43:07Z",
    "Summary": "Given a set of co-located mobile robots in an unknown anonymous graph, the robots must relocate themselves in distinct graph nodes to solve the dispersion problem. In this paper, we consider the dispersion problem for silent robots \\cite{gorain2024collaborative}, i.e., no direct, explicit communication between any two robots placed in the nodes of an oriented $n$ node ring network. The robots operate in synchronous rounds. The dispersion problem for silent mobile robots has been studied in arbitrary graphs where the robots start from a single source. In this paper, we focus on the dispersion problem for silent mobile robots where robots can start from multiple sources. The robots have unique labels from a range $[0,\\;L]$ for some positive integer $L$. Any two co-located robots do not have the information about the label of the other robot. The robots have weak multiplicity detection capability, which means they can determine if it is alone on a node. The robots are assumed to be able to identify an increase or decrease in the number of robots present on a node in a particular round. However, the robots can not get the exact number of increase or decrease in the number of robots. We have proposed a deterministic distributed algorithm that solves the dispersion of $k$ robots in an oriented ring in $O(\\log L+k)$ synchronous rounds with $O(\\log L)$ bits of memory for each robot. A lower bound $\\Omega(\\log L+k)$ on time for the dispersion of $k$ robots on a ring network is presented to establish the optimality of the proposed algorithm.",
    "Link": "http://arxiv.org/abs/2408.05491v1",
    "PDF Link": "http://arxiv.org/pdf/2408.05491v1"
  },
  {
    "Title": "Artificial Intelligence and Systems Theory: Applied to Cooperative\n  Robots",
    "Authors": "Pedro U. Lima, Luis M. M. Custodio",
    "Published": "2004-11-08T20:41:44Z",
    "Summary": "This paper describes an approach to the design of a population of cooperative robots based on concepts borrowed from Systems Theory and Artificial Intelligence. The research has been developed under the SocRob project, carried out by the Intelligent Systems Laboratory at the Institute for Systems and Robotics - Instituto Superior Tecnico (ISR/IST) in Lisbon. The acronym of the project stands both for \"Society of Robots\" and \"Soccer Robots\", the case study where we are testing our population of robots. Designing soccer robots is a very challenging problem, where the robots must act not only to shoot a ball towards the goal, but also to detect and avoid static (walls, stopped robots) and dynamic (moving robots) obstacles. Furthermore, they must cooperate to defeat an opposing team. Our past and current research in soccer robotics includes cooperative sensor fusion for world modeling, object recognition and tracking, robot navigation, multi-robot distributed task planning and coordination, including cooperative reinforcement learning in cooperative and adversarial environments, and behavior-based architectures for real time task execution of cooperating robot teams.",
    "Link": "http://arxiv.org/abs/cs/0411018v1",
    "PDF Link": "http://arxiv.org/pdf/cs/0411018v1"
  },
  {
    "Title": "Medical robotics: where we come from, where we are and where we could go",
    "Authors": "Jocelyne Troccaz",
    "Published": "2008-08-12T13:21:52Z",
    "Summary": "This short note presents a viewpoint about medical robotics.",
    "Link": "http://arxiv.org/abs/0808.1661v1",
    "PDF Link": "http://arxiv.org/pdf/0808.1661v1"
  },
  {
    "Title": "Game-Theoretic Modeling of Human Adaptation in Human-Robot Collaboration",
    "Authors": "Stefanos Nikolaidis, Swaprava Nath, Ariel D. Procaccia, Siddhartha Srinivasa",
    "Published": "2017-01-26T17:45:47Z",
    "Summary": "In human-robot teams, humans often start with an inaccurate model of the robot capabilities. As they interact with the robot, they infer the robot's capabilities and partially adapt to the robot, i.e., they might change their actions based on the observed outcomes and the robot's actions, without replicating the robot's policy. We present a game-theoretic model of human partial adaptation to the robot, where the human responds to the robot's actions by maximizing a reward function that changes stochastically over time, capturing the evolution of their expectations of the robot's capabilities. The robot can then use this model to decide optimally between taking actions that reveal its capabilities to the human and taking the best action given the information that the human currently has. We prove that under certain observability assumptions, the optimal policy can be computed efficiently. We demonstrate through a human subject experiment that the proposed model significantly improves human-robot team performance, compared to policies that assume complete adaptation of the human to the robot.",
    "Link": "http://arxiv.org/abs/1701.07790v2",
    "PDF Link": "http://arxiv.org/pdf/1701.07790v2"
  },
  {
    "Title": "Robots that Take Advantage of Human Trust",
    "Authors": "Dylan P. Losey, Dorsa Sadigh",
    "Published": "2019-09-12T16:16:21Z",
    "Summary": "Humans often assume that robots are rational. We believe robots take optimal actions given their objective; hence, when we are uncertain about what the robot's objective is, we interpret the robot's actions as optimal with respect to our estimate of its objective. This approach makes sense when robots straightforwardly optimize their objective, and enables humans to learn what the robot is trying to achieve. However, our insight is that---when robots are aware that humans learn by trusting that the robot actions are rational---intelligent robots do not act as the human expects; instead, they take advantage of the human's trust, and exploit this trust to more efficiently optimize their own objective. In this paper, we formally model instances of human-robot interaction (HRI) where the human does not know the robot's objective using a two-player game. We formulate different ways in which the robot can model the uncertain human, and compare solutions of this game when the robot has conservative, optimistic, rational, and trusting human models. In an offline linear-quadratic case study and a real-time user study, we show that trusting human models can naturally lead to communicative robot behavior, which influences end-users and increases their involvement.",
    "Link": "http://arxiv.org/abs/1909.05777v1",
    "PDF Link": "http://arxiv.org/pdf/1909.05777v1"
  },
  {
    "Title": "Animation Techniques in Human-Robot Interaction User Studies: a\n  Systematic Literature Review",
    "Authors": "Trenton Schulz, Jim Torresen, Jo Herstad",
    "Published": "2018-12-17T14:21:37Z",
    "Summary": "There are many different ways a robot can move in Human-Robot Interaction. One way is to use techniques from film animation to instruct the robot to move. This article is a systematic literature review of human-robot trials, pilots, and evaluations that have applied techniques from animation to move a robot. Through 27 articles, we find that animation techniques improves individual's interaction with robots, improving individual's perception of qualities of a robot, understanding what a robot intends to do, and showing the robot's state, or possible emotion. Animation techniques also help people relate to robots that do not resemble a human or robot. The studies in the articles show further areas for research, such as applying animation principles in other types of robots and situations, combining animation techniques with other modalities, and testing robots moving with animation techniques over the long term.",
    "Link": "http://arxiv.org/abs/1812.06784v4",
    "PDF Link": "http://arxiv.org/pdf/1812.06784v4"
  },
  {
    "Title": "Loosely Coupled Payload Transport System with Robot Replacement",
    "Authors": "Pulkit Verma, Rahul Tallamraju, Abhay Rawat, Subhasis Chand, Kamalakar Karlapalem",
    "Published": "2019-04-05T13:17:27Z",
    "Summary": "In this work, we present an algorithm for robot replacement to increase the operational time of a multi-robot payload transport system. Our system comprises a group of nonholonomic wheeled mobile robots traversing on a known trajectory. We design a multi-robot system with loosely coupled robots that ensures the system lasts much longer than the battery life of an individual robot. A system level optimization is presented, to decide on the operational state (charging or discharging) of each robot in the system. The charging state implies that the robot is not in a formation and is kept on charge whereas the discharging state implies that the robot is a part of the formation. Robot battery recharge hubs are present along the trajectory. Robots in the formation can be replaced at these hub locations with charged robots using a replacement mechanism. We showcase the efficacy of the proposed scheduling framework through simulations and experiments with real robots.",
    "Link": "http://arxiv.org/abs/1904.03049v2",
    "PDF Link": "http://arxiv.org/pdf/1904.03049v2"
  },
  {
    "Title": "Robot Vitals and Robot Health: Towards Systematically Quantifying\n  Runtime Performance Degradation in Robots Under Adverse Conditions",
    "Authors": "Aniketh Ramesh, Rustam Stolkin, Manolis Chiou",
    "Published": "2022-07-04T19:26:13Z",
    "Summary": "This paper addresses the problem of automatically detecting and quantifying performance degradation in remote mobile robots during task execution. A robot may encounter a variety of uncertainties and adversities during task execution, which can impair its ability to carry out tasks effectively and cause its performance to degrade. Such situations can be mitigated or averted by timely detection and intervention (e.g., by a remote human supervisor taking over control in teleoperation mode). Inspired by patient triaging systems in hospitals, we introduce the framework of \"robot vitals\" for estimating overall \"robot health\". A robot's vitals are a set of indicators that estimate the extent of performance degradation faced by a robot at a given point in time. Robot health is a metric that combines robot vitals into a single scalar value estimate of performance degradation. Experiments, both in simulation and on a real mobile robot, demonstrate that the proposed robot vitals and robot health can be used effectively to estimate robot performance degradation during runtime.",
    "Link": "http://arxiv.org/abs/2207.01684v1",
    "PDF Link": "http://arxiv.org/pdf/2207.01684v1"
  },
  {
    "Title": "Come Closer: The Effects of Robot Personality on Human Proxemics\n  Behaviours",
    "Authors": "Meriam Moujahid, David A. Robb, Christian Dondrup, Helen Hastie",
    "Published": "2023-09-06T13:24:45Z",
    "Summary": "Social Robots in human environments need to be able to reason about their physical surroundings while interacting with people. Furthermore, human proxemics behaviours around robots can indicate how people perceive the robots and can inform robot personality and interaction design. Here, we introduce Charlie, a situated robot receptionist that can interact with people using verbal and non-verbal communication in a dynamic environment, where users might enter or leave the scene at any time. The robot receptionist is stationary and cannot navigate. Therefore, people have full control over their personal space as they are the ones approaching the robot. We investigated the influence of different apparent robot personalities on the proxemics behaviours of the humans. The results indicate that different types of robot personalities, specifically introversion and extroversion, can influence human proxemics behaviours. Participants maintained shorter distances with the introvert robot receptionist, compared to the extrovert robot. Interestingly, we observed that human-robot proxemics were not the same as typical human-human interpersonal distances, as defined in the literature. We therefore propose new proxemics zones for human-robot interaction.",
    "Link": "http://arxiv.org/abs/2309.02979v1",
    "PDF Link": "http://arxiv.org/pdf/2309.02979v1"
  },
  {
    "Title": "Soft is Safe: Human-Robot Interaction for Soft Robots",
    "Authors": "Rajashekhar V S, Gowdham Prabhakar",
    "Published": "2025-02-03T11:26:32Z",
    "Summary": "With the presence of robots increasing in the society, the need for interacting with robots is becoming necessary. The field of Human-Robot Interaction (HRI) has emerged important since more repetitive and tiresome jobs are being done by robots. In the recent times, the field of soft robotics has seen a boom in the field of research and commercialization. The Industry 5.0 focuses on human robot collaboration which also spurs the field of soft robotics. However the HRI for soft robotics is still in the nascent stage. In this work we review and then discuss how HRI is done for soft robots. We first discuss the control, design, materials and manufacturing of soft robots. This will provide an understanding of what is being interacted with. Then we discuss about the various input and output modalities that are used in HRI. The applications where the HRI for soft robots are found in the literature are discussed in detail. Then the limitations of HRI for soft robots and various research opportunities that exist in this field are discussed in detail. It is concluded that there is a huge scope for development for HRI for soft robots.",
    "Link": "http://arxiv.org/abs/2502.01256v1",
    "PDF Link": "http://arxiv.org/pdf/2502.01256v1"
  },
  {
    "Title": "Modular Robots: extending the capabilities of one robot",
    "Authors": "Aymen Rachdi, Fedi Zrelli, Amine Kammmoun",
    "Published": "2022-10-24T13:26:18Z",
    "Summary": "For a robot to be perfect and enter the everyday life of humans,like computers did, it needs to move from special-purpose robots to general-purpose. So, the idea of modularity is considered in this project.Thus, any type of task that falls in the 4 D's of Robotization: Dull, Dirty, Dangerous and Dear can be achieved by adding a module to the robot.",
    "Link": "http://arxiv.org/abs/2211.05572v1",
    "PDF Link": "http://arxiv.org/pdf/2211.05572v1"
  },
  {
    "Title": "Cuspidal Robots",
    "Authors": "Philippe Wenger",
    "Published": "2016-10-13T13:58:59Z",
    "Summary": "This chapter is dedicated to the so-called cuspidal robots, i.e. those robots that can move from one inverse geometric solution to another without meeting a singular confuguration. This feature was discovered quite recently and has then been fascinating a lot of researchers. After a brief history of cuspidal robots, the chapter provides the main features of cuspidal robots: explanation of the non-singular change of posture, uniqueness domains, regions of feasible paths, identification and classification of cuspidal robots. The chapter focuses on 3-R orthogonal serial robots. The case of 6-dof robots and parallel robots is discussed in the end of this chapter.",
    "Link": "http://arxiv.org/abs/1610.04080v2",
    "PDF Link": "http://arxiv.org/pdf/1610.04080v2"
  },
  {
    "Title": "Effects of Interruptibility-Aware Robot Behavior",
    "Authors": "Siddhartha Banerjee, Andrew Silva, Karen Feigh, Sonia Chernova",
    "Published": "2018-04-17T17:26:30Z",
    "Summary": "As robots become increasingly prevalent in human environments, there will inevitably be times when a robot needs to interrupt a human to initiate an interaction. Our work introduces the first interruptibility-aware mobile robot system, and evaluates the effects of interruptibility-awareness on human task performance, robot task performance, and on human interpretation of the robot's social aptitude. Our results show that our robot is effective at predicting interruptibility at high accuracy, allowing it to interrupt at more appropriate times. Results of a large-scale user study show that while participants are able to maintain task performance even in the presence of interruptions, interruptibility-awareness improves the robot's task performance and improves participant social perception of the robot.",
    "Link": "http://arxiv.org/abs/1804.06383v1",
    "PDF Link": "http://arxiv.org/pdf/1804.06383v1"
  },
  {
    "Title": "On Robot Revolution and Taxation",
    "Authors": "Tshilidzi Marwala",
    "Published": "2018-08-05T18:26:34Z",
    "Summary": "Advances in artificial intelligence are resulting in the rapid automation of the work force. The tools that are used to automate are called robots. Bill Gates proposed that in order to deal with the problem of the loss of jobs and reduction of the tax revenue we ought to tax the robots. The problem with taxing the robots is that it is not easy to know what a robot is. This article studies the definition of a robot and the implication of advances in robotics on taxation. It is evident from this article that it is a difficult task to establish what a robot is and what is not a robot. It concludes that taxing robots is the same as increasing corporate tax.",
    "Link": "http://arxiv.org/abs/1808.01666v1",
    "PDF Link": "http://arxiv.org/pdf/1808.01666v1"
  },
  {
    "Title": "Graph Neural Networks for Learning Robot Team Coordination",
    "Authors": "Amanda Prorok",
    "Published": "2018-05-09T21:24:50Z",
    "Summary": "This paper shows how Graph Neural Networks can be used for learning distributed coordination mechanisms in connected teams of robots. We capture the relational aspect of robot coordination by modeling the robot team as a graph, where each robot is a node, and edges represent communication links. During training, robots learn how to pass messages and update internal states, so that a target behavior is reached. As a proxy for more complex problems, this short paper considers the problem where each robot must locally estimate the algebraic connectivity of the team's network topology.",
    "Link": "http://arxiv.org/abs/1805.03737v2",
    "PDF Link": "http://arxiv.org/pdf/1805.03737v2"
  },
  {
    "Title": "Recent Advances in Human-Robot Collaboration Towards Joint Action",
    "Authors": "Yeshasvi Tirupachuri, Gabriele Nava, Lorenzo Rapetti, Claudia Latella, Kourosh Darvish, Daniele Pucci",
    "Published": "2020-01-02T12:26:20Z",
    "Summary": "Robots existed as separate entities till now, but the horizons of a symbiotic human-robot partnership are impending. Despite all the recent technical advances in terms of hardware, robots are still not endowed with desirable relational skills that ensure a social component in their existence. This article draws from our experience as roboticists in Human-Robot Collaboration (HRC) with humanoid robots and presents some of the recent advances made towards realizing intuitive robot behaviors and partner-aware control involving physical interactions.",
    "Link": "http://arxiv.org/abs/2001.00411v1",
    "PDF Link": "http://arxiv.org/pdf/2001.00411v1"
  },
  {
    "Title": "SENSAR: A Visual Tool for Intelligent Robots for Collaborative\n  Human-Robot Interaction",
    "Authors": "Andre Cleaver, Faizan Muhammad, Amel Hassan, Elaine Short, Jivko Sinapov",
    "Published": "2020-11-09T15:50:32Z",
    "Summary": "Establishing common ground between an intelligent robot and a human requires communication of the robot's intention, behavior, and knowledge to the human to build trust and assure safety in a shared environment. This paper introduces SENSAR (Seeing Everything iN Situ with Augmented Reality), an augmented reality robotic system that enables robots to communicate their sensory and cognitive data in context over the real-world with rendered graphics, allowing a user to understand, correct, and validate the robot's perception of the world. Our system aims to support human-robot interaction research by establishing common ground where the perceptions of the human and the robot align.",
    "Link": "http://arxiv.org/abs/2011.04515v1",
    "PDF Link": "http://arxiv.org/pdf/2011.04515v1"
  },
  {
    "Title": "A Survey on End-User Robot Programming",
    "Authors": "Gopika Ajaykumar, Maureen Steele, Chien-Ming Huang",
    "Published": "2021-05-04T20:55:01Z",
    "Summary": "As robots interact with a broader range of end-users, end-user robot programming has helped democratize robot programming by empowering end-users who may not have experience in robot programming to customize robots to meet their individual contextual needs. This article surveys work on end-user robot programming, with a focus on end-user program specification. It describes the primary domains, programming phases, and design choices represented by the end-user robot programming literature. The survey concludes by highlighting open directions for further investigation to enhance and widen the reach of end-user robot programming systems.",
    "Link": "http://arxiv.org/abs/2105.01757v2",
    "PDF Link": "http://arxiv.org/pdf/2105.01757v2"
  },
  {
    "Title": "Utilising Explanations to Mitigate Robot Conversational Failures",
    "Authors": "Dimosthenis Kontogiorgos",
    "Published": "2023-07-10T10:20:09Z",
    "Summary": "This paper presents an overview of robot failure detection work from HRI and adjacent fields using failures as an opportunity to examine robot explanation behaviours. As humanoid robots remain experimental tools in the early 2020s, interactions with robots are situated overwhelmingly in controlled environments, typically studying various interactional phenomena. Such interactions suffer from real-world and large-scale experimentation and tend to ignore the 'imperfectness' of the everyday user. Robot explanations can be used to approach and mitigate failures, by expressing robot legibility and incapability, and within the perspective of common-ground. In this paper, I discuss how failures present opportunities for explanations in interactive conversational robots and what the potentials are for the intersection of HRI and explainability research.",
    "Link": "http://arxiv.org/abs/2307.04462v1",
    "PDF Link": "http://arxiv.org/pdf/2307.04462v1"
  },
  {
    "Title": "Physical Simulation of Inarticulate Robots",
    "Authors": "Guillaume Claret, Michaël Mathieu, David Naccache, Guillaume Seguin",
    "Published": "2011-04-08T11:16:59Z",
    "Summary": "In this note we study the structure and the behavior of inarticulate robots. We introduce a robot that moves by successive revolvings. The robot's structure is analyzed, simulated and discussed in detail.",
    "Link": "http://arxiv.org/abs/1104.1546v1",
    "PDF Link": "http://arxiv.org/pdf/1104.1546v1"
  },
  {
    "Title": "High-level robot programming based on CAD: dealing with unpredictable\n  environments",
    "Authors": "Pedro Neto, Nuno Mendes, Ricardo Araújo, J. Norberto Pires, A. Paulo Moreira",
    "Published": "2013-09-09T09:46:38Z",
    "Summary": "Purpose - The purpose of this paper is to present a CAD-based human-robot interface that allows non-expert users to teach a robot in a manner similar to that used by human beings to teach each other.   Design/methodology/approach - Intuitive robot programming is achieved by using CAD drawings to generate robot programs off-line. Sensory feedback allows minimization of the effects of uncertainty, providing information to adjust the robot paths during robot operation.   Findings - It was found that it is possible to generate a robot program from a common CAD drawing and run it without any major concerns about calibration or CAD model accuracy.   Research limitations/implications - A limitation of the proposed system has to do with the fact that it was designed to be used for particular technological applications.   Practical implications - Since most manufacturing companies have CAD packages in their facilities today, CAD-based robot programming may be a good option to program robots without the need for skilled robot programmers.   Originality/value - The paper proposes a new CAD-based robot programming system. Robot programs are directly generated from a CAD drawing running on a commonly available 3D CAD package (Autodesk Inventor) and not from a commercial, computer aided robotics (CAR) software, making it a simple CAD integrated solution. This is a low-cost and low-setup time system where no advanced robot programming skills are required to operate it. In summary, robot programs are generated with a high-level of abstraction from the robot language.",
    "Link": "http://arxiv.org/abs/1309.2086v1",
    "PDF Link": "http://arxiv.org/pdf/1309.2086v1"
  },
  {
    "Title": "EPANer Team Description Paper for World Robot Challenge 2020",
    "Authors": "Zhi Yan, Nathan Crombez, Li Sun",
    "Published": "2019-09-05T12:20:07Z",
    "Summary": "This paper presents the research focus and ideas incorporated in the EPANer robotics team, entering the World Robot Challenge 2020 - Partner Robot Challenge (Real Space).",
    "Link": "http://arxiv.org/abs/1909.02355v1",
    "PDF Link": "http://arxiv.org/pdf/1909.02355v1"
  },
  {
    "Title": "Fog Robotics: A Summary, Challenges and Future Scope",
    "Authors": "Siva Leela Krishna Chand Gudi, Benjamin Johnston, Mary-Anne Williams",
    "Published": "2019-08-14T03:07:12Z",
    "Summary": "Human-robot interaction plays a crucial role to make robots closer to humans. Usually, robots are limited by their own capabilities. Therefore, they utilise Cloud Robotics to enhance their dexterity. Its ability includes the sharing of information such as maps, images and the processing power. This whole process involves distributing data which intend to rise enormously. New issues can arise such as bandwidth, network congestion at backhaul and fronthaul systems resulting in high latency. Thus, it can make an impact on seamless connectivity between the robots, users and the cloud. Also, a robot may not accomplish its goal successfully within a stipulated time. As a consequence, Cloud Robotics cannot be in a position to handle the traffic imposed by robots. On the contrary, impending Fog Robotics can act as a solution by solving major problems of Cloud Robotics. Therefore to check its feasibility, we discuss the need and architectures of Fog Robotics in this paper. To evaluate the architectures, we used a realistic scenario of Fog Robotics by comparing them with Cloud Robotics. Next, latency is chosen as the primary factor for validating the effectiveness of the system. Besides, we utilised real-time latency using Pepper robot, Fog robot server and the Cloud server. Experimental results show that Fog Robotics reduces latency significantly compared to Cloud Robotics. Moreover, advantages, challenges and future scope of the Fog Robotics system is further discussed.",
    "Link": "http://arxiv.org/abs/1908.04935v1",
    "PDF Link": "http://arxiv.org/pdf/1908.04935v1"
  },
  {
    "Title": "Human-Robot Creative Interactions (HRCI): Exploring Creativity in\n  Artificial Agents Using a Story-Telling Game",
    "Authors": "Eduardo Benitez Sandoval, Ricardo Sosa, Massimiliano Cappuccio, Tomasz Bednarz",
    "Published": "2022-02-08T07:57:42Z",
    "Summary": "Creativity in social robots requires further attention in the interdisciplinary field of Human-Robot Interaction (HRI). This paper investigates the hypothesised connection between the perceived creative agency and the animacy of social robots. The goal of this work is to assess the relevance of robot movements in the attribution of creativity to robots. The results of this work inform the design of future Human-Robot Creative Interactions (HRCI). The study uses a storytelling game based on visual imagery inspired by the game 'Story Cubes' to explore the perceived creative agency of social robots. This game is used to tell a classic story for children with an alternative ending. A 2x2 experiment was designed to compare two conditions: the robot telling the original version of the story and the robot plot-twisting the end of the story. A Robotis Mini humanoid robot was used for the experiment. As a novel contribution, we propose an adaptation of the Short Scale Creative Self scale (SSCS) to measure perceived creative agency in robots. We also use the Godspeed scale to explore different attributes of social robots in this setting. We did not obtain significant main effects of the robot movements or the story in the participants' scores. However, we identified significant main effects of the robot movements in features of animacy, likeability, and perceived safety. This initial work encourages further studies experimenting with different robot embodiment and movements to evaluate the perceived creative agency in robots and inform the design of future robots that participate in creative interactions.",
    "Link": "http://arxiv.org/abs/2202.03702v1",
    "PDF Link": "http://arxiv.org/pdf/2202.03702v1"
  },
  {
    "Title": "Oblivious Robots Performing Different Tasks on Grid Without Knowing\n  their Team Members",
    "Authors": "Satakshi Ghosh, Avisek Sharma, Pritam Goswami, Buddhadeb Sau",
    "Published": "2022-10-02T16:33:25Z",
    "Summary": "Two fundamental problems of distributed computing are Gathering and Arbitrary pattern formation (\\textsc{Apf}). These two tasks are different in nature as in gathering robots meet at a point but in \\textsc{Apf} robots form a fixed pattern in distinct positions.   In most of the current literature on swarm robot algorithms, it is assumed that all robots in the system perform one single task together. Two teams of oblivious robots deployed in the same system and different teams of robots performing two different works simultaneously where no robot knows the team of another robot is a new concept in the literature introduced by Bhagat et al. [ICDCN'2020].   In this work, a swarm of silent and oblivious robots are deployed on an infinite grid under an asynchronous scheduler. The robots do not have access to any global coordinates. Some of the robots are given input of an arbitrary but unique pattern. The set of robots with the given pattern is assigned the task of forming the given pattern on the grid. The remaining robots are assigned with the task of gathering to a vertex of the grid (not fixed from earlier and not any point where a robot that is forming a pattern terminates). Each robot knows to which team it belongs, but can not recognize the team of another robot. Considering weak multiplicity detection, a distributed algorithm is presented in this paper which leads the robots with the input pattern into forming it and other robots into gathering on a vertex of the grid on which no other robot forming the pattern, terminates.",
    "Link": "http://arxiv.org/abs/2210.00567v2",
    "PDF Link": "http://arxiv.org/pdf/2210.00567v2"
  },
  {
    "Title": "Robot Structure Prior Guided Temporal Attention for Camera-to-Robot Pose\n  Estimation from Image Sequence",
    "Authors": "Yang Tian, Jiyao Zhang, Zekai Yin, Hao Dong",
    "Published": "2023-07-22T15:34:46Z",
    "Summary": "In this work, we tackle the problem of online camera-to-robot pose estimation from single-view successive frames of an image sequence, a crucial task for robots to interact with the world.",
    "Link": "http://arxiv.org/abs/2307.12106v1",
    "PDF Link": "http://arxiv.org/pdf/2307.12106v1"
  },
  {
    "Title": "The Dark Side of Ethical Robots",
    "Authors": "Dieter Vanderelst, Alan Winfield",
    "Published": "2016-06-08T14:47:35Z",
    "Summary": "Concerns over the risks associated with advances in Artificial Intelligence have prompted calls for greater efforts toward robust and beneficial AI, including machine ethics. Recently, roboticists have responded by initiating the development of so-called ethical robots. These robots would, ideally, evaluate the consequences of their actions and morally justify their choices. This emerging field promises to develop extensively over the next years. However, in this paper, we point out an inherent limitation of the emerging field of ethical robots. We show that building ethical robots also necessarily facilitates the construction of unethical robots. In three experiments, we show that it is remarkably easy to modify an ethical robot so that it behaves competitively, or even aggressively. The reason for this is that the specific AI, required to make an ethical robot, can always be exploited to make unethical robots. Hence, the development of ethical robots will not guarantee the responsible deployment of AI. While advocating for ethical robots, we conclude that preventing the misuse of robots is beyond the scope of engineering, and requires instead governance frameworks underpinned by legislation. Without this, the development of ethical robots will serve to increase the risks of robotic malpractice instead of diminishing it.",
    "Link": "http://arxiv.org/abs/1606.02583v1",
    "PDF Link": "http://arxiv.org/pdf/1606.02583v1"
  },
  {
    "Title": "A Systematic Literature Review of Experiments in Socially Assistive\n  Robotics using Humanoid Robots",
    "Authors": "Floris Erich, Masakazu Hirokawa, Kenji Suzuki",
    "Published": "2017-11-15T02:01:07Z",
    "Summary": "We perform a Systematic Literature Review to discover how Humanoid robots are being applied in Socially Assistive Robotics experiments. Our search returned 24 papers, from which 16 were included for closer analysis. To do this analysis we used a conceptual framework inspired by Behavior-based Robotics. We were interested in finding out which robot was used (most use the robot NAO), what the goals of the application were (teaching, assisting, playing, instructing), how the robot was controlled (manually in most of the experiments), what kind of behaviors the robot exhibited (reacting to touch, pointing at body parts, singing a song, dancing, among others), what kind of actuators the robot used (always motors, sometimes speakers, hardly ever any other type of actuator) and what kind of sensors the robot used (in many studies the robot did not use any sensors at all, in others the robot frequently used camera and/or microphone). The results of this study can be used for designing software frameworks targeting Humanoid Socially Assistive Robotics, especially in the context of Software Product Line Engineering projects.",
    "Link": "http://arxiv.org/abs/1711.05379v1",
    "PDF Link": "http://arxiv.org/pdf/1711.05379v1"
  },
  {
    "Title": "Swarm Relays: Distributed Self-Healing Ground-and-Air Connectivity\n  Chains",
    "Authors": "Vivek Shankar Varadharajan, David St-Onge, Bram Adams, Giovanni Beltrame",
    "Published": "2019-09-23T17:29:39Z",
    "Summary": "The coordination of robot swarms - large decentralized teams of robots - generally relies on robust and efficient inter-robot communication. Maintaining communication between robots is particularly challenging in field deployments. Unstructured environments, limited computational resources, low bandwidth, and robot failures all contribute to the complexity of connectivity maintenance. In this paper, we propose a novel lightweight algorithm to navigate a group of robots in complex environments while maintaining connectivity by building a chain of robots. The algorithm is robust to single robot failures and can heal broken communication links. The algorithm works in 3D environments: when a region is unreachable by wheeled robots, the chain is extended with flying robots. We test the performance of the algorithm using up to 100 robots in a physics-based simulator with three mazes and different robot failure scenarios. We then validate the algorithm with physical platforms: 7 wheeled robots and 6 flying ones, in homogeneous and heterogeneous scenarios.",
    "Link": "http://arxiv.org/abs/1909.10496v2",
    "PDF Link": "http://arxiv.org/pdf/1909.10496v2"
  },
  {
    "Title": "Particle robots A new specie of hybrid bio-inspired robotics",
    "Authors": "Luis A. Mateos",
    "Published": "2020-02-15T07:19:15Z",
    "Summary": "Inspired by a couple of simple organisms without eyes, neither ears. This paper presents a novel hybrid bionic robot, called \"particle robot\", which mix a macro-organism and a micro-organism in the same robot. On one hand, an interesting rather boring animal, the biological Echinoids (sea urchins) is mixed with the viruses micro-organisms, in specific the rotaviruses; together with spherical mobile robots. Analogously, from a pure robotic perspective, this bio-inspired robot can be seen as a spherical mobile robot wearing an actuated exoskeleton. The robot has two main configurations: when the spines are contracted it becomes a spherical mobile robot able to move in a fast pace on land, embedding all spherical mobile robots properties. On the other hand, when the spines or legs are extended in a controlled pattern, it can walk on flat surfaces as well as move on snow and over rocks as a bionic sea urchin. The spines of the robot are telescopic linear actuators, which combines soft and hard 3D print materials to make the actuation unit flexible for compressing it in minimal space and rigid for lifting the robot.",
    "Link": "http://arxiv.org/abs/2003.08289v1",
    "PDF Link": "http://arxiv.org/pdf/2003.08289v1"
  },
  {
    "Title": "Simplifying Robot Programming using Augmented Reality and End-User\n  Development",
    "Authors": "Enes Yigitbas, Ivan Jovanovikj, Gregor Engels",
    "Published": "2021-06-15T07:57:48Z",
    "Summary": "Robots are widespread across diverse application contexts. Teaching robots to perform tasks, in their respective contexts, demands a high domain and programming expertise. However, robot programming faces high entry barriers due to the complexity of robot programming itself. Even for experts robot programming is a cumbersome and error-prone task where faulty robot programs can be created, causing damage when being executed on a real robot. To simplify the process of robot programming, we combine Augmented Reality (AR) with principles of end-user development. By combining them, the real environment is extended with useful virtual artifacts that can enable experts as well as non-professionals to perform complex robot programming tasks. Therefore, Simple Programming Environment in Augmented Reality with Enhanced Debugging (SPEARED) was developed as a prototype for an AR-assisted robot programming environment. SPEARED makes use of AR to project a robot as well as a programming environment onto the target working space. To evaluate our approach, expert interviews with domain experts from the area of industrial automation, robotics, and AR were performed. The experts agreed that SPEARED has the potential to enrich and ease current robot programming processes.",
    "Link": "http://arxiv.org/abs/2106.07944v1",
    "PDF Link": "http://arxiv.org/pdf/2106.07944v1"
  },
  {
    "Title": "Know Thyself: Transferable Visual Control Policies Through\n  Robot-Awareness",
    "Authors": "Edward S. Hu, Kun Huang, Oleh Rybkin, Dinesh Jayaraman",
    "Published": "2021-07-19T17:56:04Z",
    "Summary": "Training visual control policies from scratch on a new robot typically requires generating large amounts of robot-specific data. How might we leverage data previously collected on another robot to reduce or even completely remove this need for robot-specific data? We propose a \"robot-aware control\" paradigm that achieves this by exploiting readily available knowledge about the robot. We then instantiate this in a robot-aware model-based RL policy by training modular dynamics models that couple a transferable, robot-aware world dynamics module with a robot-specific, potentially analytical, robot dynamics module. This also enables us to set up visual planning costs that separately consider the robot agent and the world. Our experiments on tabletop manipulation tasks with simulated and real robots demonstrate that these plug-in improvements dramatically boost the transferability of visual model-based RL policies, even permitting zero-shot transfer of visual manipulation skills onto new robots. Project website: https://www.seas.upenn.edu/~hued/rac",
    "Link": "http://arxiv.org/abs/2107.09047v3",
    "PDF Link": "http://arxiv.org/pdf/2107.09047v3"
  },
  {
    "Title": "ARROCH: Augmented Reality for Robots Collaborating with a Human",
    "Authors": "Kishan Chandan, Vidisha Kudalkar, Xiang Li, Shiqi Zhang",
    "Published": "2021-09-21T18:46:19Z",
    "Summary": "Human-robot collaboration frequently requires extensive communication, e.g., using natural language and gestures. Augmented reality (AR) has provided an alternative way of bridging the communication gap between robots and people. However, most current AR-based human-robot communication methods are unidirectional, focusing on how the human adapts to robot behaviors, and are limited to single-robot domains. In this paper, we develop AR for Robots Collaborating with a Human (ARROCH), a novel algorithm and system that supports bidirectional, multi-turn, human-multi-robot communication in indoor multi-room environments. The human can see through obstacles to observe the robots' current states and intentions, and provide feedback, while the robots' behaviors are then adjusted toward human-multi-robot teamwork. Experiments have been conducted with real robots and human participants using collaborative delivery tasks. Results show that ARROCH outperformed a standard non-AR approach in both user experience and teamwork efficiency. In addition, we have developed a novel simulation environment using Unity (for AR and human simulation) and Gazebo (for robot simulation). Results in simulation demonstrate ARROCH's superiority over AR-based baselines in human-robot collaboration.",
    "Link": "http://arxiv.org/abs/2109.10400v2",
    "PDF Link": "http://arxiv.org/pdf/2109.10400v2"
  },
  {
    "Title": "Decentralized Probabilistic Multi-Robot Collision Avoidance Using\n  Buffered Uncertainty-Aware Voronoi Cells",
    "Authors": "Hai Zhu, Bruno Brito, Javier Alonso-Mora",
    "Published": "2022-01-11T15:53:29Z",
    "Summary": "In this paper, we present a decentralized and communication-free collision avoidance approach for multi-robot systems that accounts for both robot localization and sensing uncertainties. The approach relies on the computation of an uncertainty-aware safe region for each robot to navigate among other robots and static obstacles in the environment, under the assumption of Gaussian-distributed uncertainty. In particular, at each time step, we construct a chance-constrained buffered uncertainty-aware Voronoi cell (B-UAVC) for each robot given a specified collision probability threshold. Probabilistic collision avoidance is achieved by constraining the motion of each robot to be within its corresponding B-UAVC, i.e. the collision probability between the robots and obstacles remains below the specified threshold. The proposed approach is decentralized, communication-free, scalable with the number of robots and robust to robots' localization and sensing uncertainties. We applied the approach to single-integrator, double-integrator, differential-drive robots, and robots with general nonlinear dynamics. Extensive simulations and experiments with a team of ground vehicles, quadrotors, and heterogeneous robot teams are performed to analyze and validate the proposed approach.",
    "Link": "http://arxiv.org/abs/2201.04012v1",
    "PDF Link": "http://arxiv.org/pdf/2201.04012v1"
  },
  {
    "Title": "HeRo 2.0: A Low-Cost Robot for Swarm Robotics Research",
    "Authors": "Paulo Rezeck, Hector Azpurua, Mauricio FS Correa, Luiz Chaimowicz",
    "Published": "2022-02-24T22:23:14Z",
    "Summary": "The current state of electronic component miniaturization coupled with the increasing efficiency in hardware and software allow the development of smaller and compact robotic systems. The convenience of using these small, simple, yet capable robots has gathered the research community's attention towards practical applications of swarm robotics. This paper presents the design of a novel platform for swarm robotics applications that is low cost, easy to assemble using off-the-shelf components, and deeply integrated with the most used robotic framework available today: ROS (Robot Operating System). The robotic platform is entirely open, composed of a 3D printed body and open-source software. We describe its architecture, present its main features, and evaluate its functionalities executing experiments using a couple of robots. Results demonstrate that the proposed mobile robot is very effective given its small size and reduced cost, being suitable for swarm robotics research and education.",
    "Link": "http://arxiv.org/abs/2202.12391v2",
    "PDF Link": "http://arxiv.org/pdf/2202.12391v2"
  },
  {
    "Title": "Omnidirectional robot modeling and simulation",
    "Authors": "Sandro Costa Magalhães, António Paulo Moreira, Paulo Costa",
    "Published": "2022-11-15T22:10:08Z",
    "Summary": "A robot simulation system is a basic need for any robotics application. With it, developers' teams of robots can test their algorithms and make initial calibrations without risk of damage to the real robots, assuring safety. However, building these simulation environments is usually time-consuming work, and when considering robot fleets, the simulation reveals to be computing expensive. With it, developers building teams of robots can test their algorithms and make initial calibrations without risk of damage to the real robots, assuring safety. An omnidirectional robot from the 5DPO robotics soccer team served to test this approach. The modeling issue was divided into two steps: modeling the motor's non-linear features and modeling the general behavior of the robot. A proper fitting of the robot was reached, considering the velocity robot's response.",
    "Link": "http://arxiv.org/abs/2211.08532v1",
    "PDF Link": "http://arxiv.org/pdf/2211.08532v1"
  },
  {
    "Title": "Optimization of Humanoid Robot Designs for Human-Robot Ergonomic Payload\n  Lifting",
    "Authors": "Carlotta Sartore, Lorenzo Rapetti, Daniele Pucci",
    "Published": "2022-11-24T09:44:48Z",
    "Summary": "When a human and a humanoid robot collaborate physically, ergonomics is a key factor to consider. Assuming a given humanoid robot, several control architectures exist nowadays to address ergonomic physical human-robot collaboration. This paper takes one step further by considering robot hardware parameters as optimization variables in the problem of collaborative payload lifting. The variables that parametrize robot's kinematics and dynamics ensure their physical consistency, and the human model is considered in the optimization problem. By leveraging the proposed modelling framework, the ergonomy of the interaction is maximized, here given by the agents' energy expenditure. Robot kinematic, dynamics, hardware constraints and human geometries are considered when solving the associated optimization problem. The proposed methodology is used to identify optimum hardware parameters for the design of the ergoCub robot, a humanoid possessing a degree of embodied intelligence for ergonomic interaction with humans. For the optimization problem, the starting point is the iCub humanoid robot. The obtained robot design reaches loads at heights in the range of 0.8-1.5 m with respect to the iCub robot whose range is limited to 0.8-1.2 m. The robot energy expenditure is decreased by about 33%, meanwhile, the human ergonomy is preserved, leading overall to an improved interaction.",
    "Link": "http://arxiv.org/abs/2211.13503v1",
    "PDF Link": "http://arxiv.org/pdf/2211.13503v1"
  },
  {
    "Title": "Web-based Experiment on Human Performance in Dual-Robot Teleoperation",
    "Authors": "Yuhui Wan, Chengxu Zhou",
    "Published": "2022-12-13T10:29:17Z",
    "Summary": "In most cases, upgrading from a single-robot system to a multi-robot system comes with increases in system payload and task performance. On the other hand, many multi-robot systems in open environments still rely on teleoperation. Therefore, human performance can be the bottleneck in a teleoperated multi-robot system. Based on this idea, the multi-robot system's shared autonomy and control methods are emerging research areas in open environment robot operations. However, the question remains: how much does the bottleneck of the human agent impact the system performance in a multi-robot system? This research tries to explore the question through the performance comparison of teleoperating a single-robot system and a dual-robot system in a box-pushing task. This robot teleoperation experiment on human agents employs a web-based environment to simulate the robots' two-dimensional movement. The result provides evidence of the hardship for a single human when teleoperating with more than one robot, which indicates the necessity of shared autonomy in multi-robot systems.",
    "Link": "http://arxiv.org/abs/2212.06462v1",
    "PDF Link": "http://arxiv.org/pdf/2212.06462v1"
  },
  {
    "Title": "A C++ Implementation of a Cartesian Impedance Controller for Robotic\n  Manipulators",
    "Authors": "Matthias Mayr, Julian M. Salt-Ducaju",
    "Published": "2022-12-21T17:42:33Z",
    "Summary": "Cartesian impedance control is a type of motion control strategy for robots that improves safety in partially unknown environments by achieving a compliant behavior of the robot with respect to its external forces. This compliant robot behavior has the added benefit of allowing physical human guidance of the robot. In this paper, we propose a C++ implementation of compliance control valid for any torque-commanded robotic manipulator. The proposed controller implements Cartesian impedance control to track a desired end-effector pose. Additionally, joint impedance is projected in the nullspace of the Cartesian robot motion to track a desired robot joint configuration without perturbing the Cartesian motion of the robot. The proposed implementation also allows the robot to apply desired forces and torques to its environment. Several safety features such as filtering, rate limiting, and saturation are included in the proposed implementation. The core functionalities are in a re-usable base library and a Robot Operating System (ROS) ros_control integration is provided on top of that. The implementation was tested with the KUKA LBR iiwa robot and the Franka Emika Robot (Panda) both in simulation and with the physical robots.",
    "Link": "http://arxiv.org/abs/2212.11215v1",
    "PDF Link": "http://arxiv.org/pdf/2212.11215v1"
  },
  {
    "Title": "Asynchronous Gathering of Robots with Finite Memory on a Circle under\n  Limited Visibility",
    "Authors": "Satakshi Ghosh, Avisek Sharma, Pritam Goswami, Buddhadeb Sau",
    "Published": "2023-02-15T11:40:43Z",
    "Summary": "Consider a set of $n$ mobile entities, called robots, located and operating on a continuous circle, i.e., all robots are initially in distinct locations on a circle. The \\textit{gathering} problem asks to design a distributed algorithm that allows the robots to assemble at a point on the circle. Robots are anonymous, identical, and homogeneous. Robots operate in a deterministic Look-Compute-Move cycle within the circular path. Robots agree on the clockwise direction. The robot's movement is rigid and they have limited visibility $\\pi$, i.e., each robot can only see the points of the circle which is at an angular distance strictly less than $\\pi$ from the robot.   Di Luna \\textit{et al}. [DISC'2020] provided a deterministic gathering algorithm of oblivious and silent robots on a circle in semi-synchronous (\\textsc{SSync}) scheduler. Buchin \\textit{et al}. [IPDPS(W)'2021] showed that, under full visibility, $\\mathcal{OBLOT}$ robot model with \\textsc{SSync} scheduler is incomparable to $\\mathcal{FSTA}$ robot (robots are silent but have finite persistent memory) model with asynchronous (\\textsc{ASync}) scheduler. Under limited visibility, this comparison is still unanswered. So, this work extends the work of Di Luna \\textit{et al}. [DISC'2020] under \\textsc{ASync} scheduler for $\\mathcal{FSTA}$ robot model.",
    "Link": "http://arxiv.org/abs/2302.07600v1",
    "PDF Link": "http://arxiv.org/pdf/2302.07600v1"
  },
  {
    "Title": "Safe Spot: Perceived safety of dominant and submissive appearances of\n  quadruped robots in human-robot interactions",
    "Authors": "Nanami Hashimoto, Emma Hagens, Arkady Zgonnikov, Maria Luce Lupetti",
    "Published": "2024-03-08T15:53:37Z",
    "Summary": "Unprecedented possibilities of quadruped robots have driven much research on the technical aspects of these robots. However, the social perception and acceptability of quadruped robots so far remain poorly understood. This work investigates whether the way we design quadruped robots' behaviors can affect people's perception of safety in interactions with these robots. We designed and tested a dominant and submissive personality for the quadruped robot (Boston Dynamics Spot). These were tested in two different walking scenarios (head-on and crossing interactions) in a 2x2 within-subjects study. We collected both behavioral data and subjective reports on participants' perception of the interaction. The results highlight that participants perceived the submissive robot as safer compared to the dominant one. The behavioral dynamics of interactions did not change depending on the robot's appearance. Participants' previous in-person experience with the robot was associated with lower subjective safety ratings but did not correlate with the interaction dynamics. Our findings have implications for the design of quadruped robots and contribute to the body of knowledge on the social perception of non-humanoid robots. We call for a stronger standing of felt experiences in human-robot interaction research.",
    "Link": "http://arxiv.org/abs/2403.05400v1",
    "PDF Link": "http://arxiv.org/pdf/2403.05400v1"
  },
  {
    "Title": "Embodiment in Socially Interactive Robots",
    "Authors": "Eric Deng, Bilge Mutlu, Maja Mataric",
    "Published": "2019-12-01T03:58:44Z",
    "Summary": "Physical embodiment is a required component for robots that are structurally coupled with their real-world environments. However, most socially interactive robots do not need to physically interact with their environments in order to perform their tasks. When and why should embodied robots be used instead of simpler and cheaper virtual agents? This paper reviews the existing work that explores the role of physical embodiment in socially interactive robots. This class consists of robots that are not only capable of engaging in social interaction with humans, but are using primarily their social capabilities to perform their desired functions. Socially interactive robots provide entertainment, information, and/or assistance; this last category is typically encompassed by socially assistive robotics. In all cases, such robots can achieve their primary functions without performing functional physical work. To comprehensively evaluate the existing body of work on embodiment, we first review work from established related fields including psychology, philosophy, and sociology. We then systematically review 65 studies evaluating aspects of embodiment published from 2003 to 2017 in major peer-reviewed robotics publication venues. We examine relevant aspects of the selected studies, focusing on the embodiments compared, tasks evaluated, social roles of robots, and measurements. We introduce three taxonomies for the types of robot embodiment, robot social roles, and human-robot tasks. These taxonomies are used to deconstruct the design and interaction spaces of socially interactive robots and facilitate analysis and discussion of the reviewed studies. We use this newly-defined methodology to critically discuss existing works, revealing topics within embodiment research for social interaction, assistive robotics, and service robotics.",
    "Link": "http://arxiv.org/abs/1912.00312v1",
    "PDF Link": "http://arxiv.org/pdf/1912.00312v1"
  },
  {
    "Title": "Should Collaborative Robots be Transparent?",
    "Authors": "Shahabedin Sagheb, Soham Gandhi, Dylan P. Losey",
    "Published": "2023-04-23T21:39:38Z",
    "Summary": "We often assume that robots which collaborate with humans should behave in ways that are transparent (e.g., legible, explainable). These transparent robots intentionally choose actions that convey their internal state to nearby humans: for instance, a transparent robot might exaggerate its trajectory to indicate its goal. But while transparent behavior seems beneficial for human-robot interaction, is it actually optimal? In this paper we consider collaborative settings where the human and robot have the same objective, and the human is uncertain about the robot's type (i.e., the robot's internal state). We extend a recursive combination of Bayesian Nash equilibrium and the Bellman equation to solve for optimal robot policies. Interestingly, we discover that it is not always optimal for collaborative robots to be transparent; instead, human and robot teams can sometimes achieve higher rewards when the robot is opaque. In contrast to transparent robots, opaque robots select actions that withhold information from the human. Our analysis suggests that opaque behavior becomes optimal when either (a) human-robot interactions have a short time horizon or (b) users are slow to learn from the robot's actions. We extend this theoretical analysis to user studies across 43 total participants in both online and in-person settings. We find that -- during short interactions -- users reach higher rewards when working with opaque partners, and subjectively rate opaque robots as about equal to transparent robots. See videos of our experiments here: https://youtu.be/u8q1Z7WHUuI",
    "Link": "http://arxiv.org/abs/2304.11753v2",
    "PDF Link": "http://arxiv.org/pdf/2304.11753v2"
  },
  {
    "Title": "Teaching Software Engineering through Robotics",
    "Authors": "Jiwon Shin, Andrey Rusakov, Bertrand Meyer",
    "Published": "2014-06-17T18:08:43Z",
    "Summary": "This paper presents a newly-developed robotics programming course and reports the initial results of software engineering education in robotics context. Robotics programming, as a multidisciplinary course, puts equal emphasis on software engineering and robotics. It teaches students proper software engineering -- in particular, modularity and documentation -- by having them implement four core robotics algorithms for an educational robot. To evaluate the effect of software engineering education in robotics context, we analyze pre- and post-class survey data and the four assignments our students completed for the course. The analysis suggests that the students acquired an understanding of software engineering techniques and principles.",
    "Link": "http://arxiv.org/abs/1406.4458v1",
    "PDF Link": "http://arxiv.org/pdf/1406.4458v1"
  },
  {
    "Title": "Lecture Notes: Neural Network Architectures",
    "Authors": "Evelyn Herberg",
    "Published": "2023-04-11T10:54:36Z",
    "Summary": "These lecture notes provide an overview of Neural Network architectures from a mathematical point of view. Especially, Machine Learning with Neural Networks is seen as an optimization problem. Covered are an introduction to Neural Networks and the following architectures: Feedforward Neural Network, Convolutional Neural Network, ResNet, and Recurrent Neural Network.",
    "Link": "http://arxiv.org/abs/2304.05133v2",
    "PDF Link": "http://arxiv.org/pdf/2304.05133v2"
  },
  {
    "Title": "Self-Organizing Multilayered Neural Networks of Optimal Complexity",
    "Authors": "V. Schetinin",
    "Published": "2005-04-13T13:59:55Z",
    "Summary": "The principles of self-organizing the neural networks of optimal complexity is considered under the unrepresentative learning set. The method of self-organizing the multi-layered neural networks is offered and used to train the logical neural networks which were applied to the medical diagnostics.",
    "Link": "http://arxiv.org/abs/cs/0504056v1",
    "PDF Link": "http://arxiv.org/pdf/cs/0504056v1"
  },
  {
    "Title": "Neural Network Processing Neural Networks: An efficient way to learn\n  higher order functions",
    "Authors": "Firat Tuna",
    "Published": "2019-11-06T19:15:34Z",
    "Summary": "Functions are rich in meaning and can be interpreted in a variety of ways. Neural networks were proven to be capable of approximating a large class of functions[1]. In this paper, we propose a new class of neural networks called \"Neural Network Processing Neural Networks\" (NNPNNs), which inputs neural networks and numerical values, instead of just numerical values. Thus enabling neural networks to represent and process rich structures.",
    "Link": "http://arxiv.org/abs/1911.05640v2",
    "PDF Link": "http://arxiv.org/pdf/1911.05640v2"
  },
  {
    "Title": "Guaranteed Quantization Error Computation for Neural Network Model\n  Compression",
    "Authors": "Wesley Cooke, Zihao Mo, Weiming Xiang",
    "Published": "2023-04-26T20:21:54Z",
    "Summary": "Neural network model compression techniques can address the computation issue of deep neural networks on embedded devices in industrial systems. The guaranteed output error computation problem for neural network compression with quantization is addressed in this paper. A merged neural network is built from a feedforward neural network and its quantized version to produce the exact output difference between two neural networks. Then, optimization-based methods and reachability analysis methods are applied to the merged neural network to compute the guaranteed quantization error. Finally, a numerical example is proposed to validate the applicability and effectiveness of the proposed approach.",
    "Link": "http://arxiv.org/abs/2304.13812v1",
    "PDF Link": "http://arxiv.org/pdf/2304.13812v1"
  },
  {
    "Title": "Graph Structure of Neural Networks",
    "Authors": "Jiaxuan You, Jure Leskovec, Kaiming He, Saining Xie",
    "Published": "2020-07-13T17:59:31Z",
    "Summary": "Neural networks are often represented as graphs of connections between neurons. However, despite their wide use, there is currently little understanding of the relationship between the graph structure of the neural network and its predictive performance. Here we systematically investigate how does the graph structure of neural networks affect their predictive performance. To this end, we develop a novel graph-based representation of neural networks called relational graph, where layers of neural network computation correspond to rounds of message exchange along the graph structure. Using this representation we show that: (1) a \"sweet spot\" of relational graphs leads to neural networks with significantly improved predictive performance; (2) neural network's performance is approximately a smooth function of the clustering coefficient and average path length of its relational graph; (3) our findings are consistent across many different tasks and datasets; (4) the sweet spot can be identified efficiently; (5) top-performing neural networks have graph structure surprisingly similar to those of real biological neural networks. Our work opens new directions for the design of neural architectures and the understanding on neural networks in general.",
    "Link": "http://arxiv.org/abs/2007.06559v2",
    "PDF Link": "http://arxiv.org/pdf/2007.06559v2"
  },
  {
    "Title": "Hybrid Quantum-Classical Neural Networks for Downlink Beamforming\n  Optimization",
    "Authors": "Juping Zhang, Gan Zheng, Toshiaki Koike-Akino, Kai-Kit Wong, Fraser Burton",
    "Published": "2024-08-08T20:14:39Z",
    "Summary": "This paper investigates quantum machine learning to optimize the beamforming in a multiuser multiple-input single-output downlink system. We aim to combine the power of quantum neural networks and the success of classical deep neural networks to enhance the learning performance. Specifically, we propose two hybrid quantum-classical neural networks to maximize the sum rate of a downlink system. The first one proposes a quantum neural network employing parameterized quantum circuits that follows a classical convolutional neural network. The classical neural network can be jointly trained with the quantum neural network or pre-trained leading to a fine-tuning transfer learning method. The second one designs a quantum convolutional neural network to better extract features followed by a classical deep neural network. Our results demonstrate the feasibility of the proposed hybrid neural networks, and reveal that the first method can achieve similar sum rate performance compared to a benchmark classical neural network with significantly less training parameters; while the second method can achieve higher sum rate especially in presence of many users still with less training parameters. The robustness of the proposed methods is verified using both software simulators and hardware emulators considering noisy intermediate-scale quantum devices.",
    "Link": "http://arxiv.org/abs/2408.04747v1",
    "PDF Link": "http://arxiv.org/pdf/2408.04747v1"
  },
  {
    "Title": "Cortex Neural Network: learning with Neural Network groups",
    "Authors": "Liyao Gao",
    "Published": "2018-04-10T02:33:47Z",
    "Summary": "Neural Network has been successfully applied to many real-world problems, such as image recognition and machine translation. However, for the current architecture of neural networks, it is hard to perform complex cognitive tasks, for example, to process the image and audio inputs together. Cortex, as an important architecture in the brain, is important for animals to perform the complex cognitive task. We view the architecture of Cortex in the brain as a missing part in the design of the current artificial neural network. In this paper, we purpose Cortex Neural Network (CrtxNN). The Cortex Neural Network is an upper architecture of neural networks which motivated from cerebral cortex in the brain to handle different tasks in the same learning system. It is able to identify different tasks and solve them with different methods. In our implementation, the Cortex Neural Network is able to process different cognitive tasks and perform reflection to get a higher accuracy. We provide a series of experiments to examine the capability of the cortex architecture on traditional neural networks. Our experiments proved its ability on the Cortex Neural Network can reach accuracy by 98.32% on MNIST and 62% on CIFAR10 at the same time, which can promisingly reduce the loss by 40%.",
    "Link": "http://arxiv.org/abs/1804.03313v1",
    "PDF Link": "http://arxiv.org/pdf/1804.03313v1"
  },
  {
    "Title": "Parametrical Neural Networks and Some Other Similar Architectures",
    "Authors": "Leonid B. Litinskii",
    "Published": "2006-08-18T08:28:23Z",
    "Summary": "A review of works on associative neural networks accomplished during last four years in the Institute of Optical Neural Technologies RAS is given. The presentation is based on description of parametrical neural networks (PNN). For today PNN have record recognizing characteristics (storage capacity, noise immunity and speed of operation). Presentation of basic ideas and principles is accentuated.",
    "Link": "http://arxiv.org/abs/cs/0608073v1",
    "PDF Link": "http://arxiv.org/pdf/cs/0608073v1"
  },
  {
    "Title": "Assessing Intelligence in Artificial Neural Networks",
    "Authors": "Nicholas J. Schaub, Nathan Hotaling",
    "Published": "2020-06-03T16:45:42Z",
    "Summary": "The purpose of this work was to develop of metrics to assess network architectures that balance neural network size and task performance. To this end, the concept of neural efficiency is introduced to measure neural layer utilization, and a second metric called artificial intelligence quotient (aIQ) was created to balance neural network performance and neural network efficiency. To study aIQ and neural efficiency, two simple neural networks were trained on MNIST: a fully connected network (LeNet-300-100) and a convolutional neural network (LeNet-5). The LeNet-5 network with the highest aIQ was 2.32% less accurate but contained 30,912 times fewer parameters than the highest accuracy network. Both batch normalization and dropout layers were found to increase neural efficiency. Finally, high aIQ networks are shown to be memorization and overtraining resistant, capable of learning proper digit classification with an accuracy of 92.51% even when 75% of the class labels are randomized. These results demonstrate the utility of aIQ and neural efficiency as metrics for balancing network performance and size.",
    "Link": "http://arxiv.org/abs/2006.02909v1",
    "PDF Link": "http://arxiv.org/pdf/2006.02909v1"
  },
  {
    "Title": "Rational Neural Network Controllers",
    "Authors": "Matthew Newton, Antonis Papachristodoulou",
    "Published": "2023-07-12T16:35:41Z",
    "Summary": "Neural networks have shown great success in many machine learning related tasks, due to their ability to act as general function approximators. Recent work has demonstrated the effectiveness of neural networks in control systems (known as neural feedback loops), most notably by using a neural network as a controller. However, one of the big challenges of this approach is that neural networks have been shown to be sensitive to adversarial attacks. This means that, unless they are designed properly, they are not an ideal candidate for controllers due to issues with robustness and uncertainty, which are pivotal aspects of control systems. There has been initial work on robustness to both analyse and design dynamical systems with neural network controllers. However, one prominent issue with these methods is that they use existing neural network architectures tailored for traditional machine learning tasks. These structures may not be appropriate for neural network controllers and it is important to consider alternative architectures. This paper considers rational neural networks and presents novel rational activation functions, which can be used effectively in robustness problems for neural feedback loops. Rational activation functions are replaced by a general rational neural network structure, which is convex in the neural network's parameters. A method is proposed to recover a stabilising controller from a Sum of Squares feasibility test. This approach is then applied to a refined rational neural network which is more compatible with Sum of Squares programming. Numerical examples show that this method can successfully recover stabilising rational neural network controllers for neural feedback loops with non-linear plants with noise and parametric uncertainty.",
    "Link": "http://arxiv.org/abs/2307.06287v1",
    "PDF Link": "http://arxiv.org/pdf/2307.06287v1"
  },
  {
    "Title": "Asymptotic Theory of Expectile Neural Networks",
    "Authors": "Jinghang Lin, Xiaoxi Shen, Qing Lu",
    "Published": "2020-10-31T01:14:04Z",
    "Summary": "Neural networks are becoming an increasingly important tool in applications. However, neural networks are not widely used in statistical genetics. In this paper, we propose a new neural networks method called expectile neural networks. When the size of parameter is too large, the standard maximum likelihood procedures may not work. We use sieve method to constrain parameter space. And we prove its consistency and normality under nonparametric regression framework.",
    "Link": "http://arxiv.org/abs/2011.01218v1",
    "PDF Link": "http://arxiv.org/pdf/2011.01218v1"
  },
  {
    "Title": "Combining Recurrent and Convolutional Neural Networks for Relation\n  Classification",
    "Authors": "Ngoc Thang Vu, Heike Adel, Pankaj Gupta, Hinrich Schütze",
    "Published": "2016-05-24T08:20:12Z",
    "Summary": "This paper investigates two different neural architectures for the task of relation classification: convolutional neural networks and recurrent neural networks. For both models, we demonstrate the effect of different architectural choices. We present a new context representation for convolutional neural networks for relation classification (extended middle context). Furthermore, we propose connectionist bi-directional recurrent neural networks and introduce ranking loss for their optimization. Finally, we show that combining convolutional and recurrent neural networks using a simple voting scheme is accurate enough to improve results. Our neural models achieve state-of-the-art results on the SemEval 2010 relation classification task.",
    "Link": "http://arxiv.org/abs/1605.07333v1",
    "PDF Link": "http://arxiv.org/pdf/1605.07333v1"
  },
  {
    "Title": "A Comprehensive Review of Spiking Neural Networks: Interpretation,\n  Optimization, Efficiency, and Best Practices",
    "Authors": "Kai Malcolm, Josue Casco-Rodriguez",
    "Published": "2023-03-19T22:07:27Z",
    "Summary": "Biological neural networks continue to inspire breakthroughs in neural network performance. And yet, one key area of neural computation that has been under-appreciated and under-investigated is biologically plausible, energy-efficient spiking neural networks, whose potential is especially attractive for low-power, mobile, or otherwise hardware-constrained settings. We present a literature review of recent developments in the interpretation, optimization, efficiency, and accuracy of spiking neural networks. Key contributions include identification, discussion, and comparison of cutting-edge methods in spiking neural network optimization, energy-efficiency, and evaluation, starting from first principles so as to be accessible to new practitioners.",
    "Link": "http://arxiv.org/abs/2303.10780v2",
    "PDF Link": "http://arxiv.org/pdf/2303.10780v2"
  },
  {
    "Title": "Design and development of opto-neural processors for simulation of\n  neural networks trained in image detection for potential implementation in\n  hybrid robotics",
    "Authors": "Sanjana Shetty",
    "Published": "2024-01-17T04:42:49Z",
    "Summary": "Neural networks have been employed for a wide range of processing applications like image processing, motor control, object detection and many others. Living neural networks offer advantages of lower power consumption, faster processing, and biological realism. Optogenetics offers high spatial and temporal control over biological neurons and presents potential in training live neural networks. This work proposes a simulated living neural network trained indirectly by backpropagating STDP based algorithms using precision activation by optogenetics achieving accuracy comparable to traditional neural network training algorithms.",
    "Link": "http://arxiv.org/abs/2401.10289v1",
    "PDF Link": "http://arxiv.org/pdf/2401.10289v1"
  },
  {
    "Title": "Convex Formulation of Overparameterized Deep Neural Networks",
    "Authors": "Cong Fang, Yihong Gu, Weizhong Zhang, Tong Zhang",
    "Published": "2019-11-18T13:42:04Z",
    "Summary": "Analysis of over-parameterized neural networks has drawn significant attention in recentyears. It was shown that such systems behave like convex systems under various restrictedsettings, such as for two-level neural networks, and when learning is only restricted locally inthe so-called neural tangent kernel space around specialized initializations. However, there areno theoretical techniques that can analyze fully trained deep neural networks encountered inpractice. This paper solves this fundamental problem by investigating such overparameterizeddeep neural networks when fully trained. We generalize a new technique called neural feature repopulation, originally introduced in (Fang et al., 2019a) for two-level neural networks, to analyze deep neural networks. It is shown that under suitable representations, overparameterized deep neural networks are inherently convex, and when optimized, the system can learn effective features suitable for the underlying learning task under mild conditions. This new analysis is consistent with empirical observations that deep neural networks are capable of learning efficient feature representations. Therefore, the highly unexpected result of this paper can satisfactorily explain the practical success of deep neural networks. Empirical studies confirm that predictions of our theory are consistent with results observed in practice.",
    "Link": "http://arxiv.org/abs/1911.07626v1",
    "PDF Link": "http://arxiv.org/pdf/1911.07626v1"
  },
  {
    "Title": "Approximate Bisimulation Relations for Neural Networks and Application\n  to Assured Neural Network Compression",
    "Authors": "Weiming Xiang, Zhongzhu Shao",
    "Published": "2022-02-02T16:21:19Z",
    "Summary": "In this paper, we propose a concept of approximate bisimulation relation for feedforward neural networks. In the framework of approximate bisimulation relation, a novel neural network merging method is developed to compute the approximate bisimulation error between two neural networks based on reachability analysis of neural networks. The developed method is able to quantitatively measure the distance between the outputs of two neural networks with the same inputs. Then, we apply the approximate bisimulation relation results to perform neural networks model reduction and compute the compression precision, i.e., assured neural networks compression. At last, using the assured neural network compression, we accelerate the verification processes of ACAS Xu neural networks to illustrate the effectiveness and advantages of our proposed approximate bisimulation approach.",
    "Link": "http://arxiv.org/abs/2202.01214v1",
    "PDF Link": "http://arxiv.org/pdf/2202.01214v1"
  },
  {
    "Title": "Optimal rates of approximation by shallow ReLU$^k$ neural networks and\n  applications to nonparametric regression",
    "Authors": "Yunfei Yang, Ding-Xuan Zhou",
    "Published": "2023-04-04T06:35:02Z",
    "Summary": "We study the approximation capacity of some variation spaces corresponding to shallow ReLU$^k$ neural networks. It is shown that sufficiently smooth functions are contained in these spaces with finite variation norms. For functions with less smoothness, the approximation rates in terms of the variation norm are established. Using these results, we are able to prove the optimal approximation rates in terms of the number of neurons for shallow ReLU$^k$ neural networks. It is also shown how these results can be used to derive approximation bounds for deep neural networks and convolutional neural networks (CNNs). As applications, we study convergence rates for nonparametric regression using three ReLU neural network models: shallow neural network, over-parameterized neural network, and CNN. In particular, we show that shallow neural networks can achieve the minimax optimal rates for learning H\\\"older functions, which complements recent results for deep neural networks. It is also proven that over-parameterized (deep or shallow) neural networks can achieve nearly optimal rates for nonparametric regression.",
    "Link": "http://arxiv.org/abs/2304.01561v3",
    "PDF Link": "http://arxiv.org/pdf/2304.01561v3"
  },
  {
    "Title": "Understanding Vector-Valued Neural Networks and Their Relationship with\n  Real and Hypercomplex-Valued Neural Networks",
    "Authors": "Marcos Eduardo Valle",
    "Published": "2023-09-14T13:48:16Z",
    "Summary": "Despite the many successful applications of deep learning models for multidimensional signal and image processing, most traditional neural networks process data represented by (multidimensional) arrays of real numbers. The intercorrelation between feature channels is usually expected to be learned from the training data, requiring numerous parameters and careful training. In contrast, vector-valued neural networks are conceived to process arrays of vectors and naturally consider the intercorrelation between feature channels. Consequently, they usually have fewer parameters and often undergo more robust training than traditional neural networks. This paper aims to present a broad framework for vector-valued neural networks, referred to as V-nets. In this context, hypercomplex-valued neural networks are regarded as vector-valued models with additional algebraic properties. Furthermore, this paper explains the relationship between vector-valued and traditional neural networks. Precisely, a vector-valued neural network can be obtained by placing restrictions on a real-valued model to consider the intercorrelation between feature channels. Finally, we show how V-nets, including hypercomplex-valued neural networks, can be implemented in current deep-learning libraries as real-valued networks.",
    "Link": "http://arxiv.org/abs/2309.07716v2",
    "PDF Link": "http://arxiv.org/pdf/2309.07716v2"
  },
  {
    "Title": "One weird trick for parallelizing convolutional neural networks",
    "Authors": "Alex Krizhevsky",
    "Published": "2014-04-23T22:37:56Z",
    "Summary": "I present a new way to parallelize the training of convolutional neural networks across multiple GPUs. The method scales significantly better than all alternatives when applied to modern convolutional neural networks.",
    "Link": "http://arxiv.org/abs/1404.5997v2",
    "PDF Link": "http://arxiv.org/pdf/1404.5997v2"
  },
  {
    "Title": "Nonlinear Systems Identification Using Deep Dynamic Neural Networks",
    "Authors": "Olalekan Ogunmolu, Xuejun Gu, Steve Jiang, Nicholas Gans",
    "Published": "2016-10-05T14:26:27Z",
    "Summary": "Neural networks are known to be effective function approximators. Recently, deep neural networks have proven to be very effective in pattern recognition, classification tasks and human-level control to model highly nonlinear realworld systems. This paper investigates the effectiveness of deep neural networks in the modeling of dynamical systems with complex behavior. Three deep neural network structures are trained on sequential data, and we investigate the effectiveness of these networks in modeling associated characteristics of the underlying dynamical systems. We carry out similar evaluations on select publicly available system identification datasets. We demonstrate that deep neural networks are effective model estimators from input-output data",
    "Link": "http://arxiv.org/abs/1610.01439v1",
    "PDF Link": "http://arxiv.org/pdf/1610.01439v1"
  },
  {
    "Title": "Geometric Decomposition of Feed Forward Neural Networks",
    "Authors": "Sven Cattell",
    "Published": "2016-12-08T03:28:10Z",
    "Summary": "There have been several attempts to mathematically understand neural networks and many more from biological and computational perspectives. The field has exploded in the last decade, yet neural networks are still treated much like a black box. In this work we describe a structure that is inherent to a feed forward neural network. This will provide a framework for future work on neural networks to improve training algorithms, compute the homology of the network, and other applications. Our approach takes a more geometric point of view and is unlike other attempts to mathematically understand neural networks that rely on a functional perspective.",
    "Link": "http://arxiv.org/abs/1612.02522v1",
    "PDF Link": "http://arxiv.org/pdf/1612.02522v1"
  },
  {
    "Title": "Neural Networks Architecture Evaluation in a Quantum Computer",
    "Authors": "Adenilton José da Silva, Rodolfo Luan F. de Oliveira",
    "Published": "2017-11-13T18:50:04Z",
    "Summary": "In this work, we propose a quantum algorithm to evaluate neural networks architectures named Quantum Neural Network Architecture Evaluation (QNNAE). The proposed algorithm is based on a quantum associative memory and the learning algorithm for artificial neural networks. Unlike conventional algorithms for evaluating neural network architectures, QNNAE does not depend on initialization of weights. The proposed algorithm has a binary output and results in 0 with probability proportional to the performance of the network. And its computational cost is equal to the computational cost to train a neural network.",
    "Link": "http://arxiv.org/abs/1711.04759v1",
    "PDF Link": "http://arxiv.org/pdf/1711.04759v1"
  },
  {
    "Title": "Building Compact and Robust Deep Neural Networks with Toeplitz Matrices",
    "Authors": "Alexandre Araujo",
    "Published": "2021-09-02T13:58:12Z",
    "Summary": "Deep neural networks are state-of-the-art in a wide variety of tasks, however, they exhibit important limitations which hinder their use and deployment in real-world applications. When developing and training neural networks, the accuracy should not be the only concern, neural networks must also be cost-effective and reliable. Although accurate, large neural networks often lack these properties. This thesis focuses on the problem of training neural networks which are not only accurate but also compact, easy to train, reliable and robust to adversarial examples. To tackle these problems, we leverage the properties of structured matrices from the Toeplitz family to build compact and secure neural networks.",
    "Link": "http://arxiv.org/abs/2109.00959v1",
    "PDF Link": "http://arxiv.org/pdf/2109.00959v1"
  },
  {
    "Title": "Application of Neural Network in Optimization of Chemical Process",
    "Authors": "Fei Liang, Taowen Zhang",
    "Published": "2021-10-11T00:31:00Z",
    "Summary": "Artificial neural network (ANN) has been widely used due to its strong nonlinear mapping ability, fault tolerance and self-learning ability. This article summarizes the development history of artificial neural networks, introduces three common neural network types, BP neural network, RBF neural network and convolutional neural network, and focuses on the practical application in chemical process optimization, especially the results achieved in multi-objective control optimization and process parameter improvement.",
    "Link": "http://arxiv.org/abs/2110.04942v1",
    "PDF Link": "http://arxiv.org/pdf/2110.04942v1"
  },
  {
    "Title": "Compact Matrix Quantum Group Equivariant Neural Networks",
    "Authors": "Edward Pearce-Crump",
    "Published": "2023-11-10T19:11:13Z",
    "Summary": "We derive the existence of a new type of neural network, called a compact matrix quantum group equivariant neural network, that learns from data that has an underlying quantum symmetry. We apply the Woronowicz formulation of Tannaka-Krein duality to characterise the weight matrices that appear in these neural networks for any easy compact matrix quantum group. We show that compact matrix quantum group equivariant neural networks contain, as a subclass, all compact matrix group equivariant neural networks. Moreover, we obtain characterisations of the weight matrices for many compact matrix group equivariant neural networks that have not previously appeared in the machine learning literature.",
    "Link": "http://arxiv.org/abs/2311.06358v1",
    "PDF Link": "http://arxiv.org/pdf/2311.06358v1"
  },
  {
    "Title": "Universal Approximation Theorem for Vector- and Hypercomplex-Valued\n  Neural Networks",
    "Authors": "Marcos Eduardo Valle, Wington L. Vital, Guilherme Vieira",
    "Published": "2024-01-04T13:56:13Z",
    "Summary": "The universal approximation theorem states that a neural network with one hidden layer can approximate continuous functions on compact sets with any desired precision. This theorem supports using neural networks for various applications, including regression and classification tasks. Furthermore, it is valid for real-valued neural networks and some hypercomplex-valued neural networks such as complex-, quaternion-, tessarine-, and Clifford-valued neural networks. However, hypercomplex-valued neural networks are a type of vector-valued neural network defined on an algebra with additional algebraic or geometric properties. This paper extends the universal approximation theorem for a wide range of vector-valued neural networks, including hypercomplex-valued models as particular instances. Precisely, we introduce the concept of non-degenerate algebra and state the universal approximation theorem for neural networks defined on such algebras.",
    "Link": "http://arxiv.org/abs/2401.02277v2",
    "PDF Link": "http://arxiv.org/pdf/2401.02277v2"
  },
  {
    "Title": "Detecting Neural Trojans Through Merkle Trees",
    "Authors": "Joshua Strubel",
    "Published": "2023-06-08T17:09:59Z",
    "Summary": "Deep neural networks are utilized in a growing number of industries. Much of the current literature focuses on the applications of deep neural networks without discussing the security of the network itself. One security issue facing deep neural networks is neural trojans. Through a neural trojan, a malicious actor may force the deep neural network to act in unintended ways. Several potential defenses have been proposed, but they are computationally expensive, complex, or unusable in commercial applications. We propose Merkle trees as a novel way to detect and isolate neural trojans.",
    "Link": "http://arxiv.org/abs/2306.05368v1",
    "PDF Link": "http://arxiv.org/pdf/2306.05368v1"
  },
  {
    "Title": "Performance Analysis Of Neural Network Models For Oxazolines And\n  Oxazoles Derivatives Descriptor Dataset",
    "Authors": "Doreswamy, Chanabasayya . M. Vastrad",
    "Published": "2013-12-10T16:15:48Z",
    "Summary": "Neural networks have been used successfully to a broad range of areas such as business, data mining, drug discovery and biology. In medicine, neural networks have been applied widely in medical diagnosis, detection and evaluation of new drugs and treatment cost estimation. In addition, neural networks have begin practice in data mining strategies for the aim of prediction, knowledge discovery. This paper will present the application of neural networks for the prediction and analysis of antitubercular activity of Oxazolines and Oxazoles derivatives. This study presents techniques based on the development of Single hidden layer neural network (SHLFFNN), Gradient Descent Back propagation neural network (GDBPNN), Gradient Descent Back propagation with momentum neural network (GDBPMNN), Back propagation with Weight decay neural network (BPWDNN) and Quantile regression neural network (QRNN) of artificial neural network (ANN) models Here, we comparatively evaluate the performance of five neural network techniques. The evaluation of the efficiency of each model by ways of benchmark experiments is an accepted application. Cross-validation and resampling techniques are commonly used to derive point estimates of the performances which are compared to identify methods with good properties. Predictive accuracy was evaluated using the root mean squared error (RMSE), Coefficient determination(???), mean absolute error(MAE), mean percentage error(MPE) and relative square error(RSE). We found that all five neural network models were able to produce feasible models. QRNN model is outperforms with all statistical tests amongst other four models.",
    "Link": "http://arxiv.org/abs/1312.2853v1",
    "PDF Link": "http://arxiv.org/pdf/1312.2853v1"
  },
  {
    "Title": "Why Quantization Improves Generalization: NTK of Binary Weight Neural\n  Networks",
    "Authors": "Kaiqi Zhang, Ming Yin, Yu-Xiang Wang",
    "Published": "2022-06-13T06:11:21Z",
    "Summary": "Quantized neural networks have drawn a lot of attention as they reduce the space and computational complexity during the inference. Moreover, there has been folklore that quantization acts as an implicit regularizer and thus can improve the generalizability of neural networks, yet no existing work formalizes this interesting folklore. In this paper, we take the binary weights in a neural network as random variables under stochastic rounding, and study the distribution propagation over different layers in the neural network. We propose a quasi neural network to approximate the distribution propagation, which is a neural network with continuous parameters and smooth activation function. We derive the neural tangent kernel (NTK) for this quasi neural network, and show that the eigenvalue of NTK decays at approximately exponential rate, which is comparable to that of Gaussian kernel with randomized scale. This in turn indicates that the Reproducing Kernel Hilbert Space (RKHS) of a binary weight neural network covers a strict subset of functions compared with the one with real value weights. We use experiments to verify that the quasi neural network we proposed can well approximate binary weight neural network. Furthermore, binary weight neural network gives a lower generalization gap compared with real value weight neural network, which is similar to the difference between Gaussian kernel and Laplace kernel.",
    "Link": "http://arxiv.org/abs/2206.05916v1",
    "PDF Link": "http://arxiv.org/pdf/2206.05916v1"
  },
  {
    "Title": "Bayesian Neural Networks: Essentials",
    "Authors": "Daniel T. Chang",
    "Published": "2021-06-22T13:54:17Z",
    "Summary": "Bayesian neural networks utilize probabilistic layers that capture uncertainty over weights and activations, and are trained using Bayesian inference. Since these probabilistic layers are designed to be drop-in replacement of their deterministic counter parts, Bayesian neural networks provide a direct and natural way to extend conventional deep neural networks to support probabilistic deep learning. However, it is nontrivial to understand, design and train Bayesian neural networks due to their complexities. We discuss the essentials of Bayesian neural networks including duality (deep neural networks, probabilistic models), approximate Bayesian inference, Bayesian priors, Bayesian posteriors, and deep variational learning. We use TensorFlow Probability APIs and code examples for illustration. The main problem with Bayesian neural networks is that the architecture of deep neural networks makes it quite redundant, and costly, to account for uncertainty for a large number of successive layers. Hybrid Bayesian neural networks, which use few probabilistic layers judicially positioned in the networks, provide a practical solution.",
    "Link": "http://arxiv.org/abs/2106.13594v1",
    "PDF Link": "http://arxiv.org/pdf/2106.13594v1"
  },
  {
    "Title": "Fourier Neural Networks for Function Approximation",
    "Authors": "R Subhash Chandra Bose, Kakarla Yaswanth",
    "Published": "2021-10-21T09:30:26Z",
    "Summary": "The success of Neural networks in providing miraculous results when applied to a wide variety of tasks is astonishing. Insight in the working can be obtained by studying the universal approximation property of neural networks. It is proved extensively that neural networks are universal approximators. Further it is proved that deep Neural networks are better approximators. It is specifically proved that for a narrow neural network to approximate a function which is otherwise implemented by a deep Neural network, the network take exponentially large number of neurons. In this work, we have implemented existing methodologies for a variety of synthetic functions and identified their deficiencies. Further, we examined that Fourier neural network is able to perform fairly good with only two layers in the neural network. A modified Fourier Neural network which has sinusoidal activation and two hidden layer is proposed and the results are tabulated.",
    "Link": "http://arxiv.org/abs/2111.08438v1",
    "PDF Link": "http://arxiv.org/pdf/2111.08438v1"
  },
  {
    "Title": "Genetic cellular neural networks for generating three-dimensional\n  geometry",
    "Authors": "Hugo Martay",
    "Published": "2016-03-28T20:28:09Z",
    "Summary": "There are a number of ways to procedurally generate interesting three-dimensional shapes, and a method where a cellular neural network is combined with a mesh growth algorithm is presented here. The aim is to create a shape from a genetic code in such a way that a crude search can find interesting shapes. Identical neural networks are placed at each vertex of a mesh which can communicate with neural networks on neighboring vertices. The output of the neural networks determine how the mesh grows, allowing interesting shapes to be produced emergently, mimicking some of the complexity of biological organism development. Since the neural networks' parameters can be freely mutated, the approach is amenable for use in a genetic algorithm.",
    "Link": "http://arxiv.org/abs/1603.08551v1",
    "PDF Link": "http://arxiv.org/pdf/1603.08551v1"
  },
  {
    "Title": "General Regression Neural Networks, Radial Basis Function Neural\n  Networks, Support Vector Machines, and Feedforward Neural Networks",
    "Authors": "Alison Jenkins, Vinika Gupta, Mary Lenoir",
    "Published": "2019-11-16T23:31:26Z",
    "Summary": "The aim of this project is to develop a code to discover the optimal sigma value that maximum the F1 score and the optimal sigma value that maximizes the accuracy and to find out if they are the same. Four algorithms which can be used to solve this problem are: Genetic Regression Neural Networks (GRNNs), Radial Based Function (RBF) Neural Networks (RBFNNs), Support Vector Machines (SVMs) and Feedforward Neural Network (FFNNs).",
    "Link": "http://arxiv.org/abs/1911.07115v1",
    "PDF Link": "http://arxiv.org/pdf/1911.07115v1"
  },
  {
    "Title": "Survey of Dropout Methods for Deep Neural Networks",
    "Authors": "Alex Labach, Hojjat Salehinejad, Shahrokh Valaee",
    "Published": "2019-04-25T21:21:52Z",
    "Summary": "Dropout methods are a family of stochastic techniques used in neural network training or inference that have generated significant research interest and are widely used in practice. They have been successfully applied in neural network regularization, model compression, and in measuring the uncertainty of neural network outputs. While original formulated for dense neural network layers, recent advances have made dropout methods also applicable to convolutional and recurrent neural network layers. This paper summarizes the history of dropout methods, their various applications, and current areas of research interest. Important proposed methods are described in additional detail.",
    "Link": "http://arxiv.org/abs/1904.13310v2",
    "PDF Link": "http://arxiv.org/pdf/1904.13310v2"
  },
  {
    "Title": "On neural network kernels and the storage capacity problem",
    "Authors": "Jacob A. Zavatone-Veth, Cengiz Pehlevan",
    "Published": "2022-01-12T19:47:30Z",
    "Summary": "In this short note, we reify the connection between work on the storage capacity problem in wide two-layer treelike neural networks and the rapidly-growing body of literature on kernel limits of wide neural networks. Concretely, we observe that the \"effective order parameter\" studied in the statistical mechanics literature is exactly equivalent to the infinite-width Neural Network Gaussian Process Kernel. This correspondence connects the expressivity and trainability of wide two-layer neural networks.",
    "Link": "http://arxiv.org/abs/2201.04669v1",
    "PDF Link": "http://arxiv.org/pdf/2201.04669v1"
  },
  {
    "Title": "Unary Coding for Neural Network Learning",
    "Authors": "Subhash Kak",
    "Published": "2010-09-22T22:32:37Z",
    "Summary": "This paper presents some properties of unary coding of significance for biological learning and instantaneously trained neural networks.",
    "Link": "http://arxiv.org/abs/1009.4495v1",
    "PDF Link": "http://arxiv.org/pdf/1009.4495v1"
  },
  {
    "Title": "Deep Neural Networks - A Brief History",
    "Authors": "Krzysztof J. Cios",
    "Published": "2017-01-19T18:43:56Z",
    "Summary": "Introduction to deep neural networks and their history.",
    "Link": "http://arxiv.org/abs/1701.05549v1",
    "PDF Link": "http://arxiv.org/pdf/1701.05549v1"
  },
  {
    "Title": "GPU Acceleration of Sparse Neural Networks",
    "Authors": "Aavaas Gajurel, Sushil J. Louis, Frederick C Harris",
    "Published": "2020-05-09T02:18:31Z",
    "Summary": "In this paper, we use graphics processing units(GPU) to accelerate sparse and arbitrary structured neural networks. Sparse networks have nodes in the network that are not fully connected with nodes in preceding and following layers, and arbitrary structure neural networks have different number of nodes in each layers. Sparse Neural networks with arbitrary structures are generally created in the processes like neural network pruning and evolutionary machine learning strategies. We show that we can gain significant speedup for full activation of such neural networks using graphical processing units. We do a prepossessing step to determine dependency groups for all the nodes in a network, and use that information to guide the progression of activation in the neural network. Then we compute activation for each nodes in its own separate thread in the GPU, which allows for massive parallelization. We use CUDA framework to implement our approach and compare the results of sequential and GPU implementations. Our results show that the activation of sparse neural networks lends very well to GPU acceleration and can help speed up machine learning strategies which generate such networks or other processes that have similar structure.",
    "Link": "http://arxiv.org/abs/2005.04347v1",
    "PDF Link": "http://arxiv.org/pdf/2005.04347v1"
  },
  {
    "Title": "Neural Network Pruning as Spectrum Preserving Process",
    "Authors": "Shibo Yao, Dantong Yu, Ioannis Koutis",
    "Published": "2023-07-18T05:39:32Z",
    "Summary": "Neural networks have achieved remarkable performance in various application domains. Nevertheless, a large number of weights in pre-trained deep neural networks prohibit them from being deployed on smartphones and embedded systems. It is highly desirable to obtain lightweight versions of neural networks for inference in edge devices. Many cost-effective approaches were proposed to prune dense and convolutional layers that are common in deep neural networks and dominant in the parameter space. However, a unified theoretical foundation for the problem mostly is missing. In this paper, we identify the close connection between matrix spectrum learning and neural network training for dense and convolutional layers and argue that weight pruning is essentially a matrix sparsification process to preserve the spectrum. Based on the analysis, we also propose a matrix sparsification algorithm tailored for neural network pruning that yields better pruning result. We carefully design and conduct experiments to support our arguments. Hence we provide a consolidated viewpoint for neural network pruning and enhance the interpretability of deep neural networks by identifying and preserving the critical neural weights.",
    "Link": "http://arxiv.org/abs/2307.08982v1",
    "PDF Link": "http://arxiv.org/pdf/2307.08982v1"
  },
  {
    "Title": "On Hiding Neural Networks Inside Neural Networks",
    "Authors": "Chuan Guo, Ruihan Wu, Kilian Q. Weinberger",
    "Published": "2020-02-24T05:18:29Z",
    "Summary": "Modern neural networks often contain significantly more parameters than the size of their training data. We show that this excess capacity provides an opportunity for embedding secret machine learning models within a trained neural network. Our novel framework hides the existence of a secret neural network with arbitrary desired functionality within a carrier network. We prove theoretically that the secret network's detection is computationally infeasible and demonstrate empirically that the carrier network does not compromise the secret network's disguise. Our paper introduces a previously unknown steganographic technique that can be exploited by adversaries if left unchecked.",
    "Link": "http://arxiv.org/abs/2002.10078v3",
    "PDF Link": "http://arxiv.org/pdf/2002.10078v3"
  },
  {
    "Title": "A New Constructive Method to Optimize Neural Network Architecture and\n  Generalization",
    "Authors": "Hou Muzhou, Moon Ho Lee",
    "Published": "2013-02-02T00:43:42Z",
    "Summary": "In this paper, after analyzing the reasons of poor generalization and overfitting in neural networks, we consider some noise data as a singular value of a continuous function - jump discontinuity point. The continuous part can be approximated with the simplest neural networks, which have good generalization performance and optimal network architecture, by traditional algorithms such as constructive algorithm for feed-forward neural networks with incremental training, BP algorithm, ELM algorithm, various constructive algorithm, RBF approximation and SVM. At the same time, we will construct RBF neural networks to fit the singular value with every error in, and we prove that a function with jumping discontinuity points can be approximated by the simplest neural networks with a decay RBF neural networks in by each error, and a function with jumping discontinuity point can be constructively approximated by a decay RBF neural networks in by each error and the constructive part have no generalization influence to the whole machine learning system which will optimize neural network architecture and generalization performance, reduce the overfitting phenomenon by avoid fitting the noisy data.",
    "Link": "http://arxiv.org/abs/1302.0324v1",
    "PDF Link": "http://arxiv.org/pdf/1302.0324v1"
  },
  {
    "Title": "Deep physical neural networks enabled by a backpropagation algorithm for\n  arbitrary physical systems",
    "Authors": "Logan G. Wright, Tatsuhiro Onodera, Martin M. Stein, Tianyu Wang, Darren T. Schachter, Zoey Hu, Peter L. McMahon",
    "Published": "2021-04-27T18:00:02Z",
    "Summary": "Deep neural networks have become a pervasive tool in science and engineering. However, modern deep neural networks' growing energy requirements now increasingly limit their scaling and broader use. We propose a radical alternative for implementing deep neural network models: Physical Neural Networks. We introduce a hybrid physical-digital algorithm called Physics-Aware Training to efficiently train sequences of controllable physical systems to act as deep neural networks. This method automatically trains the functionality of any sequence of real physical systems, directly, using backpropagation, the same technique used for modern deep neural networks. To illustrate their generality, we demonstrate physical neural networks with three diverse physical systems-optical, mechanical, and electrical. Physical neural networks may facilitate unconventional machine learning hardware that is orders of magnitude faster and more energy efficient than conventional electronic processors.",
    "Link": "http://arxiv.org/abs/2104.13386v1",
    "PDF Link": "http://arxiv.org/pdf/2104.13386v1"
  },
  {
    "Title": "Consistency of Neural Networks with Regularization",
    "Authors": "Xiaoxi Shen, Jinghang Lin",
    "Published": "2022-06-22T23:33:39Z",
    "Summary": "Neural networks have attracted a lot of attention due to its success in applications such as natural language processing and computer vision. For large scale data, due to the tremendous number of parameters in neural networks, overfitting is an issue in training neural networks. To avoid overfitting, one common approach is to penalize the parameters especially the weights in neural networks. Although neural networks has demonstrated its advantages in many applications, the theoretical foundation of penalized neural networks has not been well-established. Our goal of this paper is to propose the general framework of neural networks with regularization and prove its consistency. Under certain conditions, the estimated neural network will converge to true underlying function as the sample size increases. The method of sieves and the theory on minimal neural networks are used to overcome the issue of unidentifiability for the parameters. Two types of activation functions: hyperbolic tangent function(Tanh) and rectified linear unit(ReLU) have been taken into consideration. Simulations have been conducted to verify the validation of theorem of consistency.",
    "Link": "http://arxiv.org/abs/2207.01538v1",
    "PDF Link": "http://arxiv.org/pdf/2207.01538v1"
  },
  {
    "Title": "Understanding Weight Similarity of Neural Networks via Chain\n  Normalization Rule and Hypothesis-Training-Testing",
    "Authors": "Guangcong Wang, Guangrun Wang, Wenqi Liang, Jianhuang Lai",
    "Published": "2022-08-08T19:11:03Z",
    "Summary": "We present a weight similarity measure method that can quantify the weight similarity of non-convex neural networks. To understand the weight similarity of different trained models, we propose to extract the feature representation from the weights of neural networks. We first normalize the weights of neural networks by introducing a chain normalization rule, which is used for weight representation learning and weight similarity measure. We extend the traditional hypothesis-testing method to a hypothesis-training-testing statistical inference method to validate the hypothesis on the weight similarity of neural networks. With the chain normalization rule and the new statistical inference, we study the weight similarity measure on Multi-Layer Perceptron (MLP), Convolutional Neural Network (CNN), and Recurrent Neural Network (RNN), and find that the weights of an identical neural network optimized with the Stochastic Gradient Descent (SGD) algorithm converge to a similar local solution in a metric space. The weight similarity measure provides more insight into the local solutions of neural networks. Experiments on several datasets consistently validate the hypothesis of weight similarity measure.",
    "Link": "http://arxiv.org/abs/2208.04369v1",
    "PDF Link": "http://arxiv.org/pdf/2208.04369v1"
  },
  {
    "Title": "Graph Metanetworks for Processing Diverse Neural Architectures",
    "Authors": "Derek Lim, Haggai Maron, Marc T. Law, Jonathan Lorraine, James Lucas",
    "Published": "2023-12-07T18:21:52Z",
    "Summary": "Neural networks efficiently encode learned information within their parameters. Consequently, many tasks can be unified by treating neural networks themselves as input data. When doing so, recent studies demonstrated the importance of accounting for the symmetries and geometry of parameter spaces. However, those works developed architectures tailored to specific networks such as MLPs and CNNs without normalization layers, and generalizing such architectures to other types of networks can be challenging. In this work, we overcome these challenges by building new metanetworks - neural networks that take weights from other neural networks as input. Put simply, we carefully build graphs representing the input neural networks and process the graphs using graph neural networks. Our approach, Graph Metanetworks (GMNs), generalizes to neural architectures where competing methods struggle, such as multi-head attention layers, normalization layers, convolutional layers, ResNet blocks, and group-equivariant linear layers. We prove that GMNs are expressive and equivariant to parameter permutation symmetries that leave the input neural network functions unchanged. We validate the effectiveness of our method on several metanetwork tasks over diverse neural network architectures.",
    "Link": "http://arxiv.org/abs/2312.04501v2",
    "PDF Link": "http://arxiv.org/pdf/2312.04501v2"
  },
  {
    "Title": "Bayesian Learning of Neural Networks for Signal/Background\n  Discrimination in Particle Physics",
    "Authors": "Michael Pogwizd, Laura Jane Elgass, Pushpalatha C. Bhat",
    "Published": "2007-07-06T14:18:02Z",
    "Summary": "Neural networks are used extensively in classification problems in particle physics research. Since the training of neural networks can be viewed as a problem of inference, Bayesian learning of neural networks can provide more optimal and robust results than conventional learning methods. We have investigated the use of Bayesian neural networks for signal/background discrimination in the search for second generation leptoquarks at the Tevatron, as an example. We present a comparison of the results obtained from the conventional training of feedforward neural networks and networks trained with Bayesian methods.",
    "Link": "http://arxiv.org/abs/0707.0930v1",
    "PDF Link": "http://arxiv.org/pdf/0707.0930v1"
  },
  {
    "Title": "Deep Neural Networks for Pattern Recognition",
    "Authors": "Kyongsik Yun, Alexander Huyen, Thomas Lu",
    "Published": "2018-09-25T18:23:49Z",
    "Summary": "In the field of pattern recognition research, the method of using deep neural networks based on improved computing hardware recently attracted attention because of their superior accuracy compared to conventional methods. Deep neural networks simulate the human visual system and achieve human equivalent accuracy in image classification, object detection, and segmentation. This chapter introduces the basic structure of deep neural networks that simulate human neural networks. Then we identify the operational processes and applications of conditional generative adversarial networks, which are being actively researched based on the bottom-up and top-down mechanisms, the most important functions of the human visual perception process. Finally, recent developments in training strategies for effective learning of complex deep neural networks are addressed.",
    "Link": "http://arxiv.org/abs/1809.09645v1",
    "PDF Link": "http://arxiv.org/pdf/1809.09645v1"
  },
  {
    "Title": "Evidence, Definitions and Algorithms regarding the Existence of\n  Cohesive-Convergence Groups in Neural Network Optimization",
    "Authors": "Thien An L. Nguyen",
    "Published": "2024-03-08T13:23:42Z",
    "Summary": "Understanding the convergence process of neural networks is one of the most complex and crucial issues in the field of machine learning. Despite the close association of notable successes in this domain with the convergence of artificial neural networks, this concept remains predominantly theoretical. In reality, due to the non-convex nature of the optimization problems that artificial neural networks tackle, very few trained networks actually achieve convergence. To expand recent research efforts on artificial-neural-network convergence, this paper will discuss a different approach based on observations of cohesive-convergence groups emerging during the optimization process of an artificial neural network.",
    "Link": "http://arxiv.org/abs/2403.05610v1",
    "PDF Link": "http://arxiv.org/pdf/2403.05610v1"
  },
  {
    "Title": "Hybrid deep neural network based prediction method for unsteady flows\n  with moving boundaries",
    "Authors": "Renkun Han, Zhong Zhang, Yixing Wang, Ziyang Liu, Yang Zhang, Gang Chen",
    "Published": "2020-06-01T03:26:17Z",
    "Summary": "A novel hybrid deep neural network architecture is designed to capture the spatial-temporal features of unsteady flows around moving boundaries directly from high-dimensional unsteady flow fields data. The hybrid deep neural network is constituted by the convolutional neural network (CNN), improved convolutional Long-Short Term Memory neural network (ConvLSTM) and deconvolutional neural network (DeCNN). Flow fields at future time step can be predicted through flow fields by previous time steps and boundary positions at those steps by the novel hybrid deep neural network. Unsteady wake flows around a forced oscillation cylinder with various amplitudes are calculated to establish the datasets as training samples for training the hybrid deep neural networks. The trained hybrid deep neural networks are then tested by predicting the unsteady flow fields around a forced oscillation cylinder with new amplitude. The effect of neural network structure parameters on prediction accuracy was analyzed. The hybrid deep neural network, constituted by the best parameter combination, is used to predict the flow fields in the future time. The predicted flow fields are in good agreement with those calculated directly by computational fluid dynamic solver, which means that this kind of deep neural network can capture accurate spatial-temporal information from the spatial-temporal series of unsteady flows around moving boundaries. The result shows the potential capability of this kind novel hybrid deep neural network in flow control for vibrating cylinder, where the fast calculation of high-dimensional nonlinear unsteady flow around moving boundaries is needed.",
    "Link": "http://arxiv.org/abs/2006.00690v1",
    "PDF Link": "http://arxiv.org/pdf/2006.00690v1"
  },
  {
    "Title": "Neural network learning dynamics in a path integral framework",
    "Authors": "J. Balakrishnan",
    "Published": "2003-08-25T15:11:24Z",
    "Summary": "A path-integral formalism is proposed for studying the dynamical evolution in time of patterns in an artificial neural network in the presence of noise. An effective cost function is constructed which determines the unique global minimum of the neural network system. The perturbative method discussed also provides a way for determining the storage capacity of the network.",
    "Link": "http://arxiv.org/abs/cond-mat/0308503v1",
    "PDF Link": "http://arxiv.org/pdf/cond-mat/0308503v1"
  },
  {
    "Title": "Data Engineering for the Analysis of Semiconductor Manufacturing Data",
    "Authors": "Peter D. Turney",
    "Published": "2002-12-12T19:11:11Z",
    "Summary": "We have analyzed manufacturing data from several different semiconductor manufacturing plants, using decision tree induction software called Q-YIELD. The software generates rules for predicting when a given product should be rejected. The rules are intended to help the process engineers improve the yield of the product, by helping them to discover the causes of rejection. Experience with Q-YIELD has taught us the importance of data engineering -- preprocessing the data to enable or facilitate decision tree induction. This paper discusses some of the data engineering problems we have encountered with semiconductor manufacturing data. The paper deals with two broad classes of problems: engineering the features in a feature vector representation and engineering the definition of the target concept (the classes). Manufacturing process data present special problems for feature engineering, since the data have multiple levels of granularity (detail, resolution). Engineering the target concept is important, due to our focus on understanding the past, as opposed to the more common focus in machine learning on predicting the future.",
    "Link": "http://arxiv.org/abs/cs/0212040v1",
    "PDF Link": "http://arxiv.org/pdf/cs/0212040v1"
  },
  {
    "Title": "Skeena: Efficient and Consistent Cross-Engine Transactions",
    "Authors": "Jianqiu Zhang, Kaisong Huang, Tianzheng Wang, King Lv",
    "Published": "2021-08-02T04:48:51Z",
    "Summary": "Database systems are becoming increasingly multi-engine. In particular, a main-memory database engine may coexist with a traditional storage-centric engine in a system to support various applications. It is desirable to allow applications to access data in both engines using cross-engine transactions. But existing systems are either only designed for single-engine accesses, or impose many restrictions by limiting cross-engine transactions to certain isolation levels and table operations. The result is inadequate cross-engine support in terms of correctness, performance and programmability.   This paper describes Skeena, a holistic approach to cross-engine transactions. We propose a lightweight snapshot tracking structure and an atomic commit protocol to efficiently ensure correctness and support various isolation levels. Evaluation results show that Skeena maintains high performance for single-engine transactions and enables cross-engine transactions which can improve throughput by up to 30x by judiciously placing tables in different engines.",
    "Link": "http://arxiv.org/abs/2108.00632v5",
    "PDF Link": "http://arxiv.org/pdf/2108.00632v5"
  },
  {
    "Title": "Guidelines for Systematic Mapping Studies in Security Engineering",
    "Authors": "Michael Felderer, Jeffrey C. Carver",
    "Published": "2018-01-21T12:12:35Z",
    "Summary": "Security engineering in the software lifecycle aims at protecting information and systems to guarantee confidentiality, integrity, and availability. As security engineering matures and the number of research papers grows, there is an increasing need for papers that summarize results and provide an overview of the area. A systematic mapping study \"maps\" a research area by classifying papers to identify which topics are well-studied and which need additional study. Therefore, systematic mapping studies are becoming increasingly important in security engineering. This chapter provides methodological support for systematic mapping studies in security engineering based on examples from published security engineering papers. Because security engineering is similar to software engineering in that it bridges research and practice, researchers can use the same basic systematic mapping process, as follows: (1) study planning, (2) searching for studies, (3) study selection, (4) study quality assessment, (5) data extraction, (6) data classification, (7) data analysis, and (8) reporting of results. We use published mapping studies to describe the tailoring of this process for security engineering. In addition to guidance on how to perform systematic mapping studies in security engineering, this chapter should increase awareness in the security engineering community of the need for additional mapping studies.",
    "Link": "http://arxiv.org/abs/1801.06810v1",
    "PDF Link": "http://arxiv.org/pdf/1801.06810v1"
  },
  {
    "Title": "What About the Data? A Mapping Study on Data Engineering for AI Systems",
    "Authors": "Petra Heck",
    "Published": "2024-02-07T16:31:58Z",
    "Summary": "AI systems cannot exist without data. Now that AI models (data science and AI) have matured and are readily available to apply in practice, most organizations struggle with the data infrastructure to do so. There is a growing need for data engineers that know how to prepare data for AI systems or that can setup enterprise-wide data architectures for analytical projects. But until now, the data engineering part of AI engineering has not been getting much attention, in favor of discussing the modeling part. In this paper we aim to change this by perform a mapping study on data engineering for AI systems, i.e., AI data engineering. We found 25 relevant papers between January 2019 and June 2023, explaining AI data engineering activities. We identify which life cycle phases are covered, which technical solutions or architectures are proposed and which lessons learned are presented. We end by an overall discussion of the papers with implications for practitioners and researchers. This paper creates an overview of the body of knowledge on data engineering for AI. This overview is useful for practitioners to identify solutions and best practices as well as for researchers to identify gaps.",
    "Link": "http://arxiv.org/abs/2402.05156v1",
    "PDF Link": "http://arxiv.org/pdf/2402.05156v1"
  },
  {
    "Title": "Foundation Model Engineering: Engineering Foundation Models Just as\n  Engineering Software",
    "Authors": "Dezhi Ran, Mengzhou Wu, Wei Yang, Tao Xie",
    "Published": "2024-07-11T04:40:02Z",
    "Summary": "By treating data and models as the source code, Foundation Models (FMs) become a new type of software. Mirroring the concept of software crisis, the increasing complexity of FMs making FM crisis a tangible concern in the coming decade, appealing for new theories and methodologies from the field of software engineering. In this paper, we outline our vision of introducing Foundation Model (FM) engineering, a strategic response to the anticipated FM crisis with principled engineering methodologies. FM engineering aims to mitigate potential issues in FM development and application through the introduction of declarative, automated, and unified programming interfaces for both data and model management, reducing the complexities involved in working with FMs by providing a more structured and intuitive process for developers. Through the establishment of FM engineering, we aim to provide a robust, automated, and extensible framework that addresses the imminent challenges, and discovering new research opportunities for the software engineering field.",
    "Link": "http://arxiv.org/abs/2407.08176v1",
    "PDF Link": "http://arxiv.org/pdf/2407.08176v1"
  },
  {
    "Title": "On a Factorial Knowledge Architecture for Data Science-powered Software\n  Engineering",
    "Authors": "Zheng Li",
    "Published": "2021-03-02T00:57:49Z",
    "Summary": "Given the data-intensive and collaborative trend in science, the software engineering community also pays increasing attention to obtaining valuable and useful insights from data repositories. Nevertheless, applying data science to software engineering (e.g., mining software repositories) can be blindfold and meaningless, if lacking a suitable knowledge architecture (KA). By observing that software engineering practices are generally recorded through a set of factors (e.g., programmer capacity, different environmental conditions, etc.) involved in various software project aspects, we propose a factor-based hierarchical KA of software engineering to help maximize the value of software repositories and inspire future software data-driven studies. In particular, it is the organized factors and their relationships that help guide software engineering knowledge mining, while the mined knowledge will in turn be indexed/managed through the relevant factors and their interactions. This paper explains our idea about the factorial KA and concisely demonstrates a KA component, i.e. the early-version KA of software product engineering. Once fully scoped, this proposed KA will supplement the well-known SWEBOK in terms of both the factor-centric knowledge management and the coverage/implication of potential software engineering knowledge.",
    "Link": "http://arxiv.org/abs/2103.01387v1",
    "PDF Link": "http://arxiv.org/pdf/2103.01387v1"
  },
  {
    "Title": "Brain-Inspired Spike Echo State Network Dynamics for Aero-Engine\n  Intelligent Fault Prediction",
    "Authors": "Mo-Ran Liu, Tao Sun, Xi-Ming Sun",
    "Published": "2024-06-14T04:06:17Z",
    "Summary": "Aero-engine fault prediction aims to accurately predict the development trend of the future state of aero-engines, so as to diagnose faults in advance. Traditional aero-engine parameter prediction methods mainly use the nonlinear mapping relationship of time series data but generally ignore the adequate spatiotemporal features contained in aero-engine data. To this end, we propose a brain-inspired spike echo state network (Spike-ESN) model for aero-engine intelligent fault prediction, which is used to effectively capture the evolution process of aero-engine time series data in the framework of spatiotemporal dynamics. In the proposed approach, we design a spike input layer based on Poisson distribution inspired by the spike neural encoding mechanism of biological neurons, which can extract the useful temporal characteristics in aero-engine sequence data. Then, the temporal characteristics are input into a spike reservoir through the current calculation method of spike accumulation in neurons, which projects the data into a high-dimensional sparse space. In addition, we use the ridge regression method to read out the internal state of the spike reservoir. Finally, the experimental results of aero-engine states prediction demonstrate the superiority and potential of the proposed method.",
    "Link": "http://arxiv.org/abs/2406.12918v1",
    "PDF Link": "http://arxiv.org/pdf/2406.12918v1"
  },
  {
    "Title": "Hybrid Data Mining Technique for Knowledge Discovery from Engineering\n  Materials' Data sets",
    "Authors": "Doreswamy, Hemanth K. S",
    "Published": "2012-09-19T07:23:02Z",
    "Summary": "Studying materials informatics from a data mining perspective can be beneficial for manufacturing and other industrial engineering applications. Predictive data mining technique and machine learning algorithm are combined to design a knowledge discovery system for the selection of engineering materials that meet the design specifications. Predictive method-Naive Bayesian classifier and Machine learning Algorithm - Pearson correlation coefficient method were implemented respectively for materials classification and selection. The knowledge extracted from the engineering materials data sets is proposed for effective decision making in advanced engineering materials design applications.",
    "Link": "http://arxiv.org/abs/1209.4169v1",
    "PDF Link": "http://arxiv.org/pdf/1209.4169v1"
  },
  {
    "Title": "Efficient Data Ingestion in Cloud-based architecture: a Data Engineering\n  Design Pattern Proposal",
    "Authors": "Chiara Rucco, Antonella Longo, Motaz Saad",
    "Published": "2025-03-20T12:19:32Z",
    "Summary": "In today's fast-paced digital world, data has become a critical asset for enterprises across various industries. However, the exponential growth of data presents significant challenges in managing and utilizing the vast amounts of information collected. Data engineering has emerged as a vital discipline addressing these challenges by providing robust platforms for effective data management, processing, and utilization. Data Engineering Patterns (DEP) refer to standardized practices and procedures in data engineering, such as ETL (extract, transform, load) processes, data pipelining, and data streaming management. Data Engineering Design Patterns (DEDP) are best practice solutions to common problems in data engineering, involving established, tested, and optimized approaches. These include architectural decisions, data modeling techniques, and data storage and retrieval strategies. While many researchers and practitioners have identified various DEPs and proposed DEDPs, such as data mesh and lambda architecture, the challenge of high-volume data ingestion remains inadequately addressed. In this paper, we propose a data ingestion design pattern for big data in cloud architecture, incorporating both incremental and full refresh techniques. Our approach leverages a flexible, metadata-driven framework to enhance feasibility and flexibility. This allows for easy changes to the ingestion type, schema modifications, table additions, and the integration of new data sources, all with minimal effort from data engineers. Tested on the Azure cloud architecture, our experiments demonstrate that the proposed techniques significantly reduce data ingestion time. Overall, this paper advances data management practices by presenting a detailed exploration of data ingestion challenges and defining a proposal for an effective design patterns for cloud-based architectures.",
    "Link": "http://arxiv.org/abs/2503.16079v2",
    "PDF Link": "http://arxiv.org/pdf/2503.16079v2"
  },
  {
    "Title": "On the validity of pre-trained transformers for natural language\n  processing in the software engineering domain",
    "Authors": "Julian von der Mosel, Alexander Trautsch, Steffen Herbold",
    "Published": "2021-09-10T08:46:31Z",
    "Summary": "Transformers are the current state-of-the-art of natural language processing in many domains and are using traction within software engineering research as well. Such models are pre-trained on large amounts of data, usually from the general domain. However, we only have a limited understanding regarding the validity of transformers within the software engineering domain, i.e., how good such models are at understanding words and sentences within a software engineering context and how this improves the state-of-the-art. Within this article, we shed light on this complex, but crucial issue. We compare BERT transformer models trained with software engineering data with transformers based on general domain data in multiple dimensions: their vocabulary, their ability to understand which words are missing, and their performance in classification tasks. Our results show that for tasks that require understanding of the software engineering context, pre-training with software engineering data is valuable, while general domain models are sufficient for general language understanding, also within the software engineering domain.",
    "Link": "http://arxiv.org/abs/2109.04738v2",
    "PDF Link": "http://arxiv.org/pdf/2109.04738v2"
  },
  {
    "Title": "A Review into Data Science and Its Approaches in Mechanical Engineering",
    "Authors": "Ashkan Yousefi Zadeh, Meysam Shahbazy",
    "Published": "2020-12-30T23:05:29Z",
    "Summary": "Nowadays it is inevitable to use intelligent systems to improve the performance and optimization of different components of devices or factories. Furthermore, it's so essential to have appropriate predictions to make better decisions in businesses, medical studies, and engineering studies, etc. One of the newest and most widely used of these methods is a field called Data Science that all of the scientists, engineers, and factories need to learn and use in their careers. This article briefly introduced data science and reviewed its methods, especially it's usages in mechanical engineering and challenges and ways of developing data science in mechanical engineering. In the introduction, different definitions of data science and its background in technology reviewed. In the following, data science methodology which is the process that a data scientist needs to do in its works been discussed. Further, some researches in the mechanical engineering area that used data science methods in their studies, are reviewed. Eventually, it has been discussed according to the subjects that have been reviewed in the article, why it is necessary to use data science in mechanical engineering researches and projects.",
    "Link": "http://arxiv.org/abs/2012.15358v1",
    "PDF Link": "http://arxiv.org/pdf/2012.15358v1"
  },
  {
    "Title": "Code Generation for Machine Learning using Model-Driven Engineering and\n  SysML",
    "Authors": "Simon Raedler, Matthias Rupp, Eugen Rigger, Stefanie Rinderle-Ma",
    "Published": "2023-07-10T15:00:20Z",
    "Summary": "Data-driven engineering refers to systematic data collection and processing using machine learning to improve engineering systems. Currently, the implementation of data-driven engineering relies on fundamental data science and software engineering skills. At the same time, model-based engineering is gaining relevance for the engineering of complex systems. In previous work, a model-based engineering approach integrating the formalization of machine learning tasks using the general-purpose modeling language SysML is presented. However, formalized machine learning tasks still require the implementation in a specialized programming languages like Python. Therefore, this work aims to facilitate the implementation of data-driven engineering in practice by extending the previous work of formalizing machine learning tasks by integrating model transformation to generate executable code. The method focuses on the modifiability and maintainability of the model transformation so that extensions and changes to the code generation can be integrated without requiring modifications to the code generator. The presented method is evaluated for feasibility in a case study to predict weather forecasts. Based thereon, quality attributes of model transformations are assessed and discussed. Results demonstrate the flexibility and the simplicity of the method reducing efforts for implementation. Further, the work builds a theoretical basis for standardizing data-driven engineering implementation in practice.",
    "Link": "http://arxiv.org/abs/2307.05584v1",
    "PDF Link": "http://arxiv.org/pdf/2307.05584v1"
  },
  {
    "Title": "Augmenting data-driven models for energy systems through feature\n  engineering: A Python framework for feature engineering",
    "Authors": "Sandra Wilfling",
    "Published": "2023-01-04T17:37:15Z",
    "Summary": "Data-driven modeling is an approach in energy systems modeling that has been gaining popularity. In data-driven modeling, machine learning methods such as linear regression, neural networks or decision-tree based methods are being applied. While these methods do not require domain knowledge, they are sensitive to data quality. Therefore, improving data quality in a dataset is beneficial for creating machine learning-based models. The improvement of data quality can be implemented through preprocessing methods. A selected type of preprocessing is feature engineering, which focuses on evaluating and improving the quality of certain features inside the dataset. Feature engineering methods include methods such as feature creation, feature expansion, or feature selection. In this work, a Python framework containing different feature engineering methods is presented. This framework contains different methods for feature creation, expansion and selection; in addition, methods for transforming or filtering data are implemented. The implementation of the framework is based on the Python library scikit-learn. The framework is demonstrated on a case study of a use case from energy demand prediction. A data-driven model is created including selected feature engineering methods. The results show an improvement in prediction accuracy through the engineered features.",
    "Link": "http://arxiv.org/abs/2301.01720v1",
    "PDF Link": "http://arxiv.org/pdf/2301.01720v1"
  },
  {
    "Title": "Investigating student understanding of heat engine: a case study of\n  Stirling engine",
    "Authors": "Lilin Zhu, Gang Xiang",
    "Published": "2020-08-14T15:04:45Z",
    "Summary": "We report on the study of student difficulties regarding heat engine in the context of Stirling cycle within upper-division undergraduate thermal physics course. An in-class test about a Stirling engine with a regenerator was taken by three classes, and the students were asked to perform one of the most basic activities---calculate the efficiency of the heat engine. Our data suggest that quite a few students have not developed a robust conceptual understanding of basic engineering knowledge of the heat engine, including the function of the regenerator and the influence of piston movements on the heat and work involved in the engine. Most notably, although the science error ratios of the three classes were similar ($\\sim$10\\%), the engineering error ratios of the three classes were high (above 50\\%), and the class that was given a simple tutorial of engineering knowledge of heat engine exhibited significantly smaller engineering error ratio by about 20\\% than the other two classes. In addition, both the written answers and post-test interviews show that most of the students can only associate Carnot's theorem with Carnot cycle, but not with other reversible cycles working between two heat reservoirs, probably because no enough cycles except Carnot cycle were covered in the traditional Thermodynamics textbook. Our results suggest that both scientific and engineering knowledge are important and should be included in instructional approaches, especially in the Thermodynamics course taught in the countries and regions with a tradition of not paying much attention to experimental education or engineering training.",
    "Link": "http://arxiv.org/abs/2008.06405v1",
    "PDF Link": "http://arxiv.org/pdf/2008.06405v1"
  },
  {
    "Title": "EmptyHeaded: A Relational Engine for Graph Processing",
    "Authors": "Christopher R. Aberger, Susan Tu, Kunle Olukotun, Christopher Ré",
    "Published": "2015-03-09T04:02:36Z",
    "Summary": "There are two types of high-performance graph processing engines: low- and high-level engines. Low-level engines (Galois, PowerGraph, Snap) provide optimized data structures and computation models but require users to write low-level imperative code, hence ensuring that efficiency is the burden of the user. In high-level engines, users write in query languages like datalog (SociaLite) or SQL (Grail). High-level engines are easier to use but are orders of magnitude slower than the low-level graph engines. We present EmptyHeaded, a high-level engine that supports a rich datalog-like query language and achieves performance comparable to that of low-level engines. At the core of EmptyHeaded's design is a new class of join algorithms that satisfy strong theoretical guarantees but have thus far not achieved performance comparable to that of specialized graph processing engines. To achieve high performance, EmptyHeaded introduces a new join engine architecture, including a novel query optimizer and data layouts that leverage single-instruction multiple data (SIMD) parallelism. With this architecture, EmptyHeaded outperforms high-level approaches by up to three orders of magnitude on graph pattern queries, PageRank, and Single-Source Shortest Paths (SSSP) and is an order of magnitude faster than many low-level baselines. We validate that EmptyHeaded competes with the best-of-breed low-level engine (Galois), achieving comparable performance on PageRank and at most 3x worse performance on SSSP.",
    "Link": "http://arxiv.org/abs/1503.02368v7",
    "PDF Link": "http://arxiv.org/pdf/1503.02368v7"
  },
  {
    "Title": "Evaluation of Query Generators for Entity Search Engines",
    "Authors": "Stefan Endrullis, Andreas Thor, Erhard Rahm",
    "Published": "2010-03-23T13:55:40Z",
    "Summary": "Dynamic web applications such as mashups need efficient access to web data that is only accessible via entity search engines (e.g. product or publication search engines). However, most current mashup systems and applications only support simple keyword searches for retrieving data from search engines. We propose the use of more powerful search strategies building on so-called query generators. For a given set of entities query generators are able to automatically determine a set of search queries to retrieve these entities from an entity search engine. We demonstrate the usefulness of query generators for on-demand web data integration and evaluate the effectiveness and efficiency of query generators for a challenging real-world integration scenario.",
    "Link": "http://arxiv.org/abs/1003.4418v1",
    "PDF Link": "http://arxiv.org/pdf/1003.4418v1"
  },
  {
    "Title": "Virtual Sensor Modelling using Neural Networks with Coefficient-based\n  Adaptive Weights and Biases Search Algorithm for Diesel Engines",
    "Authors": "Kushagra Rastogi, Navreet Saini",
    "Published": "2017-12-22T07:06:34Z",
    "Summary": "With the explosion in the field of Big Data and introduction of more stringent emission norms every three to five years, automotive companies must not only continue to enhance the fuel economy ratings of their products, but also provide valued services to their customers such as delivering engine performance and health reports at regular intervals. A reasonable solution to both issues is installing a variety of sensors on the engine. Sensor data can be used to develop fuel economy features and will directly indicate engine performance. However, mounting a plethora of sensors is impractical in a very cost-sensitive industry. Thus, virtual sensors can replace physical sensors by reducing cost while capturing essential engine data.",
    "Link": "http://arxiv.org/abs/1712.08319v1",
    "PDF Link": "http://arxiv.org/pdf/1712.08319v1"
  },
  {
    "Title": "Push vs. Pull-Based Loop Fusion in Query Engines",
    "Authors": "Amir Shaikhha, Mohammad Dashti, Christoph Koch",
    "Published": "2016-10-28T11:00:02Z",
    "Summary": "Database query engines use pull-based or push-based approaches to avoid the materialization of data across query operators. In this paper, we study these two types of query engines in depth and present the limitations and advantages of each engine. Similarly, the programming languages community has developed loop fusion techniques to remove intermediate collections in the context of collection programming. We draw parallels between the DB and PL communities by demonstrating the connection between pipelined query engines and loop fusion techniques. Based on this connection, we propose a new type of pull-based engine, inspired by a loop fusion technique, which combines the benefits of both approaches. Then we experimentally evaluate the various engines, in the context of query compilation, for the first time in a fair environment, eliminating the biasing impact of ancillary optimizations that have traditionally only been used with one of the approaches. We show that for realistic analytical workloads, there is no considerable advantage for either form of pipelined query engine, as opposed to what recent research suggests. Also, by using microbenchmarks we show that our proposed engine dominates the existing engines by combining the benefits of both.",
    "Link": "http://arxiv.org/abs/1610.09166v1",
    "PDF Link": "http://arxiv.org/pdf/1610.09166v1"
  },
  {
    "Title": "Semantic Networks for Engineering Design: A Survey",
    "Authors": "Ji Han, Serhad Sarica, Feng Shi, Jianxi Luo",
    "Published": "2020-12-13T13:36:20Z",
    "Summary": "There have been growing uses of semantic networks in the past decade, such as leveraging large-scale pre-trained graph knowledge databases for various natural language processing (NLP) tasks in engineering design research. Therefore, the paper provides a survey of the research that has employed semantic networks in the engineering design research community. The survey reveals that engineering design researchers have primarily relied on WordNet, ConceptNet, and other common-sense semantic network databases trained on non-engineering data sources to develop methods or tools for engineering design. Meanwhile, there are emerging efforts to mine large scale technical publication and patent databases to construct engineering-contextualized semantic network databases, e.g., B-Link and TechNet, to support NLP in engineering design. On this basis, we recommend future research directions for the construction and applications of engineering-related semantic networks in engineering design research and practice.",
    "Link": "http://arxiv.org/abs/2012.07060v1",
    "PDF Link": "http://arxiv.org/pdf/2012.07060v1"
  },
  {
    "Title": "To get good student ratings should you only teach programming courses?\n  Investigation and implications of student evaluations of teaching in a\n  software engineering context",
    "Authors": "Antti Knutas, Timo Hynninen, Maija Hujala",
    "Published": "2021-02-15T11:10:10Z",
    "Summary": "Student evaluations of teaching (SET) are commonly used in universities for assessing teaching quality. However, previous literature shows that in software engineering students tend to rate certain topics higher than others: In particular students tend to value programming and software construction over software design, software engineering models and methods, or soft skills. We hypothesize that these biases also play a role in SET responses collected from students. The objective of this study is to investigate how the topic of a software engineering course affects the SET metrics. We accomplish this by performing multilevel regression analysis on SET data collected in a software engineering programme. We analyzed a total of 1295 student evaluations from 46 university courses in a Finnish university. The results of the analysis verifies that the student course evaluations exhibit similar biases as distinguished by previous software engineering education research. The type of the course can predict a higher SET rating. In our dataset, software construction and programming courses received higher SET ratings compared to courses on software engineering processes, models, and methods.",
    "Link": "http://arxiv.org/abs/2102.08179v1",
    "PDF Link": "http://arxiv.org/pdf/2102.08179v1"
  },
  {
    "Title": "Maximizing information from chemical engineering data sets: Applications\n  to machine learning",
    "Authors": "Alexander Thebelt, Johannes Wiebe, Jan Kronqvist, Calvin Tsay, Ruth Misener",
    "Published": "2022-01-25T01:25:45Z",
    "Summary": "It is well-documented how artificial intelligence can have (and already is having) a big impact on chemical engineering. But classical machine learning approaches may be weak for many chemical engineering applications. This review discusses how challenging data characteristics arise in chemical engineering applications. We identify four characteristics of data arising in chemical engineering applications that make applying classical artificial intelligence approaches difficult: (1) high variance, low volume data, (2) low variance, high volume data, (3) noisy/corrupt/missing data, and (4) restricted data with physics-based limitations. For each of these four data characteristics, we discuss applications where these data characteristics arise and show how current chemical engineering research is extending the fields of data science and machine learning to incorporate these challenges. Finally, we identify several challenges for future research.",
    "Link": "http://arxiv.org/abs/2201.10035v1",
    "PDF Link": "http://arxiv.org/pdf/2201.10035v1"
  },
  {
    "Title": "Intelligent Semantic Web Search Engines: A Brief Survey",
    "Authors": "G. Madhu, Dr. A. Govardhan, Dr. T. V. Rajinikanth",
    "Published": "2011-02-04T03:56:09Z",
    "Summary": "The World Wide Web (WWW) allows the people to share the information (data) from the large database repositories globally. The amount of information grows billions of databases. We need to search the information will specialize tools known generically search engine. There are many of search engines available today, retrieving meaningful information is difficult. However to overcome this problem in search engines to retrieve meaningful information intelligently, semantic web technologies are playing a major role. In this paper we present survey on the search engine generations and the role of search engines in intelligent web and semantic search technologies.",
    "Link": "http://arxiv.org/abs/1102.0831v1",
    "PDF Link": "http://arxiv.org/pdf/1102.0831v1"
  },
  {
    "Title": "Teaching Software Engineering through Robotics",
    "Authors": "Jiwon Shin, Andrey Rusakov, Bertrand Meyer",
    "Published": "2014-06-17T18:08:43Z",
    "Summary": "This paper presents a newly-developed robotics programming course and reports the initial results of software engineering education in robotics context. Robotics programming, as a multidisciplinary course, puts equal emphasis on software engineering and robotics. It teaches students proper software engineering -- in particular, modularity and documentation -- by having them implement four core robotics algorithms for an educational robot. To evaluate the effect of software engineering education in robotics context, we analyze pre- and post-class survey data and the four assignments our students completed for the course. The analysis suggests that the students acquired an understanding of software engineering techniques and principles.",
    "Link": "http://arxiv.org/abs/1406.4458v1",
    "PDF Link": "http://arxiv.org/pdf/1406.4458v1"
  },
  {
    "Title": "Agile Software Engineering and Systems Engineering at SKA Scale",
    "Authors": "Juande Santander-Vela",
    "Published": "2017-11-30T20:20:42Z",
    "Summary": "Systems Engineering (SE) is the set of processes and documentation required for successfully realising large-scale engineering projects, but the classical approach is not a good fit for software-intensive projects, especially when the needs of the different stakeholders are not fully known from the beginning, and requirement priorities might change. The SKA is the ultimate software-enabled telescope, with enormous amounts of computing hardware and software required to perform its data reduction. We give an overview of the system and software engineering processes in the SKA1 development, and the tension between classical and agile SE.",
    "Link": "http://arxiv.org/abs/1712.00061v2",
    "PDF Link": "http://arxiv.org/pdf/1712.00061v2"
  },
  {
    "Title": "Knowledge Engineering using Large Language Models",
    "Authors": "Bradley P. Allen, Lise Stork, Paul Groth",
    "Published": "2023-10-01T10:26:25Z",
    "Summary": "Knowledge engineering is a discipline that focuses on the creation and maintenance of processes that generate and apply knowledge. Traditionally, knowledge engineering approaches have focused on knowledge expressed in formal languages. The emergence of large language models and their capabilities to effectively work with natural language, in its broadest sense, raises questions about the foundations and practice of knowledge engineering. Here, we outline the potential role of LLMs in knowledge engineering, identifying two central directions: 1) creating hybrid neuro-symbolic knowledge systems; and 2) enabling knowledge engineering in natural language. Additionally, we formulate key open research questions to tackle these directions.",
    "Link": "http://arxiv.org/abs/2310.00637v1",
    "PDF Link": "http://arxiv.org/pdf/2310.00637v1"
  },
  {
    "Title": "Parsisanj: a semi-automatic component-based approach towards search\n  engine evaluation",
    "Authors": "Amin Heydari Alashti, Ahmad Asgharian Rezaei, Alireza Elahi, Sobhan Sayyaran, Mohammad Ghodsi",
    "Published": "2020-09-25T09:17:57Z",
    "Summary": "Accessing to required data on the internet is wide via search engines in the last two decades owing to the huge amount of available data and the high rate of new data is generating daily. Accordingly, search engines are encouraged to make the most valuable existing data on the web searchable. Knowing how to handle a large amount of data in each step of a search engines' procedure from crawling to indexing and ranking is just one of the challenges that a professional search engine should solve. Moreover, it should also have the best practices in handling users' traffics, state-of-the-art natural language processing tools, and should also address many other challenges on the edge of science and technology. As a result, evaluating these systems is too challenging due to the level of internal complexity they have, and is crucial for finding the improvement path of the existing system. Therefore, an evaluation procedure is a normal subsystem of a search engine that has the role of building its roadmap. Recently, several countries have developed national search engine programs to build an infrastructure to provide special services based on their needs on the available data of their language on the web. This research is conducted accordingly to enlighten the advancement path of two Iranian national search engines: Yooz and Parsijoo in comparison with two international ones, Google and Bing. Unlike related work, it is a semi-automatic method to evaluate the search engines at the first pace. Eventually, we obtained some interesting results which based on them the component-based improvement roadmap of national search engines could be illustrated concretely.",
    "Link": "http://arxiv.org/abs/2009.12097v1",
    "PDF Link": "http://arxiv.org/pdf/2009.12097v1"
  },
  {
    "Title": "A Survey of Pipeline Tools for Data Engineering",
    "Authors": "Anthony Mbata, Yaji Sripada, Mingjun Zhong",
    "Published": "2024-06-12T15:41:06Z",
    "Summary": "Currently, a variety of pipeline tools are available for use in data engineering. Data scientists can use these tools to resolve data wrangling issues associated with data and accomplish some data engineering tasks from data ingestion through data preparation to utilization as input for machine learning (ML). Some of these tools have essential built-in components or can be combined with other tools to perform desired data engineering operations. While some tools are wholly or partly commercial, several open-source tools are available to perform expert-level data engineering tasks. This survey examines the broad categories and examples of pipeline tools based on their design and data engineering intentions. These categories are Extract Transform Load/Extract Load Transform (ETL/ELT), pipelines for Data Integration, Ingestion, and Transformation, Data Pipeline Orchestration and Workflow Management, and Machine Learning Pipelines. The survey also provides a broad outline of the utilization with examples within these broad groups and finally, a discussion is presented with case studies indicating the usage of pipeline tools for data engineering. The studies present some first-user application experiences with sample data, some complexities of the applied pipeline, and a summary note of approaches to using these tools to prepare data for machine learning.",
    "Link": "http://arxiv.org/abs/2406.08335v1",
    "PDF Link": "http://arxiv.org/pdf/2406.08335v1"
  },
  {
    "Title": "Analysis of Software Engineering Practices in General Software and\n  Machine Learning Startups",
    "Authors": "Bishal Lakha, Kalyan Bhetwal, Nasir U. Eisty",
    "Published": "2023-04-04T04:21:09Z",
    "Summary": "Context: On top of the inherent challenges startup software companies face applying proper software engineering practices, the non-deterministic nature of machine learning techniques makes it even more difficult for machine learning (ML) startups.   Objective: Therefore, the objective of our study is to understand the whole picture of software engineering practices followed by ML startups and identify additional needs.   Method: To achieve our goal, we conducted a systematic literature review study on 37 papers published in the last 21 years. We selected papers on both general software startups and ML startups. We collected data to understand software engineering (SE) practices in five phases of the software development life-cycle: requirement engineering, design, development, quality assurance, and deployment.   Results: We find some interesting differences in software engineering practices in ML startups and general software startups. The data management and model learning phases are the most prominent among them.   Conclusion: While ML startups face many similar challenges to general software startups, the additional difficulties of using stochastic ML models require different strategies in using software engineering practices to produce high-quality products.",
    "Link": "http://arxiv.org/abs/2304.01523v1",
    "PDF Link": "http://arxiv.org/pdf/2304.01523v1"
  },
  {
    "Title": "Continual Learning Strategies for 3D Engineering Regression Problems: A\n  Benchmarking Study",
    "Authors": "Kaira M. Samuel, Faez Ahmed",
    "Published": "2025-04-16T21:40:03Z",
    "Summary": "Engineering problems that apply machine learning often involve computationally intensive methods but rely on limited datasets. As engineering data evolves with new designs and constraints, models must incorporate new knowledge over time. However, high computational costs make retraining models from scratch infeasible. Continual learning (CL) offers a promising solution by enabling models to learn from sequential data while mitigating catastrophic forgetting, where a model forgets previously learned mappings. This work introduces CL to engineering design by benchmarking several CL methods on representative regression tasks. We apply these strategies to five engineering datasets and construct nine new engineering CL benchmarks to evaluate their ability to address forgetting and improve generalization. Preliminary results show that applying existing CL methods to these tasks improves performance over naive baselines. In particular, the Replay strategy achieved performance comparable to retraining in several benchmarks while reducing training time by nearly half, demonstrating its potential for real-world engineering workflows. The code and datasets used in this work will be available at: https://github.com/kmsamuel/cl-for-engineering-release.",
    "Link": "http://arxiv.org/abs/2504.12503v1",
    "PDF Link": "http://arxiv.org/pdf/2504.12503v1"
  },
  {
    "Title": "Software engineering in start-up companies: An analysis of 88 experience\n  reports",
    "Authors": "Eriks Klotins, Michael Unterkalmsteiner, Tony Gorschek",
    "Published": "2023-11-20T19:42:37Z",
    "Summary": "Context: Start-up companies have become an important supplier of innovation and software-intensive products. The flexibility and reactiveness of start-ups enables fast development and launch of innovative products. However, a majority of software start-up companies fail before achieving any success. Among other factors, poor software engineering could be a significant contributor to the challenges experienced by start-ups. However, the state-of-practice of software engineering in start-ups, as well as the utilization of state-of-the-art is largely an unexplored area. Objective: In this study we investigate how software engineering is applied in start-up context with a focus to identify key knowledge areas and opportunities for further research. Method: We perform a multi-vocal exploratory study of 88 start-up experience reports. We develop a custom taxonomy to categorize the reported software engineering practices and their interrelation with business aspects, and apply qualitative data analysis to explore influences and dependencies between the knowledge areas. Results: We identify the most frequently reported software engineering (requirements engineering, software design and quality) and business aspect (vision and strategy development) knowledge areas, and illustrate their relationships. We also present a summary of how relevant software engineering knowledge areas are implemented in start-ups and identify potentially useful practices for adoption in start-ups. Conclusions: The results enable a more focused research on engineering practices in start-ups. We conclude that most engineering challenges in start-ups stem from inadequacies in requirements engineering. Many promising practices to address specific engineering challenges exists, however more research on adaptation of established practices, and validation of new start-up specific practices is needed.",
    "Link": "http://arxiv.org/abs/2311.12139v1",
    "PDF Link": "http://arxiv.org/pdf/2311.12139v1"
  },
  {
    "Title": "Data Engineering for HPC with Python",
    "Authors": "Vibhatha Abeykoon, Niranda Perera, Chathura Widanage, Supun Kamburugamuve, Thejaka Amila Kanewala, Hasara Maithree, Pulasthi Wickramasinghe, Ahmet Uyar, Geoffrey Fox",
    "Published": "2020-10-13T11:53:11Z",
    "Summary": "Data engineering is becoming an increasingly important part of scientific discoveries with the adoption of deep learning and machine learning. Data engineering deals with a variety of data formats, storage, data extraction, transformation, and data movements. One goal of data engineering is to transform data from original data to vector/matrix/tensor formats accepted by deep learning and machine learning applications. There are many structures such as tables, graphs, and trees to represent data in these data engineering phases. Among them, tables are a versatile and commonly used format to load and process data. In this paper, we present a distributed Python API based on table abstraction for representing and processing data. Unlike existing state-of-the-art data engineering tools written purely in Python, our solution adopts high performance compute kernels in C++, with an in-memory table representation with Cython-based Python bindings. In the core system, we use MPI for distributed memory computations with a data-parallel approach for processing large datasets in HPC clusters.",
    "Link": "http://arxiv.org/abs/2010.06312v1",
    "PDF Link": "http://arxiv.org/pdf/2010.06312v1"
  },
  {
    "Title": "Toward Reverse Engineering of VBA Based Excel Spreadsheet Applications",
    "Authors": "Domenico Amalfitano, Nicola Amatucci, Vincenzo De Simone, Anna Rita Fasolino, Porfirio Tramontana",
    "Published": "2015-03-11T16:17:53Z",
    "Summary": "Modern spreadsheet systems can be used to implement complex spreadsheet applications including data sheets, customized user forms and executable procedures written in a scripting language. These applications are often developed by practitioners that do not follow any software engineering practice and do not produce any design documentation. Thus, spreadsheet applications may be very difficult to be maintained or restructured. In this position paper we present in a nutshell two reverse engineering techniques and a tool that we are currently realizing for the abstraction of conceptual data models and business logic models.",
    "Link": "http://arxiv.org/abs/1503.03401v1",
    "PDF Link": "http://arxiv.org/pdf/1503.03401v1"
  },
  {
    "Title": "Towards the Holodeck: Fully Immersive Virtual Reality Visualisation of\n  Scientific and Engineering Data",
    "Authors": "Stefan Marks, Javier E. Estevez, Andy M. Connor",
    "Published": "2016-04-20T02:54:21Z",
    "Summary": "In this paper, we describe the development and operating principles of an immersive virtual reality (VR) visualisation environment that is designed around the use of consumer VR headsets in an existing wide area motion capture suite. We present two case studies in the application areas of visualisation of scientific and engineering data. Each of these case studies utilise a different render engine, namely a custom engine for one case and a commercial game engine for the other. The advantages and appropriateness of each approach are discussed along with suggestions for future work.",
    "Link": "http://arxiv.org/abs/1604.05797v1",
    "PDF Link": "http://arxiv.org/pdf/1604.05797v1"
  },
  {
    "Title": "Artificial intelligence-aided protein engineering: from topological data\n  analysis to deep protein language models",
    "Authors": "Yuchi Qiu, Guo-Wei Wei",
    "Published": "2023-07-27T02:14:09Z",
    "Summary": "Protein engineering is an emerging field in biotechnology that has the potential to revolutionize various areas, such as antibody design, drug discovery, food security, ecology, and more. However, the mutational space involved is too vast to be handled through experimental means alone. Leveraging accumulative protein databases, machine learning (ML) models, particularly those based on natural language processing (NLP), have considerably expedited protein engineering. Moreover, advances in topological data analysis (TDA) and artificial intelligence-based protein structure prediction, such as AlphaFold2, have made more powerful structure-based ML-assisted protein engineering strategies possible. This review aims to offer a comprehensive, systematic, and indispensable set of methodological components, including TDA and NLP, for protein engineering and to facilitate their future development.",
    "Link": "http://arxiv.org/abs/2307.14587v1",
    "PDF Link": "http://arxiv.org/pdf/2307.14587v1"
  },
  {
    "Title": "Can GPT-4 Replicate Empirical Software Engineering Research?",
    "Authors": "Jenny T. Liang, Carmen Badea, Christian Bird, Robert DeLine, Denae Ford, Nicole Forsgren, Thomas Zimmermann",
    "Published": "2023-10-03T01:27:23Z",
    "Summary": "Empirical software engineering research on production systems has brought forth a better understanding of the software engineering process for practitioners and researchers alike. However, only a small subset of production systems is studied, limiting the impact of this research. While software engineering practitioners could benefit from replicating research on their own data, this poses its own set of challenges, since performing replications requires a deep understanding of research methodologies and subtle nuances in software engineering data. Given that large language models (LLMs), such as GPT-4, show promise in tackling both software engineering- and science-related tasks, these models could help replicate and thus democratize empirical software engineering research.   In this paper, we examine GPT-4's abilities to perform replications of empirical software engineering research on new data. We study their ability to surface assumptions made in empirical software engineering research methodologies, as well as their ability to plan and generate code for analysis pipelines on seven empirical software engineering papers. We perform a user study with 14 participants with software engineering research expertise, who evaluate GPT-4-generated assumptions and analysis plans (i.e., a list of module specifications) from the papers. We find that GPT-4 is able to surface correct assumptions, but struggles to generate ones that apply common knowledge about software engineering data. In a manual analysis of the generated code, we find that the GPT-4-generated code contains correct high-level logic, given a subset of the methodology. However, the code contains many small implementation-level errors, reflecting a lack of software engineering knowledge. Our findings have implications for leveraging LLMs for software engineering research as well as practitioner data scientists in software teams.",
    "Link": "http://arxiv.org/abs/2310.01727v3",
    "PDF Link": "http://arxiv.org/pdf/2310.01727v3"
  },
  {
    "Title": "Understanding and measuring software engineer behavior: What can we\n  learn from the behavioral sciences?",
    "Authors": "Allysson Allex Araújo, Marcos Kalinowski, Daniel Graziotin",
    "Published": "2024-06-05T14:59:40Z",
    "Summary": "This paper explores the intricate challenge of understanding and measuring software engineer behavior. More specifically, we revolve around a central question: How can we enhance our understanding of software engineer behavior? Grounded in the nuanced complexities addressed within Behavioral Software Engineering (BSE), we advocate for holistic methods that integrate quantitative measures, such as psychometric instruments, and qualitative data from diverse sources. Furthermore, we delve into the relevance of this challenge within national and international contexts, highlighting the increasing interest in understanding software engineer behavior. Real-world initiatives and academic endeavors are also examined to underscore the potential for advancing this research agenda and, consequently, refining software engineering practices based on behavioral aspects. Lastly, this paper addresses different ways to evaluate the progress of this challenge by leveraging methodological skills derived from behavioral sciences, ultimately contributing to a deeper understanding of software engineer behavior and software engineering practices.",
    "Link": "http://arxiv.org/abs/2406.03342v1",
    "PDF Link": "http://arxiv.org/pdf/2406.03342v1"
  },
  {
    "Title": "Dr Web: a modern, query-based web data retrieval engine",
    "Authors": "Ylli Prifti, Alessandro Provetti, Pasquale de Meo",
    "Published": "2025-02-18T16:37:20Z",
    "Summary": "This article introduces the Data Retrieval Web Engine (also referred to as doctor web), a flexible and modular tool for extracting structured data from web pages using a simple query language. We discuss the engineering challenges addressed during its development, such as dynamic content handling and messy data extraction. Furthermore, we cover the steps for making the DR Web Engine public, highlighting its open source potential.",
    "Link": "http://arxiv.org/abs/2504.05311v1",
    "PDF Link": "http://arxiv.org/pdf/2504.05311v1"
  },
  {
    "Title": "Digital Engineering Transformation with Trustworthy AI towards Industry\n  4.0: Emerging Paradigm Shifts",
    "Authors": "Jingwei Huang",
    "Published": "2023-01-03T05:17:06Z",
    "Summary": "Digital engineering transformation is a crucial process for the engineering paradigm shifts in the fourth industrial revolution (4IR), and artificial intelligence (AI) is a critical enabling technology in digital engineering transformation. This article discusses the following research questions: What are the fundamental changes in the 4IR? More specifically, what are the fundamental changes in engineering? What is digital engineering? What are the main uncertainties there? What is trustworthy AI? Why is it important today? What are emerging engineering paradigm shifts in the 4IR? What is the relationship between the data-intensive paradigm and digital engineering transformation? What should we do for digitalization? From investigating the pattern of industrial revolutions, this article argues that ubiquitous machine intelligence (uMI) is the defining power brought by the 4IR. Digitalization is a condition to leverage ubiquitous machine intelligence. Digital engineering transformation towards Industry 4.0 has three essential building blocks: digitalization of engineering, leveraging ubiquitous machine intelligence, and building digital trust and security. The engineering design community at large is facing an excellent opportunity to bring the new capabilities of ubiquitous machine intelligence and trustworthy AI principles, as well as digital trust, together in various engineering systems design to ensure the trustworthiness of systems in Industry 4.0.",
    "Link": "http://arxiv.org/abs/2301.00951v1",
    "PDF Link": "http://arxiv.org/pdf/2301.00951v1"
  },
  {
    "Title": "Towards Understanding the Impact of Data Bugs on Deep Learning Models in\n  Software Engineering",
    "Authors": "Mehil B Shah, Mohammad Masudur Rahman, Foutse Khomh",
    "Published": "2024-11-19T00:28:20Z",
    "Summary": "Deep learning (DL) techniques have achieved significant success in various software engineering tasks (e.g., code completion by Copilot). However, DL systems are prone to bugs from many sources, including training data. Existing literature suggests that bugs in training data are highly prevalent, but little research has focused on understanding their impacts on the models used in software engineering tasks. In this paper, we address this research gap through a comprehensive empirical investigation focused on three types of data prevalent in software engineering tasks: code-based, text-based, and metric-based. Using state-of-the-art baselines, we compare the models trained on clean datasets with those trained on datasets with quality issues and without proper preprocessing. By analysing the gradients, weights, and biases from neural networks under training, we identify the symptoms of data quality and preprocessing issues. Our analysis reveals that quality issues in code data cause biased learning and gradient instability, whereas problems in text data lead to overfitting and poor generalisation of models. On the other hand, quality issues in metric data result in exploding gradients and model overfitting, and inadequate preprocessing exacerbates these effects across all three data types. Finally, we demonstrate the validity and generalizability of our findings using six new datasets. Our research provides a better understanding of the impact and symptoms of data bugs in software engineering datasets. Practitioners and researchers can leverage these findings to develop better monitoring systems and data-cleaning methods to help detect and resolve data bugs in deep learning systems.",
    "Link": "http://arxiv.org/abs/2411.12137v1",
    "PDF Link": "http://arxiv.org/pdf/2411.12137v1"
  },
  {
    "Title": "Standardizing Knowledge Engineering Practices with a Reference\n  Architecture",
    "Authors": "Bradley P. Allen, Filip Ilievski",
    "Published": "2024-04-04T17:46:32Z",
    "Summary": "Knowledge engineering is the process of creating and maintaining knowledge-producing systems. Throughout the history of computer science and AI, knowledge engineering workflows have been widely used given the importance of high-quality knowledge for reliable intelligent agents. Meanwhile, the scope of knowledge engineering, as apparent from its target tasks and use cases, has been shifting, together with its paradigms such as expert systems, semantic web, and language modeling. The intended use cases and supported user requirements between these paradigms have not been analyzed globally, as new paradigms often satisfy prior pain points while possibly introducing new ones. The recent abstraction of systemic patterns into a boxology provides an opening for aligning the requirements and use cases of knowledge engineering with the systems, components, and software that can satisfy them best. This paper proposes a vision of harmonizing the best practices in the field of knowledge engineering by leveraging the software engineering methodology of creating reference architectures. We describe how a reference architecture can be iteratively designed and implemented to associate user needs with recurring systemic patterns, building on top of existing knowledge engineering workflows and boxologies. We provide a six-step roadmap that can enable the development of such an architecture, providing an initial design and outcome of the definition of architectural scope, selection of information sources, and analysis. We expect that following through on this vision will lead to well-grounded reference architectures for knowledge engineering, will advance the ongoing initiatives of organizing the neurosymbolic knowledge engineering space, and will build new links to the software architectures and data science communities.",
    "Link": "http://arxiv.org/abs/2404.03624v1",
    "PDF Link": "http://arxiv.org/pdf/2404.03624v1"
  },
  {
    "Title": "Performance Oriented Query Processing In GEO Based Location Search\n  Engines",
    "Authors": "M. Umamaheswari, S. Sivasubramanian",
    "Published": "2010-05-06T10:09:09Z",
    "Summary": "Geographic location search engines allow users to constrain and order search results in an intuitive manner by focusing a query on a particular geographic region. Geographic search technology, also called location search, has recently received significant interest from major search engine companies. Academic research in this area has focused primarily on techniques for extracting geographic knowledge from the web. In this paper, we study the problem of efficient query processing in scalable geographic search engines. Query processing is a major bottleneck in standard web search engines, and the main reason for the thousands of machines used by the major engines. Geographic search engine query processing is different in that it requires a combination of text and spatial data processing techniques. We propose several algorithms for efficient query processing in geographic search engines, integrate them into an existing web search query processor, and evaluate them on large sets of real data and query traces.",
    "Link": "http://arxiv.org/abs/1005.0961v1",
    "PDF Link": "http://arxiv.org/pdf/1005.0961v1"
  },
  {
    "Title": "Identifying Talented Software Engineering Students through Data-driven\n  Skill Assessment",
    "Authors": "Jun Lin, Han Yu, Zhiqi Shen",
    "Published": "2014-11-23T06:20:33Z",
    "Summary": "For software development companies, one of the most important objectives is to identify and acquire talented software engineers in order to maintain a skilled team that can produce competitive products. Traditional approaches for finding talented young software engineers are mainly through programming contests of various forms which mostly test participants' programming skills. However, successful software engineering in practice requires a wider range of skills from team members including analysis, design, programming, testing, communication, collaboration, and self-management, etc. In this paper, we explore potential ways to identify talented software engineering students in a data-driven manner through an Agile Project Management (APM) platform. Through our proposed HASE online APM tool, we conducted a study involving 21 Scrum teams consisting of over 100 undergraduate software engineering students in multi-week coursework projects in 2014. During this study, students performed over 10,000 ASD activities logged by HASE. We demonstrate the possibility and potentials of this new research direction, and discuss its implications for software engineering education and industry recruitment.",
    "Link": "http://arxiv.org/abs/1411.6197v1",
    "PDF Link": "http://arxiv.org/pdf/1411.6197v1"
  },
  {
    "Title": "Imitation of Life: A Search Engine for Biologically Inspired Design",
    "Authors": "Hen Emuna, Nadav Borenstein, Xin Qian, Hyeonsu Kang, Joel Chan, Aniket Kittur, Dafna Shahaf",
    "Published": "2023-12-20T00:45:27Z",
    "Summary": "Biologically Inspired Design (BID), or Biomimicry, is a problem-solving methodology that applies analogies from nature to solve engineering challenges. For example, Speedo engineers designed swimsuits based on shark skin. Finding relevant biological solutions for real-world problems poses significant challenges, both due to the limited biological knowledge engineers and designers typically possess and to the limited BID resources. Existing BID datasets are hand-curated and small, and scaling them up requires costly human annotations.   In this paper, we introduce BARcode (Biological Analogy Retriever), a search engine for automatically mining bio-inspirations from the web at scale. Using advances in natural language understanding and data programming, BARcode identifies potential inspirations for engineering challenges. Our experiments demonstrate that BARcode can retrieve inspirations that are valuable to engineers and designers tackling real-world problems, as well as recover famous historical BID examples. We release data and code; we view BARcode as a step towards addressing the challenges that have historically hindered the practical application of BID to engineering innovation.",
    "Link": "http://arxiv.org/abs/2312.12681v1",
    "PDF Link": "http://arxiv.org/pdf/2312.12681v1"
  },
  {
    "Title": "Generative AI and Empirical Software Engineering: A Paradigm Shift",
    "Authors": "Christoph Treude, Margaret-Anne Storey",
    "Published": "2025-02-12T04:13:07Z",
    "Summary": "The widespread adoption of generative AI in software engineering marks a paradigm shift, offering new opportunities to design and utilize software engineering tools while influencing both developers and the artifacts they create. Traditional empirical methods in software engineering, including quantitative, qualitative, and mixed-method approaches, are well established. However, this paradigm shift introduces novel data types and redefines many concepts in the software engineering process. The roles of developers, users, agents, and researchers increasingly overlap, blurring the distinctions between these social and technical actors within the field.   This paper examines how integrating AI into software engineering challenges traditional research paradigms. It focuses on the research phenomena that we investigate, the methods and theories that we employ, the data we analyze, and the threats to validity that emerge in this new context. Through this exploration, our goal is to understand how AI adoption disrupts established software development practices that creates new opportunities for empirical software engineering research.",
    "Link": "http://arxiv.org/abs/2502.08108v1",
    "PDF Link": "http://arxiv.org/pdf/2502.08108v1"
  },
  {
    "Title": "BigDataBench: a Big Data Benchmark Suite from Web Search Engines",
    "Authors": "Wanling Gao, Yuqing Zhu, Zhen Jia, Chunjie Luo, Lei Wang, Zhiguo Li, Jianfeng Zhan, Yong Qi, Yongqiang He, Shiming Gong, Xiaona Li, Shujie Zhang, Bizhu Qiu",
    "Published": "2013-07-01T10:27:48Z",
    "Summary": "This paper presents our joint research efforts on big data benchmarking with several industrial partners. Considering the complexity, diversity, workload churns, and rapid evolution of big data systems, we take an incremental approach in big data benchmarking. For the first step, we pay attention to search engines, which are the most important domain in Internet services in terms of the number of page views and daily visitors. However, search engine service providers treat data, applications, and web access logs as business confidentiality, which prevents us from building benchmarks. To overcome those difficulties, with several industry partners, we widely investigated the open source solutions in search engines, and obtained the permission of using anonymous Web access logs. Moreover, with two years' great efforts, we created a sematic search engine named ProfSearch (available from http://prof.ict.ac.cn). These efforts pave the path for our big data benchmark suite from search engines---BigDataBench, which is released on the web page (http://prof.ict.ac.cn/BigDataBench). We report our detailed analysis of search engine workloads, and present our benchmarking methodology. An innovative data generation methodology and tool are proposed to generate scalable volumes of big data from a small seed of real data, preserving semantics and locality of data. Also, we preliminarily report two case studies using BigDataBench for both system and architecture researches.",
    "Link": "http://arxiv.org/abs/1307.0320v1",
    "PDF Link": "http://arxiv.org/pdf/1307.0320v1"
  },
  {
    "Title": "Physics-informed neural networks for predicting gas flow dynamics and\n  unknown parameters in diesel engines",
    "Authors": "Kamaljyoti Nath, Xuhui Meng, Daniel J Smith, George Em Karniadakis",
    "Published": "2023-04-26T19:37:18Z",
    "Summary": "This paper presents a physics-informed neural network (PINN) approach for monitoring the health of diesel engines. The aim is to evaluate the engine dynamics, identify unknown parameters in a \"mean value\" model, and anticipate maintenance requirements. The PINN model is applied to diesel engines with a variable-geometry turbocharger and exhaust gas recirculation, using measurement data of selected state variables. The results demonstrate the ability of the PINN model to predict simultaneously both unknown parameters and dynamics accurately with both clean and noisy data, and the importance of the self-adaptive weight in the loss function for faster convergence. The input data for these simulations are derived from actual engine running conditions, while the outputs are simulated data, making this a practical case study of PINN's ability to predict real-world dynamical systems. The mean value model of the diesel engine incorporates empirical formulae to represent certain states, but these formulae may not be generalizable to other engines. To address this, the study considers the use of deep neural networks (DNNs) in addition to the PINN model. The DNNs are trained using laboratory test data and are used to model the engine-specific empirical formulae in the mean value model, allowing for a more flexible and adaptive representation of the engine's states. In other words, the mean value model uses both the PINN model and the DNNs to represent the engine's states, with the PINN providing a physics-based understanding of the engine's overall dynamics and the DNNs offering a more engine-specific and adaptive representation of the empirical formulae. By combining these two approaches, the study aims to offer a comprehensive and versatile approach to monitoring the health and performance of diesel engines.",
    "Link": "http://arxiv.org/abs/2304.13799v2",
    "PDF Link": "http://arxiv.org/pdf/2304.13799v2"
  },
  {
    "Title": "SIED, a Data Privacy Engineering Framework",
    "Authors": "Kato Mivule",
    "Published": "2013-09-25T17:04:59Z",
    "Summary": "While a number of data privacy techniques have been proposed in the recent years, a few frameworks have been suggested for the implementation of the data privacy process. Most of the proposed approaches are tailored towards implementing a specific data privacy algorithm but not the overall data privacy engineering and design process. Therefore, as a contribution, this study proposes SIED (Specification, Implementation, Evaluation, and Dissemination), a conceptual framework that takes a holistic approach to the data privacy engineering procedure by looking at the specifications, implementation, evaluation, and finally, dissemination of the privatized data sets.",
    "Link": "http://arxiv.org/abs/1309.6576v1",
    "PDF Link": "http://arxiv.org/pdf/1309.6576v1"
  },
  {
    "Title": "Feature Engineering on LMS Data to Optimize Student Performance\n  Prediction",
    "Authors": "Keith Hubbard, Sheilla Amponsah",
    "Published": "2025-04-03T13:11:42Z",
    "Summary": "Nearly every educational institution uses a learning management system (LMS), often producing terabytes of data generated by thousands of people. We examine LMS grade and login data from a regional comprehensive university, specifically documenting key considerations for engineering features from these data when trying to predict student performance. We specifically document changes to LMS data patterns since Covid-19, which are critical for data scientists to account for when using historic data. We compare numerous engineered features and approaches to utilizing those features for machine learning. We finish with a summary of the implications of including these features into more comprehensive student performance models.",
    "Link": "http://arxiv.org/abs/2504.02916v1",
    "PDF Link": "http://arxiv.org/pdf/2504.02916v1"
  },
  {
    "Title": "Weighted Unsupervised Domain Adaptation Considering Geometry Features\n  and Engineering Performance of 3D Design Data",
    "Authors": "Seungyeon Shin, Namwoo Kang",
    "Published": "2023-09-08T00:26:44Z",
    "Summary": "The product design process in manufacturing involves iterative design modeling and analysis to achieve the target engineering performance, but such an iterative process is time consuming and computationally expensive. Recently, deep learning-based engineering performance prediction models have been proposed to accelerate design optimization. However, they only guarantee predictions on training data and may be inaccurate when applied to new domain data. In particular, 3D design data have complex features, which means domains with various distributions exist. Thus, the utilization of deep learning has limitations due to the heavy data collection and training burdens. We propose a bi-weighted unsupervised domain adaptation approach that considers the geometry features and engineering performance of 3D design data. It is specialized for deep learning-based engineering performance predictions. Domain-invariant features can be extracted through an adversarial training strategy by using hypothesis discrepancy, and a multi-output regression task can be performed with the extracted features to predict the engineering performance. In particular, we present a source instance weighting method suitable for 3D design data to avoid negative transfers. The developed bi-weighting strategy based on the geometry features and engineering performance of engineering structures is incorporated into the training process. The proposed model is tested on a wheel impact analysis problem to predict the magnitude of the maximum von Mises stress and the corresponding location of 3D road wheels. This mechanism can reduce the target risk for unlabeled target domains on the basis of weighted multi-source domain knowledge and can efficiently replace conventional finite element analysis.",
    "Link": "http://arxiv.org/abs/2309.04499v1",
    "PDF Link": "http://arxiv.org/pdf/2309.04499v1"
  },
  {
    "Title": "A Supervised Machine-Learning Approach For Turboshaft Engine Dynamic\n  Modeling Under Real Flight Conditions",
    "Authors": "Damiano Paniccia, Francesco Aldo Tucci, Joel Guerrero, Luigi Capone, Nicoletta Sanguini, Tommaso Benacchio, Luigi Bottasso",
    "Published": "2025-02-19T21:45:53Z",
    "Summary": "Rotorcraft engines are highly complex, nonlinear thermodynamic systems that operate under varying environmental and flight conditions. Simulating their dynamics is crucial for design, fault diagnostics, and deterioration control phases, and requires robust and reliable control systems to estimate engine performance throughout flight envelope. However, the development of detailed physical models of the engine based on numerical simulations is a very challenging task due to the complex and entangled physics driving the engine. In this scenario, data-driven machine-learning techniques are of great interest to the aircraft engine community, due to their ability to describe nonlinear systems' dynamic behavior and enable online performance estimation, achieving excellent results with accuracy competitive with the state of the art. In this work, we explore different Neural Network architectures to model the turboshaft engine of Leonardo's AW189P4 prototype, aiming to predict the engine torque. The models are trained on an extensive database of real flight tests featuring a variety of operational maneuvers performed under different flight conditions, providing a comprehensive representation of the engine's performance. To complement the neural network approach, we apply Sparse Identification of Nonlinear Dynamics (SINDy) to derive a low-dimensional dynamical model from the available data, describing the relationship between fuel flow and engine torque. The resulting model showcases SINDy's capability to recover the actual physics underlying the engine dynamics and demonstrates its potential for investigating more complex aspects of the engine. The results prove that data-driven engine models can exploit a wider range of parameters than standard transfer function-based approaches, enabling the use of trained schemes to simulate nonlinear effects in different engines and helicopters.",
    "Link": "http://arxiv.org/abs/2502.14120v2",
    "PDF Link": "http://arxiv.org/pdf/2502.14120v2"
  },
  {
    "Title": "The Rise of Quantum Internet Computing",
    "Authors": "Seng W. Loke",
    "Published": "2022-08-01T10:36:13Z",
    "Summary": "This article highlights quantum Internet computing as referring to distributed quantum computing over the quantum Internet, analogous to (classical) Internet computing involving (classical) distributed computing over the (classical) Internet. Relevant to quantum Internet computing would be areas of study such as quantum protocols for distributed nodes using quantum information for computations, quantum cloud computing, delegated verifiable blind or private computing, non-local gates, and distributed quantum applications, over Internet-scale distances.",
    "Link": "http://arxiv.org/abs/2208.00733v1",
    "PDF Link": "http://arxiv.org/pdf/2208.00733v1"
  },
  {
    "Title": "Unconventional Quantum Computing Devices",
    "Authors": "Seth Lloyd",
    "Published": "2000-03-31T22:07:23Z",
    "Summary": "This paper investigates a variety of unconventional quantum computation devices, including fermionic quantum computers and computers that exploit nonlinear quantum mechanics. It is shown that unconventional quantum computing devices can in principle compute some quantities more rapidly than `conventional' quantum computers.",
    "Link": "http://arxiv.org/abs/quant-ph/0003151v1",
    "PDF Link": "http://arxiv.org/pdf/quant-ph/0003151v1"
  },
  {
    "Title": "Geometrical perspective on quantum states and quantum computation",
    "Authors": "Zeqian Chen",
    "Published": "2013-11-20T02:23:12Z",
    "Summary": "We interpret quantum computing as a geometric evolution process by reformulating finite quantum systems via Connes' noncommutative geometry. In this formulation, quantum states are represented as noncommutative connections, while gauge transformations on the connections play a role of unitary quantum operations. Thereby, a geometrical model for quantum computation is presented, which is equivalent to the quantum circuit model. This result shows a geometric way of realizing quantum computing and as such, provides an alternative proposal of building a quantum computer.",
    "Link": "http://arxiv.org/abs/1311.4939v1",
    "PDF Link": "http://arxiv.org/pdf/1311.4939v1"
  },
  {
    "Title": "Quantum Computation and Quantum Information",
    "Authors": "Yazhen Wang",
    "Published": "2012-10-02T11:47:37Z",
    "Summary": "Quantum computation and quantum information are of great current interest in computer science, mathematics, physical sciences and engineering. They will likely lead to a new wave of technological innovations in communication, computation and cryptography. As the theory of quantum physics is fundamentally stochastic, randomness and uncertainty are deeply rooted in quantum computation, quantum simulation and quantum information. Consequently quantum algorithms are random in nature, and quantum simulation utilizes Monte Carlo techniques extensively. Thus statistics can play an important role in quantum computation and quantum simulation, which in turn offer great potential to revolutionize computational statistics. While only pseudo-random numbers can be generated by classical computers, quantum computers are able to produce genuine random numbers; quantum computers can exponentially or quadratically speed up median evaluation, Monte Carlo integration and Markov chain simulation. This paper gives a brief review on quantum computation, quantum simulation and quantum information. We introduce the basic concepts of quantum computation and quantum simulation and present quantum algorithms that are known to be much faster than the available classic algorithms. We provide a statistical framework for the analysis of quantum algorithms and quantum simulation.",
    "Link": "http://arxiv.org/abs/1210.0736v1",
    "PDF Link": "http://arxiv.org/pdf/1210.0736v1"
  },
  {
    "Title": "Probabilistic Process Algebra to Unifying Quantum and Classical\n  Computing in Closed Systems",
    "Authors": "Yong Wang",
    "Published": "2016-10-08T08:48:09Z",
    "Summary": "We have unified quantum and classical computing in open quantum systems called qACP which is a quantum generalization of process algebra ACP. But, an axiomatization for quantum and classical processes with an assumption of closed quantum systems is still missing. For closed quantum systems, unitary operator, quantum measurement and quantum entanglement are three basic components for quantum computing. This leads to probability unavoidable. Along the solution of qACP to unify quantum and classical computing in open quantum systems, we unify quantum and classical computing with an assumption of closed systems under the framework of ACP-like probabilistic process algebra. This unification make it can be used widely in verification for quantum and classical computing mixed systems, such as most quantum communication protocols.",
    "Link": "http://arxiv.org/abs/1610.02500v1",
    "PDF Link": "http://arxiv.org/pdf/1610.02500v1"
  },
  {
    "Title": "Google Quantum AI's Quest for Error-Corrected Quantum Computers",
    "Authors": "M. AbuGhanem",
    "Published": "2024-09-23T15:56:14Z",
    "Summary": "Quantum computers stand at the forefront of technological innovation, offering exponential computational speed-ups that challenge classical computing capabilities. At the cutting edge of this transformation is Google Quantum AI, a leader in driving forward the development of practical quantum computers. This article provides a comprehensive review of Google Quantum AI's pivotal role in the quantum computing landscape over the past decade, emphasizing their significant strides towards achieving quantum computational supremacy. By exploring their advancements and contributions in quantum hardware, quantum software, error correction, and quantum algorithms, this study highlights the transformative impact of Google Quantum AI's initiatives in shaping the future of quantum computing technology.",
    "Link": "http://arxiv.org/abs/2410.00917v1",
    "PDF Link": "http://arxiv.org/pdf/2410.00917v1"
  },
  {
    "Title": "Optimal Stochastic Resource Allocation for Distributed Quantum Computing",
    "Authors": "Napat Ngoenriang, Minrui Xu, Sucha Supittayapornpong, Dusit Niyato, Han Yu, Xuemin, Shen",
    "Published": "2022-09-16T02:37:32Z",
    "Summary": "With the advent of interconnected quantum computers, i.e., distributed quantum computing (DQC), multiple quantum computers can now collaborate via quantum networks to perform massively complex computational tasks. However, DQC faces problems sharing quantum information because it cannot be cloned or duplicated between quantum computers. Thanks to advanced quantum mechanics, quantum computers can teleport quantum information across quantum networks. However, challenges to utilizing efficiently quantum resources, e.g., quantum computers and quantum channels, arise in DQC due to their capabilities and properties, such as uncertain qubit fidelity and quantum channel noise. In this paper, we propose a resource allocation scheme for DQC based on stochastic programming to minimize the total deployment cost for quantum resources. Essentially, the two-stage stochastic programming model is formulated to handle the uncertainty of quantum computing demands, computing power, and fidelity in quantum networks. The performance evaluation demonstrates the effectiveness and ability of the proposed scheme to balance the utilization of quantum computers and on-demand quantum computers while minimizing the overall cost of provisioning under uncertainty.",
    "Link": "http://arxiv.org/abs/2210.02886v1",
    "PDF Link": "http://arxiv.org/pdf/2210.02886v1"
  },
  {
    "Title": "Quantum Computational Complexity",
    "Authors": "John Watrous",
    "Published": "2008-04-21T20:07:38Z",
    "Summary": "This article surveys quantum computational complexity, with a focus on three fundamental notions: polynomial-time quantum computations, the efficient verification of quantum proofs, and quantum interactive proof systems. Properties of quantum complexity classes based on these notions, such as BQP, QMA, and QIP, are presented. Other topics in quantum complexity, including quantum advice, space-bounded quantum computation, and bounded-depth quantum circuits, are also discussed.",
    "Link": "http://arxiv.org/abs/0804.3401v1",
    "PDF Link": "http://arxiv.org/pdf/0804.3401v1"
  },
  {
    "Title": "Quantum Computers and Quantum Computer Languages: Quantum Assembly\n  Language and Quantum C Language",
    "Authors": "Stephen Blaha",
    "Published": "2002-01-18T15:08:05Z",
    "Summary": "We show a representation of Quantum Computers defines Quantum Turing Machines with associated Quantum Grammars. We then create examples of Quantum Grammars. Lastly we develop an algebraic approach to high level Quantum Languages using Quantum Assembly language and Quantum C language as examples.",
    "Link": "http://arxiv.org/abs/quant-ph/0201082v1",
    "PDF Link": "http://arxiv.org/pdf/quant-ph/0201082v1"
  },
  {
    "Title": "IBM Quantum Computers: Evolution, Performance, and Future Directions",
    "Authors": "M. AbuGhanem",
    "Published": "2024-09-17T07:50:50Z",
    "Summary": "Quantum computers represent a transformative frontier in computational technology, promising exponential speedups beyond classical computing limits. IBM Quantum has led significant advancements in both hardware and software, providing access to quantum hardware via IBM Cloud since 2016, achieving a milestone with the world's first accessible quantum computer. This article explores IBM's quantum computing journey, focusing on the development of practical quantum computers. We summarize the evolution and advancements of IBM Quantum's processors across generations, including their recent breakthrough surpassing the 1,000-qubit barrier. The paper reviews detailed performance metrics across various hardware, tracing their evolution over time and highlighting IBM Quantum's transition from the noisy intermediate-scale quantum (NISQ) computing era towards fault-tolerant quantum computing capabilities.",
    "Link": "http://arxiv.org/abs/2410.00916v1",
    "PDF Link": "http://arxiv.org/pdf/2410.00916v1"
  },
  {
    "Title": "Duality quantum computer and the efficient quantum simulations",
    "Authors": "Shi-Jie Wei, Gui-Lu Long",
    "Published": "2015-07-12T07:50:13Z",
    "Summary": "In this paper, we firstly briefly review the duality quantum computer. Distinctly, the generalized quantum gates, the basic evolution operators in a duality quantum computer are no longer unitary, and they can be expressed in terms of linear combinations of unitary operators. All linear bounded operators can be realized in a duality quantum computer, and unitary operators are just the extreme points of the set of generalized quantum gates. A d-slits duality quantum computer can be realized in an ordinary quantum computer with an additional qudit using the duality quantum computing mode. Duality quantum computer provides flexibility and clear physical picture in designing quantum algorithms, serving as a useful bridge between quantum and classical algorithms. In this review, we will show that duality quantum computer can simulate quantum systems more efficiently than ordinary quantum computers by providing descriptions of the recent efficient quantum simulation algorithms of Childs et al [Quantum Information & Computation, 12(11-12): 901-924 (2012)] for the fast simulation of quantum systems with a sparse Hamiltonian, and the quantum simulation algorithm by Berry et al [Phys. Rev. Lett. 114, 090502 (2015)], which provides exponential improvement in precision for simulating systems with a sparse Hamiltonian.",
    "Link": "http://arxiv.org/abs/1507.03200v1",
    "PDF Link": "http://arxiv.org/pdf/1507.03200v1"
  },
  {
    "Title": "Simulations of Many-Body Quantum Systems by a Quantum Computer",
    "Authors": "Stephen Wiesner",
    "Published": "1996-03-26T14:44:18Z",
    "Summary": "We suggest that quantum computers can solve quantum many-body problems that are impracticable to solve on a classical computer.",
    "Link": "http://arxiv.org/abs/quant-ph/9603028v1",
    "PDF Link": "http://arxiv.org/pdf/quant-ph/9603028v1"
  },
  {
    "Title": "A bird's eye view of quantum computers",
    "Authors": "Giuliano Benenti, Giuliano Strini",
    "Published": "2007-03-13T15:47:07Z",
    "Summary": "Quantum computers are discussed in the general framework of computation, the laws of physics and the foundations of quantum mechanics.",
    "Link": "http://arxiv.org/abs/quant-ph/0703105v1",
    "PDF Link": "http://arxiv.org/pdf/quant-ph/0703105v1"
  },
  {
    "Title": "A Note on Bulk Quantum Turing Machine",
    "Authors": "Tetsushi Matsui",
    "Published": "2004-11-12T18:56:15Z",
    "Summary": "Recently, among experiments for realization of quantum computers, NMR quantum computers have achieved the most impressive succession. There is a model of the NMR quantum computation,namely Atsumi and Nishino's bulk quantum Turing Machine. It assumes, however, an unnatural assumption with quantum mechanics. We, then, define a more natural and quantum mechanically realizable modified bulk quantum Turing Machine, and show its computational ability by comparing complexity classes with quantum Turing Machine's counter part.",
    "Link": "http://arxiv.org/abs/cs/0411037v2",
    "PDF Link": "http://arxiv.org/pdf/cs/0411037v2"
  },
  {
    "Title": "Pulse controlled noise suppressed quantum computation",
    "Authors": "Lu-Ming Duan, Guang-Can Guo",
    "Published": "1998-07-26T10:23:48Z",
    "Summary": "To make arbitrarily accurate quantum computation possible, practical realization of quantum computers will require suppressing noise in quantum memory and gate operations to make it below a threshold value. A scheme based on realistic quantum computer models is described for suppressing noise in quantum computation without the cost of stringent quantum computing resources.",
    "Link": "http://arxiv.org/abs/quant-ph/9807072v1",
    "PDF Link": "http://arxiv.org/pdf/quant-ph/9807072v1"
  },
  {
    "Title": "From Distributed Quantum Computing to Quantum Internet Computing: an\n  Overview",
    "Authors": "Seng W. Loke",
    "Published": "2022-08-22T07:58:59Z",
    "Summary": "The possibility of quantum computing has been proposed decades ago, at least as far back as the 1980s, and distributed quantum computing has been studied around two decades ago. Recent times have seen experimental successes and advances in quantum computer hardware and in quantum networking, leading towards the quantum Internet. We provide in this paper an overview of concepts and ideas in distributed quantum computing since over two decades ago as well as look at recent efforts in the area, and consider how, with the development of the quantum Internet, distributed quantum computing is evolving into quantum Internet computing.",
    "Link": "http://arxiv.org/abs/2208.10127v2",
    "PDF Link": "http://arxiv.org/pdf/2208.10127v2"
  },
  {
    "Title": "Quantum Cybernetics and Complex Quantum Systems Science - A Quantum\n  Connectionist Exploration",
    "Authors": "Carlos Pedro Gonçalves",
    "Published": "2014-02-05T19:48:24Z",
    "Summary": "Quantum cybernetics and its connections to complex quantum systems science is addressed from the perspective of complex quantum computing systems. In this way, the notion of an autonomous quantum computing system is introduced in regards to quantum artificial intelligence, and applied to quantum artificial neural networks, considered as autonomous quantum computing systems, which leads to a quantum connectionist framework within quantum cybernetics for complex quantum computing systems. Several examples of quantum feedforward neural networks are addressed in regards to Boolean functions' computation, multilayer quantum computation dynamics, entanglement and quantum complementarity. The examples provide a framework for a reflection on the role of quantum artificial neural networks as a general framework for addressing complex quantum systems that perform network-based quantum computation, possible consequences are drawn regarding quantum technologies, as well as fundamental research in complex quantum systems science and quantum biology.",
    "Link": "http://arxiv.org/abs/1402.1141v1",
    "PDF Link": "http://arxiv.org/pdf/1402.1141v1"
  },
  {
    "Title": "Quantum Computing for Multi Period Asset Allocation",
    "Authors": "Queenie Sun, Nicholas Grablevsky, Huaizhang Deng, Pooya Azadi",
    "Published": "2024-10-15T19:04:29Z",
    "Summary": "Portfolio construction has been a long-standing topic of research in finance. The computational complexity and the time taken both increase rapidly with the number of investments in the portfolio. It becomes difficult, even impossible for classic computers to solve. Quantum computing is a new way of computing which takes advantage of quantum superposition and entanglement. It changes how such problems are approached and is not constrained by some of the classic computational complexity. Studies have shown that quantum computing can offer significant advantages over classical computing in many fields. The application of quantum computing has been constrained by the unavailability of actual quantum computers. In the past decade, there has been the rapid development of the large-scale quantum computer. However, software development for quantum computing is slow in many fields. In our study, we apply quantum computing to a multi-asset portfolio simulation. The simulation is based on historic data, covariance, and expected returns, all calculated using quantum computing. Although technically a solvable problem for classical computing, we believe the software development is important to the future application of quantum computing in finance. We conducted this study through simulation of a quantum computer and the use of Rensselaer Polytechnic Institute's IBM quantum computer.",
    "Link": "http://arxiv.org/abs/2410.11997v1",
    "PDF Link": "http://arxiv.org/pdf/2410.11997v1"
  },
  {
    "Title": "An Introduction to Quantum Computing",
    "Authors": "Noson S. Yanofsky",
    "Published": "2007-08-02T02:50:42Z",
    "Summary": "Quantum Computing is a new and exciting field at the intersection of mathematics, computer science and physics. It concerns a utilization of quantum mechanics to improve the efficiency of computation. Here we present a gentle introduction to some of the ideas in quantum computing. The paper begins by motivating the central ideas of quantum mechanics and quantum computation with simple toy models. From there we move on to a formal presentation of the small fraction of (finite dimensional) quantum mechanics that we will need for basic quantum computation. Central notions of quantum architecture (qubits and quantum gates) are described. The paper ends with a presentation of one of the simplest quantum algorithms: Deutsch's algorithm. Our presentation demands neither advanced mathematics nor advanced physics.",
    "Link": "http://arxiv.org/abs/0708.0261v1",
    "PDF Link": "http://arxiv.org/pdf/0708.0261v1"
  },
  {
    "Title": "Quantum Computing via The Bethe Ansatz",
    "Authors": "Yong Zhang",
    "Published": "2011-06-20T18:24:51Z",
    "Summary": "We recognize quantum circuit model of computation as factorisable scattering model and propose that a quantum computer is associated with a quantum many-body system solved by the Bethe ansatz. As an typical example to support our perspectives on quantum computation, we study quantum computing in one-dimensional nonrelativistic system with delta-function interaction, where the two-body scattering matrix satisfies the factorisation equation (the quantum Yang--Baxter equation) and acts as a parametric two-body quantum gate. We conclude by comparing quantum computing via the factorisable scattering with topological quantum computing.",
    "Link": "http://arxiv.org/abs/1106.3982v1",
    "PDF Link": "http://arxiv.org/pdf/1106.3982v1"
  },
  {
    "Title": "How to Compute Using Quantum Walks",
    "Authors": "Viv Kendon",
    "Published": "2020-04-03T01:51:03Z",
    "Summary": "Quantum walks are widely and successfully used to model diverse physical processes. This leads to computation of the models, to explore their properties. Quantum walks have also been shown to be universal for quantum computing. This is a more subtle result than is often appreciated, since it applies to computations run on qubit-based quantum computers in the single walker case, and physical quantum walks in the multi-walker case (quantum cellular automata). Nonetheless, quantum walks are powerful tools for quantum computing when correctly applied. In this paper, I explain the relationship between quantum walks as models and quantum walks as computational tools, and give some examples of their application in both contexts.",
    "Link": "http://arxiv.org/abs/2004.01329v1",
    "PDF Link": "http://arxiv.org/pdf/2004.01329v1"
  },
  {
    "Title": "A Uniform Quantum Computing Model Based on Virtual Quantum Processors",
    "Authors": "George Gesek",
    "Published": "2023-02-24T17:07:37Z",
    "Summary": "Quantum Computers, one fully realized, can represent an exponential boost in computing power. However, the computational power of the current quantum computers, referred to as Noisy Internediate Scale Quantum, or NISQ, is severely limited because of environmental and intrinsic noise, as well as the very low connectivity between qubits compared to their total amount. We propose a virtual quantum processor that emulates a generic hybrid quantum machine which can serve as a logical version of quantum computing hardware. This hybrid classical quantum machine powers quantum-logical computations which are substitutable by future native quantum processors.",
    "Link": "http://arxiv.org/abs/2302.12750v1",
    "PDF Link": "http://arxiv.org/pdf/2302.12750v1"
  },
  {
    "Title": "Quantum computer: an appliance for playing market games",
    "Authors": "Edward W. Piotrowski, Jan Sladkowski",
    "Published": "2003-05-05T07:12:35Z",
    "Summary": "Recent development in quantum computation and quantum information theory allows to extend the scope of game theory for the quantum world. The authors have recently proposed a quantum description of financial market in terms of quantum game theory. The paper contain an analysis of such markets that shows that there would be advantage in using quantum computers and quantum strategies.",
    "Link": "http://arxiv.org/abs/quant-ph/0305017v1",
    "PDF Link": "http://arxiv.org/pdf/quant-ph/0305017v1"
  },
  {
    "Title": "Integrable Quantum Computation",
    "Authors": "Yong Zhang",
    "Published": "2011-11-16T20:44:35Z",
    "Summary": "Integrable quantum computation is defined as quantum computing via the integrable condition, in which two-qubit gates are either nontrivial unitary solutions of the Yang--Baxter equation or the Swap gate (permutation). To make the definition clear, in this article, we explore the physics underlying the quantum circuit model, and then present a unified description on both quantum computing via the Bethe ansatz and quantum computing via the Yang--Baxter equation.",
    "Link": "http://arxiv.org/abs/1111.3940v1",
    "PDF Link": "http://arxiv.org/pdf/1111.3940v1"
  },
  {
    "Title": "Quantum Computational Supremacy",
    "Authors": "Aram W Harrow, Ashley Montanaro",
    "Published": "2018-09-20T01:31:16Z",
    "Summary": "The field of quantum algorithms aims to find ways to speed up the solution of computational problems by using a quantum computer. A key milestone in this field will be when a universal quantum computer performs a computational task that is beyond the capability of any classical computer, an event known as quantum supremacy. This would be easier to achieve experimentally than full-scale quantum computing, but involves new theoretical challenges. Here we present the leading proposals to achieve quantum supremacy, and discuss how we can reliably compare the power of a classical computer to the power of a quantum computer.",
    "Link": "http://arxiv.org/abs/1809.07442v1",
    "PDF Link": "http://arxiv.org/pdf/1809.07442v1"
  },
  {
    "Title": "Quantum Computation",
    "Authors": "Barry C Sanders",
    "Published": "2024-08-10T05:59:33Z",
    "Summary": "This chapter summarizes quantum computation, including the motivation for introducing quantum resources into computation and how quantum computation is done. Finally, this chapter articulates advantages and limitations of quantum computation, both fundamental and practical.",
    "Link": "http://arxiv.org/abs/2408.05448v2",
    "PDF Link": "http://arxiv.org/pdf/2408.05448v2"
  },
  {
    "Title": "Post-Hartree-Fock method in Quantum Chemistry for Quantum Computer",
    "Authors": "Yutaka Shikano, Hiroshi C. Watanabe, Ken M. Nakanishi, Yu-ya Ohnishi",
    "Published": "2020-11-03T07:46:13Z",
    "Summary": "Quantum computational chemistry is a potential application of quantum computers that is expected to effectively solve several quantum-chemistry problems, particularly the electronic structure problem. Quantum computational chemistry can be compared to the conventional computational devices. This review comprehensively investigates the applications and overview of quantum computational chemistry, including a review of the Hartree-Fock method for quantum information scientists. Quantum algorithms, quantum phase estimation, and variational quantum eigensolver, have been applied to the post-Hartree-Fock method.",
    "Link": "http://arxiv.org/abs/2011.01544v1",
    "PDF Link": "http://arxiv.org/pdf/2011.01544v1"
  },
  {
    "Title": "Phase Information in Quantum Oracle Computing",
    "Authors": "J. Machta",
    "Published": "1998-05-07T20:50:15Z",
    "Summary": "Computational devices may be supplied with external sources of information (oracles). Quantum oracles may transmit phase information which is available to a quantum computer but not a classical computer. One consequence of this observation is that there is an oracle which is of no assistance to a classical computer but which allows a quantum computer to solve undecidable problems. Thus useful relativized separations between quantum and classical complexity classes must exclude the transmission of phase information from oracle to computer.",
    "Link": "http://arxiv.org/abs/quant-ph/9805022v1",
    "PDF Link": "http://arxiv.org/pdf/quant-ph/9805022v1"
  },
  {
    "Title": "Covert Quantum Internet",
    "Authors": "Kamil Bradler, George Siopsis, Alex Wozniakowski",
    "Published": "2017-04-24T15:25:18Z",
    "Summary": "We apply covert quantum communication based on entanglement generated from the Minkowski vacuum to the setting of quantum computation and quantum networks. Our approach hides the generation and distribution of entanglement in quantum networks by taking advantage of relativistic quantum effects. We devise a suite of covert quantum teleportation protocols that utilize the shared entanglement, local operations, and covert classical communication to transfer or process quantum information in stealth. As an application of our covert suite, we construct two prominent examples of measurement-based quantum computation, namely the teleportation-based quantum computer and the one-way quantum computer. In the latter case we explore the covert generation of graph states, and subsequently outline a protocol for the covert implementation of universal blind quantum computation.",
    "Link": "http://arxiv.org/abs/1704.07281v1",
    "PDF Link": "http://arxiv.org/pdf/1704.07281v1"
  },
  {
    "Title": "Photonic Quantum Computing",
    "Authors": "Jacquiline Romero, Gerard Milburn",
    "Published": "2024-04-04T11:09:04Z",
    "Summary": "Photonic quantum computation refers to quantum computation that uses photons as the physical system for doing the quantum computation. Photons are ideal quantum systems because they operate at room temperature, and photonic technologies are relatively mature. The field is largely divided between discrete- and continuous-variable photonic quantum computation. In discrete-variable (DV) photonic quantum computation, quantum information is represented by one or more modal properties (e.g. polarization) that take on distinct values from a finite set. Quantum information is processed via operations on these modal properties and eventually measured using single photon detectors. In continuous-variable (CV) photonic quantum computation, quantum information is represented by properties of the electromagnetic field that take on any value in an interval (e.g. position). The electromagnetic field is transformed via Gaussian and non-Gaussian operations, and then detected via homodyne detection. Both CV and DV photonic quantum computation have been realized experimentally and they each have a unique set of challenges that need to be overcome to achieve scalable photonic universal quantum computation. This article is an introduction to photonic quantum computing, charting its development from the early days of linear optical quantum computing to recent developments in quantum machine learning.",
    "Link": "http://arxiv.org/abs/2404.03367v1",
    "PDF Link": "http://arxiv.org/pdf/2404.03367v1"
  },
  {
    "Title": "Photonic Quantum Computers",
    "Authors": "M. AbuGhanem",
    "Published": "2024-09-12T17:16:38Z",
    "Summary": "In the pursuit of scalable and fault-tolerant quantum computing architectures, photonic-based quantum computers have emerged as a leading frontier. This article provides a comprehensive overview of advancements in photonic quantum computing, developed by leading industry players, examining current performance, architectural designs, and strategies for developing large-scale, fault-tolerant photonic quantum computers. It also highlights recent groundbreaking experiments that leverage the unique advantages of photonic technologies, underscoring their transformative potential. This review captures a pivotal moment of photonic quantum computing in the noisy intermediate-scale quantum (NISQ) era, offering insights into how photonic quantum computers might reshape the future of quantum computing.",
    "Link": "http://arxiv.org/abs/2409.08229v1",
    "PDF Link": "http://arxiv.org/pdf/2409.08229v1"
  },
  {
    "Title": "Quantum Computing in Transport Science: A Review",
    "Authors": "Chence Niu, Elnaz Irannezhad, Casey Myers, Vinayak Dixit",
    "Published": "2025-03-27T09:28:33Z",
    "Summary": "Quantum computing, leveraging the principles of quantum mechanics, has been found to significantly enhance computational capabilities in principle, in some cases beyond classical computing limits. This paper explores quantum computing's potential to address complex, large-scale problems in transportation systems. It focuses on three principal paradigms: Gate-based quantum computing, Quantum annealing, and Quantum machine learning, which, though based on gate-based quantum computing, is treated as distinct due to its unique methods and applications. Each paradigm's foundational concepts, practical applications, and potential impacts on the field are discussed to provide a comprehensive overview of quantum computing strategies and their future implications.",
    "Link": "http://arxiv.org/abs/2503.21302v1",
    "PDF Link": "http://arxiv.org/pdf/2503.21302v1"
  },
  {
    "Title": "Quantum computer and its quasiclassical model",
    "Authors": "Timur F. Kamalov",
    "Published": "2001-09-28T08:01:12Z",
    "Summary": "Could the theories with hidden variables be employed for creation of a quantum computer? A particular scheme of quasiclassical model quantum computer structure is describe.",
    "Link": "http://arxiv.org/abs/quant-ph/0109152v1",
    "PDF Link": "http://arxiv.org/pdf/quant-ph/0109152v1"
  },
  {
    "Title": "Research on quantum compilation of neutral atom quantum computing\n  platform",
    "Authors": "Chongyuan Xu",
    "Published": "2025-01-09T14:20:32Z",
    "Summary": "Quantum compilation is the process of decomposing high-level quantum algorithms or arbitrary unitary operations into quantum circuits composed of a specific set of quantum gates. Neutral atom quantum computing platform is a quantum computing implementation method with high controllability and scalability, but its quantum compilation method is not mature. We systematically review the quantum compilation methods based on matrix decomposition, and propose a compilation algorithm suitable for neutral atom quantum computing, which can effectively decompose any unitary operation into a series of quantum gates suitable for the neutral atom platform, and ensure that the generated quantum circuits can run directly on the platform.",
    "Link": "http://arxiv.org/abs/2501.05266v1",
    "PDF Link": "http://arxiv.org/pdf/2501.05266v1"
  },
  {
    "Title": "Quantum computer for dummies (in Russian)",
    "Authors": "Andrey Grozin",
    "Published": "2011-08-17T11:13:51Z",
    "Summary": "An introduction (in Russian) to quantum computers, quantum cryptography, and quantum teleportation for students who have no previous knowledge of these subjects, but know quantum mechanics. Several simple examples are considered in detail using the quantum computer emulator QCL.",
    "Link": "http://arxiv.org/abs/1108.3445v1",
    "PDF Link": "http://arxiv.org/pdf/1108.3445v1"
  },
  {
    "Title": "Multiparty Delegated Quantum Computing",
    "Authors": "Elham Kashefi, Anna Pappa",
    "Published": "2016-06-29T17:47:53Z",
    "Summary": "Quantum computing has seen tremendous progress in the past years. However, due to limitations in scalability of quantum technologies, it seems that we are far from constructing universal quantum computers for everyday users. A more feasible solution is the delegation of computation to powerful quantum servers on the network. This solution was proposed in previous studies of Blind Quantum Computation, with guarantees for both the secrecy of the input and of the computation being performed. In this work, we further develop this idea of computing over encrypted data, to propose a multiparty delegated quantum computing protocol in the measurement-based quantum computing framework.",
    "Link": "http://arxiv.org/abs/1606.09200v2",
    "PDF Link": "http://arxiv.org/pdf/1606.09200v2"
  },
  {
    "Title": "What is a quantum computer, and how do we build one?",
    "Authors": "Carlos A. Perez-Delgado, Pieter Kok",
    "Published": "2009-06-24T13:14:20Z",
    "Summary": "The DiVincenzo criteria for implementing a quantum computer have been seminal in focussing both experimental and theoretical research in quantum information processing. These criteria were formulated specifically for the circuit model of quantum computing. However, several new models for quantum computing (paradigms) have been proposed that do not seem to fit the criteria well. The question is therefore what are the general criteria for implementing quantum computers. To this end, a formal operational definition of a quantum computer is introduced. It is then shown that according to this definition a device is a quantum computer if it obeys the following four criteria: Any quantum computer must (1) have a quantum memory; (2) facilitate a controlled quantum evolution of the quantum memory; (3) include a method for cooling the quantum memory; and (4) provide a readout mechanism for subsets of the quantum memory. The criteria are met when the device is scalable and operates fault-tolerantly. We discuss various existing quantum computing paradigms, and how they fit within this framework. Finally, we lay out a roadmap for selecting an avenue towards building a quantum computer. This is summarized in a decision tree intended to help experimentalists determine the most natural paradigm given a particular physical implementation.",
    "Link": "http://arxiv.org/abs/0906.4344v2",
    "PDF Link": "http://arxiv.org/pdf/0906.4344v2"
  },
  {
    "Title": "Fault-Tolerant Postselected Quantum Computation: Threshold Analysis",
    "Authors": "E. Knill",
    "Published": "2004-04-19T21:17:56Z",
    "Summary": "The schemes for fault-tolerant postselected quantum computation given in [Knill, Fault-Tolerant Postselected Quantum Computation: Schemes, http://arxiv.org/abs/quant-ph/0402171] are analyzed to determine their error-tolerance. The analysis is based on computer-assisted heuristics. It indicates that if classical and quantum communication delays are negligible, then scalable qubit-based quantum computation is possible with errors above 1% per elementary quantum gate.",
    "Link": "http://arxiv.org/abs/quant-ph/0404104v1",
    "PDF Link": "http://arxiv.org/pdf/quant-ph/0404104v1"
  },
  {
    "Title": "Why now is the right time to study quantum computing",
    "Authors": "Aram W. Harrow",
    "Published": "2014-12-30T21:00:25Z",
    "Summary": "Quantum computing is a good way to justify difficult physics experiments. But until quantum computers are built, do computer scientists need to know anything about quantum information? In fact, quantum computing is not merely a recipe for new computing devices, but a new way of looking at the world that has been astonishingly intellectually productive. In this article, I'll talk about where quantum computing came from, what it is, and what we can learn from it.",
    "Link": "http://arxiv.org/abs/1501.00011v1",
    "PDF Link": "http://arxiv.org/pdf/1501.00011v1"
  },
  {
    "Title": "Simulation of Two-Qubit Grover Algorithm in MBQC with Universal Blind\n  Quantum Computation",
    "Authors": "Youngkyung Lee, Doyoung Chung",
    "Published": "2025-03-12T06:29:35Z",
    "Summary": "The advancement of quantum computing technology has led to the emergence of early-stage quantum cloud computing services. To fully realize the potential of quantum cloud computing, it is essential to develop techniques that ensure the privacy of both data and functions. Quantum computations often leverage superposition to evaluate a function on all possible inputs simultaneously, making function privacy a critical requirement. In 2009, Broadbent et al. introduced the Universal Blind Quantum Computation (UBQC) protocol, which is based on Measurement-Based Quantum Computation (MBQC) and provides a framework for ensuring both function and data privacy in quantum computing. Although theoretical results indicate an equivalence between MBQC and circuitbased quantum computation, translating MBQC into circuitbased implementations remains challenging due to higher qubit requirements and the complexity of the transformation process. Consequently, current quantum cloud computing platforms are limited in their ability to simulate MBQC efficiently. This paper presents an efficient method to simulate MBQC on circuit-based quantum computing platforms. We validate this approach by implementing the two-qubit Grover algorithm in the MBQC framework and further demonstrate blindness by applying the UBQC protocol. This work verifies the simulation of a blind quantum computation using the two-qubit Grover algorithm on a circuit-based quantum computing platform.",
    "Link": "http://arxiv.org/abs/2503.09099v1",
    "PDF Link": "http://arxiv.org/pdf/2503.09099v1"
  },
  {
    "Title": "Continuous Variable Quantum Algorithms: an Introduction",
    "Authors": "Samantha Buck, Robin Coleman, Hayk Sargsyan",
    "Published": "2021-07-05T17:26:25Z",
    "Summary": "Quantum computing is usually associated with discrete quantum states and physical quantities possessing discrete eigenvalue spectrum. However, quantum computing in general is any computation accomplished by the exploitation of quantum properties of physical quantities, discrete or otherwise. It has been shown that physical quantities with continuous eigenvalue spectrum can be used for quantum computing as well. Currently, continuous variable quantum computing is a rapidly developing field both theoretically and experimentally. In this pedagogical introduction we present the basic theoretical concepts behind it and the tools for algorithm development. The paper targets readers with discrete quantum computing background, who are new to continuous variable quantum computing.",
    "Link": "http://arxiv.org/abs/2107.02151v1",
    "PDF Link": "http://arxiv.org/pdf/2107.02151v1"
  },
  {
    "Title": "Quantum computing: principles and applications",
    "Authors": "Guanru Feng, Dawei Lu, Jun Li, Tao Xin, Bei Zeng",
    "Published": "2023-10-13T20:12:28Z",
    "Summary": "People are witnessing quantum computing revolutions nowadays. Progress in the number of qubits, coherence times and gate fidelities are happening. Although quantum error correction era has not arrived, the research and development of quantum computing have inspired insights and breakthroughs in quantum technologies, both in theories and in experiments. In this review, we introduce the basic principles of quantum computing and the multilayer architecture for a quantum computer. There are different experimental platforms for implementing quantum computing. In this review, based on a mature experimental platform, the Nuclear Magnetic Resonance (NMR) platform, we introduce the basic steps to experimentally implement quantum computing, as well as common challenges and techniques.",
    "Link": "http://arxiv.org/abs/2310.09386v1",
    "PDF Link": "http://arxiv.org/pdf/2310.09386v1"
  },
  {
    "Title": "Quantum algorithmic information theory",
    "Authors": "Karl Svozil",
    "Published": "1995-10-05T12:09:46Z",
    "Summary": "The agenda of quantum algorithmic information theory, ordered `top-down,' is the quantum halting amplitude, followed by the quantum algorithmic information content, which in turn requires the theory of quantum computation. The fundamental atoms processed by quantum computation are the quantum bits which are dealt with in quantum information theory. The theory of quantum computation will be based upon a model of universal quantum computer whose elementary unit is a two-port interferometer capable of arbitrary $U(2)$ transformations. Basic to all these considerations is quantum theory, in particular Hilbert space quantum mechanics.",
    "Link": "http://arxiv.org/abs/quant-ph/9510005v1",
    "PDF Link": "http://arxiv.org/pdf/quant-ph/9510005v1"
  },
  {
    "Title": "Quantum reservoir computing: a reservoir approach toward quantum machine\n  learning on near-term quantum devices",
    "Authors": "Keisuke Fujii, Kohei Nakajima",
    "Published": "2020-11-10T04:45:52Z",
    "Summary": "Quantum systems have an exponentially large degree of freedom in the number of particles and hence provide a rich dynamics that could not be simulated on conventional computers. Quantum reservoir computing is an approach to use such a complex and rich dynamics on the quantum systems as it is for temporal machine learning. In this chapter, we explain quantum reservoir computing and related approaches, quantum extreme learning machine and quantum circuit learning, starting from a pedagogical introduction to quantum mechanics and machine learning. All these quantum machine learning approaches are experimentally feasible and effective on the state-of-the-art quantum devices.",
    "Link": "http://arxiv.org/abs/2011.04890v1",
    "PDF Link": "http://arxiv.org/pdf/2011.04890v1"
  },
  {
    "Title": "Efficient Quantum Transforms",
    "Authors": "Peter Hoyer",
    "Published": "1997-02-12T00:52:17Z",
    "Summary": "Quantum mechanics requires the operation of quantum computers to be unitary, and thus makes it important to have general techniques for developing fast quantum algorithms for computing unitary transforms. A quantum routine for computing a generalized Kronecker product is given. Applications include re-development of the networks for computing the Walsh-Hadamard and the quantum Fourier transform. New networks for two wavelet transforms are given. Quantum computation of Fourier transforms for non-Abelian groups is defined. A slightly relaxed definition is shown to simplify the analysis and the networks that computes the transforms. Efficient networks for computing such transforms for a class of metacyclic groups are introduced. A novel network for computing a Fourier transform for a group used in quantum error-correction is also given.",
    "Link": "http://arxiv.org/abs/quant-ph/9702028v1",
    "PDF Link": "http://arxiv.org/pdf/quant-ph/9702028v1"
  },
  {
    "Title": "Exploring Airline Gate-Scheduling Optimization Using Quantum Computers",
    "Authors": "Hamed Mohammadbagherpoor, Patrick Dreher, Mohannad Ibrahim, Young-Hyun Oh, James Hall, Richard E Stone, Mirela Stojkovic",
    "Published": "2021-11-18T01:44:52Z",
    "Summary": "This paper investigates the application of quantum computing technology to airline gate-scheduling quadratic assignment problems (QAP). We explore the quantum computing hardware architecture and software environment required for porting classical versions of these type of problems to quantum computers. We discuss the variational quantum eigensolver and the inclusion of space-efficient graph coloring to the Quadratic Unconstrained Binary Optimization (QUBO). These enhanced quantum computing algorithms are tested with an 8 gate and 24 flight test case using both the IBM quantum computing simulator and a 27 qubit superconducting transmon IBM quantum computing hardware platform.",
    "Link": "http://arxiv.org/abs/2111.09472v1",
    "PDF Link": "http://arxiv.org/pdf/2111.09472v1"
  },
  {
    "Title": "Quantum Chaos & Quantum Computers",
    "Authors": "D. L. Shepelyansky",
    "Published": "2000-06-15T17:47:14Z",
    "Summary": "The standard generic quantum computer model is studied analytically and numerically and the border for emergence of quantum chaos, induced by imperfections and residual inter-qubit couplings, is determined. This phenomenon appears in an isolated quantum computer without any external decoherence. The onset of quantum chaos leads to quantum computer hardware melting, strong quantum entropy growth and destruction of computer operability. The time scales for development of quantum chaos and ergodicity are determined. In spite the fact that this phenomenon is rather dangerous for quantum computing it is shown that the quantum chaos border for inter-qubit coupling is exponentially larger than the energy level spacing between quantum computer eigenstates and drops only linearly with the number of qubits n. As a result the ideal multi-qubit structure of the computer remains rather robust against imperfections. This opens a broad parameter region for a possible realization of quantum computer. The obtained results are related to the recent studies of quantum chaos in such many-body systems as nuclei, complex atoms and molecules, finite Fermi systems and quantum spin glass shards which are also reviewed in the paper.",
    "Link": "http://arxiv.org/abs/quant-ph/0006073v1",
    "PDF Link": "http://arxiv.org/pdf/quant-ph/0006073v1"
  },
  {
    "Title": "Review on Quantum Walk Computing: Theory, Implementation, and\n  Application",
    "Authors": "Xiaogang Qiang, Shixin Ma, Haijing Song",
    "Published": "2024-04-05T15:45:35Z",
    "Summary": "Classical random walk formalism shows a significant role across a wide range of applications. As its quantum counterpart, the quantum walk is proposed as an important theoretical model for quantum computing. By exploiting the quantum effects such as superposition, interference and entanglement, quantum walks and their variety have been extensively studied for achieving beyond classical computing power, and they have been broadly used in designing quantum algorithms in fields ranging from algebraic and optimization problems, graph and network analysis, to quantum Hamiltonian and biochemical process simulations, and even further quantum walk models have proven their capabilities for universal quantum computation. Compared to the conventional quantum circuit models, quantum walks show a feasible path for implementing application-specific quantum computing in particularly the noisy intermediate-scale quantum era. Recently remarkable progress has been achieved in implementing a wide variety of quantum walks and quantum walk applications, demonstrating the great potential of quantum walks. In this review, we provide a thorough summary of quantum walks and quantum walk computing, including aspects of quantum walk theories and characteristics, advances in their physical implementations and the flourishingly developed quantum walk computing applications. We also discuss the challenges facing quantum walk computing, toward realizing a practical quantum computer in the near future.",
    "Link": "http://arxiv.org/abs/2404.04178v1",
    "PDF Link": "http://arxiv.org/pdf/2404.04178v1"
  },
  {
    "Title": "DQC$^2$O: Distributed Quantum Computing for Collaborative Optimization\n  in Future Networks",
    "Authors": "Napat Ngoenriang, Minrui Xu, Jiawen Kang, Dusit Niyato, Han Yu, Xuemin, Shen",
    "Published": "2022-09-16T02:44:52Z",
    "Summary": "With the advantages of high-speed parallel processing, quantum computers can efficiently solve large-scale complex optimization problems in future networks. However, due to the uncertain qubit fidelity and quantum channel noise, distributed quantum computing which relies on quantum networks connected through entanglement faces a lot of challenges for exchanging information across quantum computers. In this paper, we propose an adaptive distributed quantum computing approach to manage quantum computers and quantum channels for solving optimization tasks in future networks. Firstly, we describe the fundamentals of quantum computing and its distributed concept in quantum networks. Secondly, to address the uncertainty of future demands of collaborative optimization tasks and instability over quantum networks, we propose a quantum resource allocation scheme based on stochastic programming for minimizing quantum resource consumption. Finally, based on the proposed approach, we discuss the potential applications for collaborative optimization in future networks, such as smart grid management, IoT cooperation, and UAV trajectory planning. Promising research directions that can lead to the design and implementation of future distributed quantum computing frameworks are also highlighted.",
    "Link": "http://arxiv.org/abs/2210.02887v1",
    "PDF Link": "http://arxiv.org/pdf/2210.02887v1"
  },
  {
    "Title": "Quantum memristors for neuromorphic quantum machine learning",
    "Authors": "Lucas Lamata",
    "Published": "2024-12-25T20:21:24Z",
    "Summary": "Quantum machine learning may permit to realize more efficient machine learning calculations with near-term quantum devices. Among the diverse quantum machine learning paradigms which are currently being considered, quantum memristors are promising as a way of combining, in the same quantum hardware, a unitary evolution with the nonlinearity provided by the measurement and feedforward. Thus, an efficient way of deploying neuromorphic quantum computing for quantum machine learning may be enabled.",
    "Link": "http://arxiv.org/abs/2412.18979v1",
    "PDF Link": "http://arxiv.org/pdf/2412.18979v1"
  },
  {
    "Title": "Natural Disasters Detection in Social Media and Satellite imagery: a\n  survey",
    "Authors": "Naina Said, Kashif Ahmad, Michael Regular, Konstantin Pogorelov, Laiq Hassan, Nasir Ahmad, Nicola Conci",
    "Published": "2019-01-14T13:06:25Z",
    "Summary": "The analysis of natural disaster-related multimedia content got great attention in recent years. Being one of the most important sources of information, social media have been crawled over the years to collect and analyze disaster-related multimedia content. Satellite imagery has also been widely explored for disasters analysis. In this paper, we survey the existing literature on disaster detection and analysis of the retrieved information from social media and satellites. Literature on disaster detection and analysis of related multimedia content on the basis of the nature of the content can be categorized into three groups, namely (i) disaster detection in text; (ii) analysis of disaster-related visual content from social media; and (iii) disaster detection in satellite imagery. We extensively review different approaches proposed in these three domains. Furthermore, we also review benchmarking datasets available for the evaluation of disaster detection frameworks. Moreover, we provide a detailed discussion on the insights obtained from the literature review, and identify future trends and challenges, which will provide an important starting point for the researchers in the field.",
    "Link": "http://arxiv.org/abs/1901.04277v1",
    "PDF Link": "http://arxiv.org/pdf/1901.04277v1"
  },
  {
    "Title": "Context-Aware Change Detection With Semi-Supervised Learning",
    "Authors": "Ritu Yadav, Andrea Nascetti, Yifang Ban",
    "Published": "2023-06-15T08:17:49Z",
    "Summary": "Change detection using earth observation data plays a vital role in quantifying the impact of disasters in affected areas. While data sources like Sentinel-2 provide rich optical information, they are often hindered by cloud cover, limiting their usage in disaster scenarios. However, leveraging pre-disaster optical data can offer valuable contextual information about the area such as landcover type, vegetation cover, soil types, enabling a better understanding of the disaster's impact. In this study, we develop a model to assess the contribution of pre-disaster Sentinel-2 data in change detection tasks, focusing on disaster-affected areas. The proposed Context-Aware Change Detection Network (CACDN) utilizes a combination of pre-disaster Sentinel-2 data, pre and post-disaster Sentinel-1 data and ancillary Digital Elevation Models (DEM) data. The model is validated on flood and landslide detection and evaluated using three metrics: Area Under the Precision-Recall Curve (AUPRC), Intersection over Union (IoU), and mean IoU. The preliminary results show significant improvement (4\\%, AUPRC, 3-7\\% IoU, 3-6\\% mean IoU) in model's change detection capabilities when incorporated with pre-disaster optical data reflecting the effectiveness of using contextual information for accurate flood and landslide detection.",
    "Link": "http://arxiv.org/abs/2306.08935v1",
    "PDF Link": "http://arxiv.org/pdf/2306.08935v1"
  },
  {
    "Title": "Social Media Analytics in Disaster Response: A Comprehensive Review",
    "Authors": "Mohammadsepehr Karimiziarani",
    "Published": "2023-07-08T20:49:18Z",
    "Summary": "Social media has emerged as a valuable resource for disaster management, revolutionizing the way emergency response and recovery efforts are conducted during natural disasters. This review paper aims to provide a comprehensive analysis of social media analytics for disaster management. The abstract begins by highlighting the increasing prevalence of natural disasters and the need for effective strategies to mitigate their impact. It then emphasizes the growing influence of social media in disaster situations, discussing its role in disaster detection, situational awareness, and emergency communication. The abstract explores the challenges and opportunities associated with leveraging social media data for disaster management purposes. It examines methodologies and techniques used in social media analytics, including data collection, preprocessing, and analysis, with a focus on data mining and machine learning approaches. The abstract also presents a thorough examination of case studies and best practices that demonstrate the successful application of social media analytics in disaster response and recovery. Ethical considerations and privacy concerns related to the use of social media data in disaster scenarios are addressed. The abstract concludes by identifying future research directions and potential advancements in social media analytics for disaster management. The review paper aims to provide practitioners and researchers with a comprehensive understanding of the current state of social media analytics in disaster management, while highlighting the need for continued research and innovation in this field.",
    "Link": "http://arxiv.org/abs/2307.04046v1",
    "PDF Link": "http://arxiv.org/pdf/2307.04046v1"
  },
  {
    "Title": "From Satellite Imagery to Disaster Insights",
    "Authors": "Jigar Doshi, Saikat Basu, Guan Pang",
    "Published": "2018-12-17T20:08:23Z",
    "Summary": "The use of satellite imagery has become increasingly popular for disaster monitoring and response. After a disaster, it is important to prioritize rescue operations, disaster response and coordinate relief efforts. These have to be carried out in a fast and efficient manner since resources are often limited in disaster-affected areas and it's extremely important to identify the areas of maximum damage. However, most of the existing disaster mapping efforts are manual which is time-consuming and often leads to erroneous results. In order to address these issues, we propose a framework for change detection using Convolutional Neural Networks (CNN) on satellite images which can then be thresholded and clustered together into grids to find areas which have been most severely affected by a disaster. We also present a novel metric called Disaster Impact Index (DII) and use it to quantify the impact of two natural disasters - the Hurricane Harvey flood and the Santa Rosa fire. Our framework achieves a top F1 score of 81.2% on the gridded flood dataset and 83.5% on the gridded fire dataset.",
    "Link": "http://arxiv.org/abs/1812.07033v1",
    "PDF Link": "http://arxiv.org/pdf/1812.07033v1"
  },
  {
    "Title": "A review on application of data mining techniques to combat natural\n  disasters",
    "Authors": "Saptarsi Goswami, Sanjay Chakraborty, Sanhita Ghosh, Amlan Chakrabarti, Basabi Chakraborty",
    "Published": "2016-10-31T15:34:49Z",
    "Summary": "Thousands of human lives are lost every year around the globe, apart from significant damage on property, animal life, etc., due to natural disasters (e.g., earthquake, flood, tsunami, hurricane and other storms, landslides, cloudburst, heat wave, forest fire). In this paper, we focus on reviewing the application of data mining and analytical techniques designed so far for (i) prediction, (ii) detection, and (iii) development of appropriate disaster management strategy based on the collected data from disasters. A detailed description of availability of data from geological observatories (seismological, hydrological), satellites, remote sensing and newer sources like social networking sites as twitter is presented. An extensive and in-depth literature study on current techniques for disaster prediction, detection and management has been done and the results are summarized according to various types of disasters. Finally a framework for building a disaster management database for India hosted on open source Big Data platform like Hadoop in a phased manner has been proposed. The study has special focus on India which ranks among top five counties in terms of absolute number of the loss of human life.",
    "Link": "http://arxiv.org/abs/1610.09974v1",
    "PDF Link": "http://arxiv.org/pdf/1610.09974v1"
  },
  {
    "Title": "Towards a disaster response system based on cognitive radio ad hoc\n  networks",
    "Authors": "Noman Islam, Ghazala Shafi Shaikh",
    "Published": "2017-10-03T14:57:36Z",
    "Summary": "This paper presents an approach towards disaster management based on cognitive radio ad hoc network. Despite the growing interests on cognitive radio ad hoc networks, not much work has been reported on using them for disaster management. This paper discusses opportunities for disaster management based on cognitive radio ad hoc networks. In this direction, the paper presents a novel technique for disaster detection based on Artificial Neural Network (ANN). The ANN is trained using backward propagation algorithm. An ANN-based spectrum sensing scheme is also presented. Finally, a service discovery scheme is presented for coordination during the time of disaster. The simulation of proposed approach has been performed in NS-2 simulator. The proposed approach shows very low false negative alarm rate using the proposed disaster detection system. The spectrum switching time of spectrum sensing scheme is also analyzed along with an analysis of latency of proposed service discovery scheme",
    "Link": "http://arxiv.org/abs/1710.02404v1",
    "PDF Link": "http://arxiv.org/pdf/1710.02404v1"
  },
  {
    "Title": "A cognitive radio ad hoc networks based disaster management scheme with\n  efficient spectrum management, collaboration and interoperability",
    "Authors": "Noman Islam, Ghazala Shafi Sheikh, Zeeshan Islam",
    "Published": "2018-09-17T14:13:29Z",
    "Summary": "In this work, a disaster management scheme based on cognitive radio ad hoc network (CRAHN) has been presented. Disaster management has been a big problem for mankind for years. However, still not much research work has been presented on this problem. Technology has been employed in past few years to address this problem. Cognitive radio ad hoc network presents a viable solution for disaster management. It can be deployed rapidly without infrastructure and it solves the spectrum scarcity and congestion issues that arise during disaster. This paper presents a novel solution for disaster management. It provides a multi-layer perceptron (MLP) based disaster detection scheme based on WSN. To solve the spectrum scarcity problem, a MLP based spectrum management scheme has been proposed. In order to ensure collaboration among rescue workers during disaster, a novel service discovery scheme has been proposed. To ensure interoperability during communication, XML format has been recommended. A real-time GUI has been proposed to provide shared situation awareness to rescue workers and enabling better decision making. The proposed approach has been implemented in NS-2 simulator. The results show accurate disaster detection, efficient spectrum usage, and interoperability and collaboration among nodes with reduced latency.",
    "Link": "http://arxiv.org/abs/1810.05090v1",
    "PDF Link": "http://arxiv.org/pdf/1810.05090v1"
  },
  {
    "Title": "Disaster Monitoring with Wikipedia and Online Social Networking Sites:\n  Structured Data and Linked Data Fragments to the Rescue?",
    "Authors": "Thomas Steiner, Ruben Verborgh",
    "Published": "2015-01-26T10:58:20Z",
    "Summary": "In this paper, we present the first results of our ongoing early-stage research on a realtime disaster detection and monitoring tool. Based on Wikipedia, it is language-agnostic and leverages user-generated multimedia content shared on online social networking sites to help disaster responders prioritize their efforts. We make the tool and its source code publicly available as we make progress on it. Furthermore, we strive to publish detected disasters and accompanying multimedia content following the Linked Data principles to facilitate its wide consumption, redistribution, and evaluation of its usefulness.",
    "Link": "http://arxiv.org/abs/1501.06329v1",
    "PDF Link": "http://arxiv.org/pdf/1501.06329v1"
  },
  {
    "Title": "Active Learning for Event Detection in Support of Disaster Analysis\n  Applications",
    "Authors": "Naina Said, Kashif Ahmad, Nicola Conci, Ala Al-Fuqaha",
    "Published": "2019-09-27T10:28:10Z",
    "Summary": "Disaster analysis in social media content is one of the interesting research domains having abundance of data. However, there is a lack of labeled data that can be used to train machine learning models for disaster analysis applications. Active learning is one of the possible solutions to such problem. To this aim, in this paper we propose and assess the efficacy of an active learning based framework for disaster analysis using images shared on social media outlets. Specifically, we analyze the performance of different active learning techniques employing several sampling and disagreement strategies. Moreover, we collect a large-scale dataset covering images from eight common types of natural disasters. The experimental results show that the use of active learning techniques for disaster analysis using images results in a performance comparable to that obtained using human annotated images, and could be used in frameworks for disaster analysis in images without tedious job of manual annotation.",
    "Link": "http://arxiv.org/abs/1909.12601v1",
    "PDF Link": "http://arxiv.org/pdf/1909.12601v1"
  },
  {
    "Title": "DisasterNets: Embedding Machine Learning in Disaster Mapping",
    "Authors": "Qingsong Xu, Yilei Shi, Xiao Xiang Zhu",
    "Published": "2023-06-16T12:50:46Z",
    "Summary": "Disaster mapping is a critical task that often requires on-site experts and is time-consuming. To address this, a comprehensive framework is presented for fast and accurate recognition of disasters using machine learning, termed DisasterNets. It consists of two stages, space granulation and attribute granulation. The space granulation stage leverages supervised/semi-supervised learning, unsupervised change detection, and domain adaptation with/without source data techniques to handle different disaster mapping scenarios. Furthermore, the disaster database with the corresponding geographic information field properties is built by using the attribute granulation stage. The framework is applied to earthquake-triggered landslide mapping and large-scale flood mapping. The results demonstrate a competitive performance for high-precision, high-efficiency, and cross-scene recognition of disasters. To bridge the gap between disaster mapping and machine learning communities, we will provide an openly accessible tool based on DisasterNets. The framework and tool will be available at https://github.com/HydroPML/DisasterNets.",
    "Link": "http://arxiv.org/abs/2306.09815v1",
    "PDF Link": "http://arxiv.org/pdf/2306.09815v1"
  },
  {
    "Title": "Assessing out-of-domain generalization for robust building damage\n  detection",
    "Authors": "Vitus Benson, Alexander Ecker",
    "Published": "2020-11-20T10:30:43Z",
    "Summary": "An important step for limiting the negative impact of natural disasters is rapid damage assessment after a disaster occurred. For instance, building damage detection can be automated by applying computer vision techniques to satellite imagery. Such models operate in a multi-domain setting: every disaster is inherently different (new geolocation, unique circumstances), and models must be robust to a shift in distribution between disaster imagery available for training and the images of the new event. Accordingly, estimating real-world performance requires an out-of-domain (OOD) test set. However, building damage detection models have so far been evaluated mostly in the simpler yet unrealistic in-distribution (IID) test setting. Here we argue that future work should focus on the OOD regime instead. We assess OOD performance of two competitive damage detection models and find that existing state-of-the-art models show a substantial generalization gap: their performance drops when evaluated OOD on new disasters not used during training. Moreover, IID performance is not predictive of OOD performance, rendering current benchmarks uninformative about real-world performance. Code and model weights are available at https://github.com/ecker-lab/robust-bdd.",
    "Link": "http://arxiv.org/abs/2011.10328v1",
    "PDF Link": "http://arxiv.org/pdf/2011.10328v1"
  },
  {
    "Title": "CARE: Content Aware Redundancy Elimination for Disaster Communications\n  on Damaged Networks",
    "Authors": "Udi Weinsberg, Athula Balachandran, Nina Taft, Gianluca Iannaccone, Vyas Sekar, Srinivasan Seshan",
    "Published": "2012-06-08T16:43:42Z",
    "Summary": "During a disaster scenario, situational awareness information, such as location, physical status and images of the surrounding area, is essential for minimizing loss of life, injury, and property damage. Today's handhelds make it easy for people to gather data from within the disaster area in many formats, including text, images and video. Studies show that the extreme anxiety induced by disasters causes humans to create a substantial amount of repetitive and redundant content. Transporting this content outside the disaster zone can be problematic when the network infrastructure is disrupted by the disaster.   This paper presents the design of a novel architecture called CARE (Content-Aware Redundancy Elimination) for better utilizing network resources in disaster-affected regions. Motivated by measurement-driven insights on redundancy patterns found in real-world disaster area photos, we demonstrate that CARE can detect the semantic similarity between photos in the networking layer, thus reducing redundant transfers and improving buffer utilization. Using DTN simulations, we explore the boundaries of the usefulness of deploying CARE on a damaged network, and show that CARE can reduce packet delivery times and drops, and enables 20-40% more unique information to reach the rescue teams outside the disaster area than when CARE is not deployed.",
    "Link": "http://arxiv.org/abs/1206.1815v1",
    "PDF Link": "http://arxiv.org/pdf/1206.1815v1"
  },
  {
    "Title": "Breaking\" Disasters: Predicting and Characterizing the Global News\n  Value of Natural and Man-made Disasters",
    "Authors": "Armineh Nourbakhsh, Quanzhi Li, Xiaomo Liu, Sameena Shah",
    "Published": "2017-09-08T02:40:35Z",
    "Summary": "Due to their often unexpected nature, natural and man-made disasters are difficult to monitor and detect for journalists and disaster management response teams. Journalists are increasingly relying on signals from social media to detect such stories in their early stage of development. Twitter, which features a vast network of local news outlets, is a major source of early signal for disaster detection. Journalists who work for global desks often follow these sources via Twitter's lists, but have to comb through thousands of small-scale or low-impact stories to find events that may be globally relevant. These are events that have a large scope, high impact, or potential geo-political relevance. We propose a model for automatically identifying events from local news sources that may break on a global scale within the next 24 hours. The results are promising and can be used in a predictive setting to help journalists manage their sources more effectively, or in a descriptive manner to analyze media coverage of disasters. Through the feature evaluation process, we also address the question: \"what makes a disaster event newsworthy on a global scale?\" As part of our data collection process, we have created a list of local sources of disaster/accident news on Twitter, which we have made publicly available.",
    "Link": "http://arxiv.org/abs/1709.02510v1",
    "PDF Link": "http://arxiv.org/pdf/1709.02510v1"
  },
  {
    "Title": "Generalizable Disaster Damage Assessment via Change Detection with\n  Vision Foundation Model",
    "Authors": "Kyeongjin Ahn, Sungwon Han, Sungwon Park, Jihee Kim, Sangyoon Park, Meeyoung Cha",
    "Published": "2024-06-12T09:21:28Z",
    "Summary": "The increasing frequency and intensity of natural disasters call for rapid and accurate damage assessment. In response, disaster benchmark datasets from high-resolution satellite imagery have been constructed to develop methods for detecting damaged areas. However, these methods face significant challenges when applied to previously unseen regions due to the limited geographical and disaster-type diversity in the existing datasets. We introduce DAVI (Disaster Assessment with VIsion foundation model), a novel approach that addresses domain disparities and detects structural damage at the building level without requiring ground-truth labels for target regions. DAVI combines task-specific knowledge from a model trained on source regions with task-agnostic knowledge from an image segmentation model to generate pseudo labels indicating potential damage in target regions. It then utilizes a two-stage refinement process, which operate at both pixel and image levels, to accurately identify changes in disaster-affected areas. Our evaluation, including a case study on the 2023 T\\\"urkiye earthquake, demonstrates that our model achieves exceptional performance across diverse terrains (e.g., North America, Asia, and the Middle East) and disaster types (e.g., wildfires, hurricanes, and tsunamis). This confirms its robustness in disaster assessment without dependence on ground-truth labels and highlights its practical applicability.",
    "Link": "http://arxiv.org/abs/2406.08020v2",
    "PDF Link": "http://arxiv.org/pdf/2406.08020v2"
  },
  {
    "Title": "Prototype-oriented Unsupervised Change Detection for Disaster Management",
    "Authors": "Youngtack Oh, Minseok Seo, Doyi Kim, Junghoon Seo",
    "Published": "2023-10-15T07:06:01Z",
    "Summary": "Climate change has led to an increased frequency of natural disasters such as floods and cyclones. This emphasizes the importance of effective disaster monitoring. In response, the remote sensing community has explored change detection methods. These methods are primarily categorized into supervised techniques, which yield precise results but come with high labeling costs, and unsupervised techniques, which eliminate the need for labeling but involve intricate hyperparameter tuning. To address these challenges, we propose a novel unsupervised change detection method named Prototype-oriented Unsupervised Change Detection for Disaster Management (PUCD). PUCD captures changes by comparing features from pre-event, post-event, and prototype-oriented change synthesis images via a foundational model, and refines results using the Segment Anything Model (SAM). Although PUCD is an unsupervised change detection, it does not require complex hyperparameter tuning. We evaluate PUCD framework on the LEVIR-Extension dataset and the disaster dataset and it achieves state-of-the-art performance compared to other methods on the LEVIR-Extension dataset.",
    "Link": "http://arxiv.org/abs/2310.09759v2",
    "PDF Link": "http://arxiv.org/pdf/2310.09759v2"
  },
  {
    "Title": "Disaster Anomaly Detector via Deeper FCDDs for Explainable Initial\n  Responses",
    "Authors": "Takato Yasuno, Masahiro Okano, Junichiro Fujii",
    "Published": "2023-06-05T00:44:39Z",
    "Summary": "Extreme natural disasters can have devastating effects on both urban and rural areas. In any disaster event, an initial response is the key to rescue within 72 hours and prompt recovery. During the initial stage of disaster response, it is important to quickly assess the damage over a wide area and identify priority areas. Among machine learning algorithms, deep anomaly detection is effective in detecting devastation features that are different from everyday features. In addition, explainable computer vision applications should justify the initial responses. In this paper, we propose an anomaly detection application utilizing deeper fully convolutional data descriptions (FCDDs), that enables the localization of devastation features and visualization of damage-marked heatmaps. More specifically, we show numerous training and test results for a dataset AIDER with the four disaster categories: collapsed buildings, traffic incidents, fires, and flooded areas. We also implement ablation studies of anomalous class imbalance and the data scale competing against the normal class. Our experiments provide results of high accuracies over 95% for F1. Furthermore, we found that the deeper FCDD with a VGG16 backbone consistently outperformed other baselines CNN27, ResNet101, and Inceptionv3. This study presents a new solution that offers a disaster anomaly detection application for initial responses with higher accuracy and devastation explainability, providing a novel contribution to the prompt disaster recovery problem in the research area of anomaly scene understanding. Finally, we discuss future works to improve more robust, explainable applications for effective initial responses.",
    "Link": "http://arxiv.org/abs/2306.02517v2",
    "PDF Link": "http://arxiv.org/pdf/2306.02517v2"
  },
  {
    "Title": "Mining Social Media to Inform Peatland Fire and Haze Disaster Management",
    "Authors": "Mark Kibanov, Gerd Stumme, Imaduddin Amin, Jong Gun Lee",
    "Published": "2017-06-16T18:57:06Z",
    "Summary": "Peatland fires and haze events are disasters with national, regional and international implications. The phenomena lead to direct damage to local assets, as well as broader economic and environmental losses. Satellite imagery is still the main and often the only available source of information for disaster management. In this article, we test the potential of social media to assist disaster management. To this end, we compare insights from two datasets: fire hotspots detected via NASA satellite imagery and almost all GPS-stamped tweets from Sumatra Island, Indonesia, posted during 2014. Sumatra Island is chosen as it regularly experiences a significant number of haze events, which affect citizens in Indonesia as well as in nearby countries including Malaysia and Singapore. We analyse temporal correlations between the datasets and their geo-spatial interdependence. Furthermore, we show how Twitter data reveals changes in users' behavior during severe haze events. Overall, we demonstrate that social media is a valuable source of complementary and supplementary information for haze disaster management. Based on our methodology and findings, an analytics tool to improve peatland fire and haze disaster management by the Indonesian authorities is under development.",
    "Link": "http://arxiv.org/abs/1706.05406v2",
    "PDF Link": "http://arxiv.org/pdf/1706.05406v2"
  },
  {
    "Title": "Deep Learning-based Aerial Image Segmentation with Open Data for\n  Disaster Impact Assessment",
    "Authors": "Ananya Gupta, Simon Watson, Hujun Yin",
    "Published": "2020-06-10T00:19:58Z",
    "Summary": "Satellite images are an extremely valuable resource in the aftermath of natural disasters such as hurricanes and tsunamis where they can be used for risk assessment and disaster management. In order to provide timely and actionable information for disaster response, in this paper a framework utilising segmentation neural networks is proposed to identify impacted areas and accessible roads in post-disaster scenarios. The effectiveness of pretraining with ImageNet on the task of aerial image segmentation has been analysed and performances of popular segmentation models compared. Experimental results show that pretraining on ImageNet usually improves the segmentation performance for a number of models. Open data available from OpenStreetMap (OSM) is used for training, forgoing the need for time-consuming manual annotation. The method also makes use of graph theory to update road network data available from OSM and to detect the changes caused by a natural disaster. Extensive experiments on data from the 2018 tsunami that struck Palu, Indonesia show the effectiveness of the proposed framework. ENetSeparable, with 30% fewer parameters compared to ENet, achieved comparable segmentation results to that of the state-of-the-art networks.",
    "Link": "http://arxiv.org/abs/2006.05575v1",
    "PDF Link": "http://arxiv.org/pdf/2006.05575v1"
  },
  {
    "Title": "Revisiting Classical Bagging with Modern Transfer Learning for\n  On-the-fly Disaster Damage Detector",
    "Authors": "Junghoon Seo, Seungwon Lee, Beomsu Kim, Taegyun Jeon",
    "Published": "2019-10-04T12:47:58Z",
    "Summary": "Automatic post-disaster damage detection using aerial imagery is crucial for quick assessment of damage caused by disaster and development of a recovery plan. The main problem preventing us from creating an applicable model in practice is that damaged (positive) examples we are trying to detect are much harder to obtain than undamaged (negative) examples, especially in short time. In this paper, we revisit the classical bootstrap aggregating approach in the context of modern transfer learning for data-efficient disaster damage detection. Unlike previous classical ensemble learning articles, our work points out the effectiveness of simple bagging in deep transfer learning that has been underestimated in the context of imbalanced classification. Benchmark results on the AIST Building Change Detection dataset show that our approach significantly outperforms existing methodologies, including the recently proposed disentanglement learning.",
    "Link": "http://arxiv.org/abs/1910.01911v1",
    "PDF Link": "http://arxiv.org/pdf/1910.01911v1"
  },
  {
    "Title": "Post-disaster building indoor damage and survivor detection using\n  autonomous path planning and deep learning with unmanned aerial vehicles",
    "Authors": "Xiao Pan, Sina Tavasoli, T. Y. Yang, Sina Poorghasem",
    "Published": "2025-03-13T04:13:48Z",
    "Summary": "Rapid response to natural disasters such as earthquakes is a crucial element in ensuring the safety of civil infrastructures and minimizing casualties. Traditional manual inspection is labour-intensive, time-consuming, and can be dangerous for inspectors and rescue workers. This paper proposed an autonomous inspection approach for structural damage inspection and survivor detection in the post-disaster building indoor scenario, which incorporates an autonomous navigation method, deep learning-based damage and survivor detection method, and a customized low-cost micro aerial vehicle (MAV) with onboard sensors. Experimental studies in a pseudo-post-disaster office building have shown the proposed methodology can achieve high accuracy in structural damage inspection and survivor detection. Overall, the proposed inspection approach shows great potential to improve the efficiency of existing manual post-disaster building inspection.",
    "Link": "http://arxiv.org/abs/2503.10027v1",
    "PDF Link": "http://arxiv.org/pdf/2503.10027v1"
  },
  {
    "Title": "SiamixFormer: a fully-transformer Siamese network with temporal Fusion\n  for accurate building detection and change detection in bi-temporal remote\n  sensing images",
    "Authors": "Amir Mohammadian, Foad Ghaderi",
    "Published": "2022-08-01T07:35:45Z",
    "Summary": "Building detection and change detection using remote sensing images can help urban and rescue planning. Moreover, they can be used for building damage assessment after natural disasters. Currently, most of the existing models for building detection use only one image (pre-disaster image) to detect buildings. This is based on the idea that post-disaster images reduce the model's performance because of presence of destroyed buildings. In this paper, we propose a siamese model, called SiamixFormer, which uses pre- and post-disaster images as input. Our model has two encoders and has a hierarchical transformer architecture. The output of each stage in both encoders is given to a temporal transformer for feature fusion in a way that query is generated from pre-disaster images and (key, value) is generated from post-disaster images. To this end, temporal features are also considered in feature fusion. Another advantage of using temporal transformers in feature fusion is that they can better maintain large receptive fields generated by transformer encoders compared with CNNs. Finally, the output of the temporal transformer is given to a simple MLP decoder at each stage. The SiamixFormer model is evaluated on xBD, and WHU datasets, for building detection and on LEVIR-CD and CDD datasets for change detection and could outperform the state-of-the-art.",
    "Link": "http://arxiv.org/abs/2208.00657v2",
    "PDF Link": "http://arxiv.org/pdf/2208.00657v2"
  },
  {
    "Title": "UAV-Assisted Real-Time Disaster Detection Using Optimized Transformer\n  Model",
    "Authors": "Branislava Jankovic, Sabina Jangirova, Waseem Ullah, Latif U. Khan, Mohsen Guizani",
    "Published": "2025-01-21T12:29:45Z",
    "Summary": "Dangerous surroundings and difficult-to-reach landscapes introduce significant complications for adequate disaster management and recuperation. These problems can be solved by engaging unmanned aerial vehicles (UAVs) provided with embedded platforms and optical sensors. In this work, we focus on enabling onboard aerial image processing to ensure proper and real-time disaster detection. Such a setting usually causes challenges due to the limited hardware resources of UAVs. However, privacy, connectivity, and latency issues can be avoided. We suggest a UAV-assisted edge framework for disaster detection, leveraging our proposed model optimized for onboard real-time aerial image classification. The optimization of the model is achieved using post-training quantization techniques. To address the limited number of disaster cases in existing benchmark datasets and therefore ensure real-world adoption of our model, we construct a novel dataset, DisasterEye, featuring disaster scenes captured by UAVs and individuals on-site. Experimental results reveal the efficacy of our model, reaching high accuracy with lowered inference latency and memory use on both traditional machines and resource-limited devices. This shows that the scalability and adaptability of our method make it a powerful solution for real-time disaster management on resource-constrained UAV platforms.",
    "Link": "http://arxiv.org/abs/2501.12087v2",
    "PDF Link": "http://arxiv.org/pdf/2501.12087v2"
  },
  {
    "Title": "GRASP EARTH: Intuitive Software for Discovering Changes on the Planet",
    "Authors": "Waku Hatakeyama, Shirou Kawakita, Ryohei Izawa, Masanari Kimura",
    "Published": "2022-03-02T09:08:42Z",
    "Summary": "Detecting changes on the Earth, such as urban development, deforestation, or natural disaster, is one of the research fields that is attracting a great deal of attention. One promising tool to solve these problems is satellite imagery. However, satellite images require huge amount of storage, therefore users are required to set Area of Interests first, which was not suitable for detecting potential areas for disaster or development. To tackle with this problem, we develop the novel tool, namely GRASP EARTH, which is the simple change detection application based on Google Earth Engine. GRASP EARTH allows us to handle satellite imagery easily and it has used for disaster monitoring and urban development monitoring.",
    "Link": "http://arxiv.org/abs/2203.00955v1",
    "PDF Link": "http://arxiv.org/pdf/2203.00955v1"
  },
  {
    "Title": "Sarcasm Detection in a Disaster Context",
    "Authors": "Tiberiu Sosea, Junyi Jessy Li, Cornelia Caragea",
    "Published": "2023-08-16T05:58:12Z",
    "Summary": "During natural disasters, people often use social media platforms such as Twitter to ask for help, to provide information about the disaster situation, or to express contempt about the unfolding event or public policies and guidelines. This contempt is in some cases expressed as sarcasm or irony. Understanding this form of speech in a disaster-centric context is essential to improving natural language understanding of disaster-related tweets. In this paper, we introduce HurricaneSARC, a dataset of 15,000 tweets annotated for intended sarcasm, and provide a comprehensive investigation of sarcasm detection using pre-trained language models. Our best model is able to obtain as much as 0.70 F1 on our dataset. We also demonstrate that the performance on HurricaneSARC can be improved by leveraging intermediate task transfer learning. We release our data and code at https://github.com/tsosea2/HurricaneSarc.",
    "Link": "http://arxiv.org/abs/2308.08156v1",
    "PDF Link": "http://arxiv.org/pdf/2308.08156v1"
  },
  {
    "Title": "Building Disaster Damage Assessment in Satellite Imagery with\n  Multi-Temporal Fusion",
    "Authors": "Ethan Weber, Hassan Kané",
    "Published": "2020-04-12T02:06:12Z",
    "Summary": "Automatic change detection and disaster damage assessment are currently procedures requiring a huge amount of labor and manual work by satellite imagery analysts. In the occurrences of natural disasters, timely change detection can save lives. In this work, we report findings on problem framing, data processing and training procedures which are specifically helpful for the task of building damage assessment using the newly released xBD dataset. Our insights lead to substantial improvement over the xBD baseline models, and we score among top results on the xView2 challenge leaderboard. We release our code used for the competition.",
    "Link": "http://arxiv.org/abs/2004.05525v1",
    "PDF Link": "http://arxiv.org/pdf/2004.05525v1"
  },
  {
    "Title": "RescueADI: Adaptive Disaster Interpretation in Remote Sensing Images\n  with Autonomous Agents",
    "Authors": "Zhuoran Liu, Danpei Zhao, Bo Yuan",
    "Published": "2024-10-17T09:36:52Z",
    "Summary": "Current methods for disaster scene interpretation in remote sensing images (RSIs) mostly focus on isolated tasks such as segmentation, detection, or visual question-answering (VQA). However, current interpretation methods often fail at tasks that require the combination of multiple perception methods and specialized tools. To fill this gap, this paper introduces Adaptive Disaster Interpretation (ADI), a novel task designed to solve requests by planning and executing multiple sequentially correlative interpretation tasks to provide a comprehensive analysis of disaster scenes. To facilitate research and application in this area, we present a new dataset named RescueADI, which contains high-resolution RSIs with annotations for three connected aspects: planning, perception, and recognition. The dataset includes 4,044 RSIs, 16,949 semantic masks, 14,483 object bounding boxes, and 13,424 interpretation requests across nine challenging request types. Moreover, we propose a new disaster interpretation method employing autonomous agents driven by large language models (LLMs) for task planning and execution, proving its efficacy in handling complex disaster interpretations. The proposed agent-based method solves various complex interpretation requests such as counting, area calculation, and path-finding without human intervention, which traditional single-task approaches cannot handle effectively. Experimental results on RescueADI demonstrate the feasibility of the proposed task and show that our method achieves an accuracy 9% higher than existing VQA methods, highlighting its advantages over conventional disaster interpretation approaches. The dataset will be publicly available.",
    "Link": "http://arxiv.org/abs/2410.13384v1",
    "PDF Link": "http://arxiv.org/pdf/2410.13384v1"
  },
  {
    "Title": "Low-supervision urgency detection and transfer in short crisis messages",
    "Authors": "Mayank Kejriwal, Peilin Zhou",
    "Published": "2019-07-15T20:43:53Z",
    "Summary": "Humanitarian disasters have been on the rise in recent years due to the effects of climate change and socio-political situations such as the refugee crisis. Technology can be used to best mobilize resources such as food and water in the event of a natural disaster, by semi-automatically flagging tweets and short messages as indicating an urgent need. The problem is challenging not just because of the sparseness of data in the immediate aftermath of a disaster, but because of the varying characteristics of disasters in developing countries (making it difficult to train just one system) and the noise and quirks in social media. In this paper, we present a robust, low-supervision social media urgency system that adapts to arbitrary crises by leveraging both labeled and unlabeled data in an ensemble setting. The system is also able to adapt to new crises where an unlabeled background corpus may not be available yet by utilizing a simple and effective transfer learning methodology. Experimentally, our transfer learning and low-supervision approaches are found to outperform viable baselines with high significance on myriad disaster datasets.",
    "Link": "http://arxiv.org/abs/1907.06745v1",
    "PDF Link": "http://arxiv.org/pdf/1907.06745v1"
  },
  {
    "Title": "xBD: A Dataset for Assessing Building Damage from Satellite Imagery",
    "Authors": "Ritwik Gupta, Richard Hosfelt, Sandra Sajeev, Nirav Patel, Bryce Goodman, Jigar Doshi, Eric Heim, Howie Choset, Matthew Gaston",
    "Published": "2019-11-21T05:30:13Z",
    "Summary": "We present xBD, a new, large-scale dataset for the advancement of change detection and building damage assessment for humanitarian assistance and disaster recovery research. Natural disaster response requires an accurate understanding of damaged buildings in an affected region. Current response strategies require in-person damage assessments within 24-48 hours of a disaster. Massive potential exists for using aerial imagery combined with computer vision algorithms to assess damage and reduce the potential danger to human life. In collaboration with multiple disaster response agencies, xBD provides pre- and post-event satellite imagery across a variety of disaster events with building polygons, ordinal labels of damage level, and corresponding satellite metadata. Furthermore, the dataset contains bounding boxes and labels for environmental factors such as fire, water, and smoke. xBD is the largest building damage assessment dataset to date, containing 850,736 building annotations across 45,362 km\\textsuperscript{2} of imagery.",
    "Link": "http://arxiv.org/abs/1911.09296v1",
    "PDF Link": "http://arxiv.org/pdf/1911.09296v1"
  },
  {
    "Title": "CNN-Based Semantic Change Detection in Satellite Imagery",
    "Authors": "Ananya Gupta, Elisabeth Welburn, Simon Watson, Hujun Yin",
    "Published": "2020-06-10T01:06:03Z",
    "Summary": "Timely disaster risk management requires accurate road maps and prompt damage assessment. Currently, this is done by volunteers manually marking satellite imagery of affected areas but this process is slow and often error-prone. Segmentation algorithms can be applied to satellite images to detect road networks. However, existing methods are unsuitable for disaster-struck areas as they make assumptions about the road network topology which may no longer be valid in these scenarios. Herein, we propose a CNN-based framework for identifying accessible roads in post-disaster imagery by detecting changes from pre-disaster imagery. Graph theory is combined with the CNN output for detecting semantic changes in road networks with OpenStreetMap data. Our results are validated with data of a tsunami-affected region in Palu, Indonesia acquired from DigitalGlobe.",
    "Link": "http://arxiv.org/abs/2006.05589v1",
    "PDF Link": "http://arxiv.org/pdf/2006.05589v1"
  },
  {
    "Title": "Detecting individual internal displacements following a sudden-onset\n  disaster using time series analysis of call detail records",
    "Authors": "Tracey Li, Jesper Dejby, Maximilian Albert, Linus Bengtsson, Veronique Lefebvre",
    "Published": "2019-08-06T21:55:03Z",
    "Summary": "We present a method for analysing mobile phone call detail records to identify individuals whom we believe to be have been internally displaced as a result of a sudden-onset disaster. We model each anonymous individual's movements trajectory as a piecewise-constant time series signal, assume that a disaster-induced displacement is exhibited as a level shift from an individual's 'normal' location, and then apply a step detection algorithm to identify level shifts in the signal. In contrast to typical methods that are used to analyse mobility patterns from call detail records, where the aggregate movements of large groups of individuals are analysed, our method offers the advantage that no assumptions regarding the destination or duration of an individual's displacement are necessary. We have applied the method to the datasets from three disasters - the 2010 earthquake in Haiti, the 2015 Gorkha earthquake in Nepal, and Hurricane Matthew in Haiti in 2016. Our results demonstrate that this method can facilitate improvements in the analysis and modelling of the mobility of internally displaced persons in post-disaster scenarios, using call detail records. Such analyses can be used to complement traditional survey methods to assess the scale and characteristics of disaster-induced displacements in a timely manner.",
    "Link": "http://arxiv.org/abs/1908.02377v1",
    "PDF Link": "http://arxiv.org/pdf/1908.02377v1"
  },
  {
    "Title": "Detecting natural disasters, damage, and incidents in the wild",
    "Authors": "Ethan Weber, Nuria Marzo, Dim P. Papadopoulos, Aritro Biswas, Agata Lapedriza, Ferda Ofli, Muhammad Imran, Antonio Torralba",
    "Published": "2020-08-20T20:09:42Z",
    "Summary": "Responding to natural disasters, such as earthquakes, floods, and wildfires, is a laborious task performed by on-the-ground emergency responders and analysts. Social media has emerged as a low-latency data source to quickly understand disaster situations. While most studies on social media are limited to text, images offer more information for understanding disaster and incident scenes. However, no large-scale image datasets for incident detection exists. In this work, we present the Incidents Dataset, which contains 446,684 images annotated by humans that cover 43 incidents across a variety of scenes. We employ a baseline classification model that mitigates false-positive errors and we perform image filtering experiments on millions of social media images from Flickr and Twitter. Through these experiments, we show how the Incidents Dataset can be used to detect images with incidents in the wild. Code, data, and models are available online at http://incidentsdataset.csail.mit.edu.",
    "Link": "http://arxiv.org/abs/2008.09188v1",
    "PDF Link": "http://arxiv.org/pdf/2008.09188v1"
  },
  {
    "Title": "Uneven Coverage of Natural Disasters in Wikipedia: the Case of Flood",
    "Authors": "Valerio Lorini, Javier Rando, Diego Saez-Trumper, Carlos Castillo",
    "Published": "2020-01-23T21:13:34Z",
    "Summary": "The usage of non-authoritative data for disaster management presents the opportunity of accessing timely information that might not be available through other means, as well as the challenge of dealing with several layers of biases. Wikipedia, a collaboratively-produced encyclopedia, includes in-depth information about many natural and human-made disasters, and its editors are particularly good at adding information in real-time as a crisis unfolds. In this study, we focus on the English version of Wikipedia, that is by far the most comprehensive version of this encyclopedia. Wikipedia tends to have good coverage of disasters, particularly those having a large number of fatalities. However, we also show that a tendency to cover events in wealthy countries and not cover events in poorer ones permeates Wikipedia as a source for disaster-related information. By performing careful automatic content analysis at a large scale, we show how the coverage of floods in Wikipedia is skewed towards rich, English-speaking countries, in particular the US and Canada. We also note how coverage of floods in countries with the lowest income, as well as countries in South America, is substantially lower than the coverage of floods in middle-income countries. These results have implications for systems using Wikipedia or similar collaborative media platforms as an information source for detecting emergencies or for gathering valuable information for disaster response.",
    "Link": "http://arxiv.org/abs/2001.08810v1",
    "PDF Link": "http://arxiv.org/pdf/2001.08810v1"
  },
  {
    "Title": "Deep-Disaster: Unsupervised Disaster Detection and Localization Using\n  Visual Data",
    "Authors": "Soroor Shekarizadeh, Razieh Rastgoo, Saif Al-Kuwari, Mohammad Sabokrou",
    "Published": "2022-01-31T19:21:44Z",
    "Summary": "Social media plays a significant role in sharing essential information, which helps humanitarian organizations in rescue operations during and after disaster incidents. However, developing an efficient method that can provide rapid analysis of social media images in the early hours of disasters is still largely an open problem, mainly due to the lack of suitable datasets and the sheer complexity of this task. In addition, supervised methods can not generalize well to novel disaster incidents. In this paper, inspired by the success of Knowledge Distillation (KD) methods, we propose an unsupervised deep neural network to detect and localize damages in social media images. Our proposed KD architecture is a feature-based distillation approach that comprises a pre-trained teacher and a smaller student network, with both networks having similar GAN architecture containing a generator and a discriminator. The student network is trained to emulate the behavior of the teacher on training input samples, which, in turn, contain images that do not include any damaged regions. Therefore, the student network only learns the distribution of no damage data and would have different behavior from the teacher network-facing damages. To detect damage, we utilize the difference between features generated by two networks using a defined score function that demonstrates the probability of damages occurring. Our experimental results on the benchmark dataset confirm that our approach outperforms state-of-the-art methods in detecting and localizing the damaged areas, especially for novel disaster types.",
    "Link": "http://arxiv.org/abs/2202.00050v1",
    "PDF Link": "http://arxiv.org/pdf/2202.00050v1"
  },
  {
    "Title": "End-to-end Remote Sensing Change Detection of Unregistered Bi-temporal\n  Images for Natural Disasters",
    "Authors": "Guiqin Zhao, Lianlei Shan, Weiqiang Wang",
    "Published": "2023-07-27T18:04:45Z",
    "Summary": "Change detection based on remote sensing images has been a prominent area of interest in the field of remote sensing. Deep networks have demonstrated significant success in detecting changes in bi-temporal remote sensing images and have found applications in various fields. Given the degradation of natural environments and the frequent occurrence of natural disasters, accurately and swiftly identifying damaged buildings in disaster-stricken areas through remote sensing images holds immense significance. This paper aims to investigate change detection specifically for natural disasters. Considering that existing public datasets used in change detection research are registered, which does not align with the practical scenario where bi-temporal images are not matched, this paper introduces an unregistered end-to-end change detection synthetic dataset called xBD-E2ECD. Furthermore, we propose an end-to-end change detection network named E2ECDNet, which takes an unregistered bi-temporal image pair as input and simultaneously generates the flow field prediction result and the change detection prediction result. It is worth noting that our E2ECDNet also supports change detection for registered image pairs, as registration can be seen as a special case of non-registration. Additionally, this paper redefines the criteria for correctly predicting a positive case and introduces neighborhood-based change detection evaluation metrics. The experimental results have demonstrated significant improvements.",
    "Link": "http://arxiv.org/abs/2307.15128v2",
    "PDF Link": "http://arxiv.org/pdf/2307.15128v2"
  },
  {
    "Title": "Towards Cross-Disaster Building Damage Assessment with Graph\n  Convolutional Networks",
    "Authors": "Ali Ismail, Mariette Awad",
    "Published": "2022-01-25T15:25:21Z",
    "Summary": "In the aftermath of disasters, building damage maps are obtained using change detection to plan rescue operations. Current convolutional neural network approaches do not consider the similarities between neighboring buildings for predicting the damage. We present a novel graph-based building damage detection solution to capture these relationships. Our proposed model architecture learns from both local and neighborhood features to predict building damage. Specifically, we adopt the sample and aggregate graph convolution strategy to learn aggregation functions that generalize to unseen graphs which is essential for alleviating the time needed to obtain predictions for new disasters. Our experiments on the xBD dataset and comparisons with a classical convolutional neural network reveal that while our approach is handicapped by class imbalance, it presents a promising and distinct advantage when it comes to cross-disaster generalization.",
    "Link": "http://arxiv.org/abs/2201.10395v1",
    "PDF Link": "http://arxiv.org/pdf/2201.10395v1"
  },
  {
    "Title": "PDSR: Efficient UAV Deployment for Swift and Accurate Post-Disaster\n  Search and Rescue",
    "Authors": "Alaa Awad Abdellatif, Ali Elmancy, Amr Mohamed, Ahmed Massoud, Wadha Lebda, Khalid K. Naji",
    "Published": "2024-10-30T12:46:15Z",
    "Summary": "This paper introduces a comprehensive framework for Post-Disaster Search and Rescue (PDSR), aiming to optimize search and rescue operations leveraging Unmanned Aerial Vehicles (UAVs). The primary goal is to improve the precision and availability of sensing capabilities, particularly in various catastrophic scenarios. Central to this concept is the rapid deployment of UAV swarms equipped with diverse sensing, communication, and intelligence capabilities, functioning as an integrated system that incorporates multiple technologies and approaches for efficient detection of individuals buried beneath rubble or debris following a disaster. Within this framework, we propose architectural solution and address associated challenges to ensure optimal performance in real-world disaster scenarios. The proposed framework aims to achieve complete coverage of damaged areas significantly faster than traditional methods using a multi-tier swarm architecture. Furthermore, integrating multi-modal sensing data with machine learning for data fusion could enhance detection accuracy, ensuring precise identification of survivors.",
    "Link": "http://arxiv.org/abs/2410.22982v1",
    "PDF Link": "http://arxiv.org/pdf/2410.22982v1"
  },
  {
    "Title": "Can SDN Mitigate Disasters?",
    "Authors": "Vincent Gramoli, Guillaume Jourjon, Olivier Mehani",
    "Published": "2014-10-16T05:24:46Z",
    "Summary": "Datacenter networks and services are at risk in the face of disasters. Existing fault-tolerant storage services cannot even achieve a nil recovery point objective (RPO) as client-generated data may get lost before the termination of their migration across geo-replicated datacenters. SDN has proved instrumental in exploiting application-level information to optimise the routing of information. In this paper, we propose Software Defined Edge (SDE) or the implementation of SDN at the network edge to achieve nil RPO. We illustrate our proposal with a fault-tolerant key-value store that experimentally recovers from disaster within 30s. Although SDE is inherently fault-tolerant and scalable, its deployment raises new challenges on the partnership between ISPs and CDN providers. We conclude that failure detection information at the SDN-level can effectively benefit applications to recover from disaster.",
    "Link": "http://arxiv.org/abs/1410.4296v2",
    "PDF Link": "http://arxiv.org/pdf/1410.4296v2"
  },
  {
    "Title": "NFV and SDN - based Distributed IoT Gateway for Large-Scale Disaster\n  Management",
    "Authors": "Carla Mouradian, Narjes Tahghigh Jahromi, Roch H. Glitho",
    "Published": "2018-08-21T12:35:30Z",
    "Summary": "Large-scale disaster management applications are among the several realistic applications of the IoT. Fire detection and earthquake early warning applications are just two examples. Several IoT devices are used in such applications e.g., sensors and robots. These sensors and robots are usually heterogeneous. Moreover, in disaster scenarios, the existing communication infrastructure may become completely or partially destroyed, leaving mobile ad-hoc networks the only alternative to provide connectivity. Utilizing these applications raises new challenges such as the need for dynamic, flexible, and distributed gateways which can accommodate new applications and new IoT devices. Network Functions Virtualization (NFV) and Software Defined Networking (SDN) are emerging paradigms that can help to overcome these challenges. This paper leverages NFV and SDN to propose an architecture for on-the-fly distributed gateway provisioning in large-scale disaster management. In the proposed architecture, the gateway functions are provisioned as Virtual Network Functions (VNFs) that are chained on-the-fly in the IoT domain using SDN. A prototype is built and the performance results are presented.",
    "Link": "http://arxiv.org/abs/1808.06874v1",
    "PDF Link": "http://arxiv.org/pdf/1808.06874v1"
  },
  {
    "Title": "Identifying Malicious Players in GWAP-based Disaster Monitoring\n  Crowdsourcing System",
    "Authors": "Changkun Ou, Yifei Zhan, Yaxi Chen",
    "Published": "2019-09-14T14:49:11Z",
    "Summary": "Disaster monitoring is challenging due to the lake of infrastructures in monitoring areas. Based on the theory of Game-With-A-Purpose (GWAP), this paper contributes to a novel large-scale crowdsourcing disaster monitoring system. The system analyzes tagged satellite pictures from anonymous players, and then reports aggregated and evaluated monitoring results to its stakeholders. An algorithm based on directed graph centralities is presented to address the core issues of malicious user detection and disaster level calculation. Our method can be easily applied in other human computation systems. In the end, some issues with possible solutions are discussed for our future work.",
    "Link": "http://arxiv.org/abs/1910.01459v1",
    "PDF Link": "http://arxiv.org/pdf/1910.01459v1"
  },
  {
    "Title": "Animal inspired Application of a Variant of Mel Spectrogram for Seismic\n  Data Processing",
    "Authors": "Samayan Bhattacharya, Sk Shahnawaz",
    "Published": "2021-09-22T13:41:57Z",
    "Summary": "Predicting disaster events from seismic data is of paramount importance and can save thousands of lives, especially in earthquake-prone areas and habitations around volcanic craters. The drastic rise in the number of seismic monitoring stations in recent years has allowed the collection of a huge quantity of data, outpacing the capacity of seismologists. Due to the complex nature of the seismological data, it is often difficult for seismologists to detect subtle patterns with major implications. Machine learning algorithms have been demonstrated to be effective in classification and prediction tasks for seismic data. It has been widely known that some animals can sense disasters like earthquakes from seismic signals well before the disaster strikes. Mel spectrogram has been widely used for speech recognition as it scales the actual frequencies according to human hearing. In this paper, we propose a variant of the Mel spectrogram to scale the raw frequencies of seismic data to the hearing of such animals that can sense disasters from seismic signals. We are using a Computer vision algorithm along with clustering that allows for the classification of unlabelled seismic data.",
    "Link": "http://arxiv.org/abs/2109.10733v1",
    "PDF Link": "http://arxiv.org/pdf/2109.10733v1"
  },
  {
    "Title": "Disaster mapping from satellites: damage detection with crowdsourced\n  point labels",
    "Authors": "Danil Kuzin, Olga Isupova, Brooke D. Simmons, Steven Reece",
    "Published": "2021-11-05T18:32:22Z",
    "Summary": "High-resolution satellite imagery available immediately after disaster events is crucial for response planning as it facilitates broad situational awareness of critical infrastructure status such as building damage, flooding, and obstructions to access routes. Damage mapping at this scale would require hundreds of expert person-hours. However, a combination of crowdsourcing and recent advances in deep learning reduces the effort needed to just a few hours in real time. Asking volunteers to place point marks, as opposed to shapes of actual damaged areas, significantly decreases the required analysis time for response during the disaster. However, different volunteers may be inconsistent in their marking. This work presents methods for aggregating potentially inconsistent damage marks to train a neural network damage detector.",
    "Link": "http://arxiv.org/abs/2111.03693v1",
    "PDF Link": "http://arxiv.org/pdf/2111.03693v1"
  },
  {
    "Title": "Large-Scale Landslides Detection from Satellite Images with Incomplete\n  Labels",
    "Authors": "Masanari Kimura",
    "Published": "2019-10-16T02:04:10Z",
    "Summary": "Earthquakes and tropical cyclones cause the suffering of millions of people around the world every year. The resulting landslides exacerbate the effects of these disasters. Landslide detection is, therefore, a critical task for the protection of human life and livelihood in mountainous areas. To tackle this problem, we propose a combination of satellite technology and Deep Neural Networks (DNNs). We evaluate the performance of multiple DNN-based methods for landslide detection on actual satellite images of landslide damage. Our analysis demonstrates the potential for a meaningful social impact in terms of disasters and rescue.",
    "Link": "http://arxiv.org/abs/1910.07129v1",
    "PDF Link": "http://arxiv.org/pdf/1910.07129v1"
  },
  {
    "Title": "Measuring Economic Resilience to Natural Disasters with Big Economic\n  Transaction Data",
    "Authors": "Elena Alfaro Martinez, Maria Hernandez Rubio, Roberto Maestre Martinez, Juan Murillo Arias, Dario Patane, Amanda Zerbe, Robert Kirkpatrick, Miguel Luengo-Oroz, Amanda Zerbe",
    "Published": "2016-09-28T01:20:23Z",
    "Summary": "This research explores the potential to analyze bank card payments and ATM cash withdrawals in order to map and quantify how people are impacted by and recover from natural disasters. Our approach defines a disaster-affected community's economic recovery time as the time needed to return to baseline activity levels in terms of number of bank card payments and ATM cash withdrawals. For Hurricane Odile, which hit the state of Baja California Sur (BCS) in Mexico between 15 and 17 September 2014, we measured and mapped communities' economic recovery time, which ranged from 2 to 40 days in different locations. We found that -- among individuals with a bank account -- the lower the income level, the shorter the time needed for economic activity to return to normal levels. Gender differences in recovery times were also detected and quantified. In addition, our approach evaluated how communities prepared for the disaster by quantifying expenditure growth in food or gasoline before the hurricane struck. We believe this approach opens a new frontier in measuring the economic impact of disasters with high temporal and spatial resolution, and in understanding how populations bounce back and adapt.",
    "Link": "http://arxiv.org/abs/1609.09340v1",
    "PDF Link": "http://arxiv.org/pdf/1609.09340v1"
  },
  {
    "Title": "Automated Detection of Pre-Disaster Building Images from Google Street\n  View",
    "Authors": "Chul Min Yeum, Ali Lenjani, Shirley J. Dyke, Ilias Bilionis",
    "Published": "2019-02-13T01:14:23Z",
    "Summary": "After a disaster, teams of structural engineers collect vast amounts of images from damaged buildings to obtain lessons and gain knowledge from the event. Images of damaged buildings and components provide valuable evidence to understand the consequences on our structures. However, in many cases, images of damaged buildings are often captured without sufficient spatial context. Also, they may be hard to recognize in cases with severe damage. Incorporating past images showing a pre-disaster condition of such buildings is helpful to accurately evaluate possible circumstances related to a building's failure. One of the best resources to observe the pre-disaster condition of the buildings is Google Street View. A sequence of 360 panorama images which are captured along streets enables all-around views at each location on the street. Once a user knows the GPS information near the building, all external views of the building can be made available. In this study, we develop an automated technique to extract past building images from 360 panorama images serviced by Google Street View. Users only need to provide a geo-tagged image, collected near the target building, and the rest of the process is fully automated. High-quality and undistorted building images are extracted from past panoramas. Since the panoramas are collected from various locations near the building along the street, the user can identify its pre-disaster conditions from the full set of external views.",
    "Link": "http://arxiv.org/abs/1902.10816v1",
    "PDF Link": "http://arxiv.org/pdf/1902.10816v1"
  },
  {
    "Title": "Transformer-based Flood Scene Segmentation for Developing Countries",
    "Authors": "Ahan M R, Roshan Roy, Shreyas Sunil Kulkarni, Vaibhav Soni, Ashish Chittora",
    "Published": "2022-10-09T10:29:41Z",
    "Summary": "Floods are large-scale natural disasters that often induce a massive number of deaths, extensive material damage, and economic turmoil. The effects are more extensive and longer-lasting in high-population and low-resource developing countries. Early Warning Systems (EWS) constantly assess water levels and other factors to forecast floods, to help minimize damage. Post-disaster, disaster response teams undertake a Post Disaster Needs Assessment (PDSA) to assess structural damage and determine optimal strategies to respond to highly affected neighbourhoods. However, even today in developing countries, EWS and PDSA analysis of large volumes of image and video data is largely a manual process undertaken by first responders and volunteers. We propose FloodTransformer, which to the best of our knowledge, is the first visual transformer-based model to detect and segment flooded areas from aerial images at disaster sites. We also propose a custom metric, Flood Capacity (FC) to measure the spatial extent of water coverage and quantify the segmented flooded area for EWS and PDSA analyses. We use the SWOC Flood segmentation dataset and achieve 0.93 mIoU, outperforming all other methods. We further show the robustness of this approach by validating across unseen flood images from other flood data sources.",
    "Link": "http://arxiv.org/abs/2210.04218v1",
    "PDF Link": "http://arxiv.org/pdf/2210.04218v1"
  },
  {
    "Title": "Post-Hurricane Debris Segmentation Using Fine-Tuned Foundational Vision\n  Models",
    "Authors": "Kooshan Amini, Yuhao Liu, Jamie Ellen Padgett, Guha Balakrishnan, Ashok Veeraraghavan",
    "Published": "2025-04-17T00:08:50Z",
    "Summary": "Timely and accurate detection of hurricane debris is critical for effective disaster response and community resilience. While post-disaster aerial imagery is readily available, robust debris segmentation solutions applicable across multiple disaster regions remain limited. Developing a generalized solution is challenging due to varying environmental and imaging conditions that alter debris' visual signatures across different regions, further compounded by the scarcity of training data. This study addresses these challenges by fine-tuning pre-trained foundational vision models, achieving robust performance with a relatively small, high-quality dataset. Specifically, this work introduces an open-source dataset comprising approximately 1,200 manually annotated aerial RGB images from Hurricanes Ian, Ida, and Ike. To mitigate human biases and enhance data quality, labels from multiple annotators are strategically aggregated and visual prompt engineering is employed. The resulting fine-tuned model, named fCLIPSeg, achieves a Dice score of 0.70 on data from Hurricane Ida -- a disaster event entirely excluded during training -- with virtually no false positives in debris-free areas. This work presents the first event-agnostic debris segmentation model requiring only standard RGB imagery during deployment, making it well-suited for rapid, large-scale post-disaster impact assessments and recovery planning.",
    "Link": "http://arxiv.org/abs/2504.12542v1",
    "PDF Link": "http://arxiv.org/pdf/2504.12542v1"
  },
  {
    "Title": "ContCommRTD: A Distributed Content-based Misinformation-aware Community\n  Detection System for Real-Time Disaster Reporting",
    "Authors": "Elena-Simona Apostol, Ciprian-Octavian Truică, Adrian Paschke",
    "Published": "2023-01-30T15:28:47Z",
    "Summary": "Real-time social media data can provide useful information on evolving hazards. Alongside traditional methods of disaster detection, the integration of social media data can considerably enhance disaster management. In this paper, we investigate the problem of detecting geolocation-content communities on Twitter and propose a novel distributed system that provides in near real-time information on hazard-related events and their evolution. We show that content-based community analysis leads to better and faster dissemination of reports on hazards. Our distributed disaster reporting system analyzes the social relationship among worldwide geolocated tweets, and applies topic modeling to group tweets by topics. Considering for each tweet the following information: user, timestamp, geolocation, retweets, and replies, we create a publisher-subscriber distribution model for topics. We use content similarity and the proximity of nodes to create a new model for geolocation-content based communities. Users can subscribe to different topics in specific geographical areas or worldwide and receive real-time reports regarding these topics. As misinformation can lead to increase damage if propagated in hazards related tweets, we propose a new deep learning model to detect fake news. The misinformed tweets are then removed from display. We also show empirically the scalability capabilities of the proposed system.",
    "Link": "http://arxiv.org/abs/2301.12984v1",
    "PDF Link": "http://arxiv.org/pdf/2301.12984v1"
  },
  {
    "Title": "One-class Damage Detector Using Deeper Fully-Convolutional Data\n  Descriptions for Civil Application",
    "Authors": "Takato Yasuno, Masahiro Okano, Junichiro Fujii",
    "Published": "2023-03-03T06:27:15Z",
    "Summary": "Infrastructure managers must maintain high standards to ensure user satisfaction during the lifecycle of infrastructures. Surveillance cameras and visual inspections have enabled progress in automating the detection of anomalous features and assessing the occurrence of deterioration. However, collecting damage data is typically time consuming and requires repeated inspections. The one-class damage detection approach has an advantage in that normal images can be used to optimize model parameters. Additionally, visual evaluation of heatmaps enables us to understand localized anomalous features. The authors highlight damage vision applications utilized in the robust property and localized damage explainability. First, we propose a civil-purpose application for automating one-class damage detection reproducing a fully convolutional data description (FCDD) as a baseline model. We have obtained accurate and explainable results demonstrating experimental studies on concrete damage and steel corrosion in civil engineering. Additionally, to develop a more robust application, we applied our method to another outdoor domain that contains complex and noisy backgrounds using natural disaster datasets collected using various devices. Furthermore, we propose a valuable solution of deeper FCDDs focusing on other powerful backbones to improve the performance of damage detection and implement ablation studies on disaster datasets. The key results indicate that the deeper FCDDs outperformed the baseline FCDD on datasets representing natural disaster damage caused by hurricanes, typhoons, earthquakes, and four-event disasters.",
    "Link": "http://arxiv.org/abs/2303.01732v3",
    "PDF Link": "http://arxiv.org/pdf/2303.01732v3"
  },
  {
    "Title": "Building Damage Detection in Satellite Imagery Using Convolutional\n  Neural Networks",
    "Authors": "Joseph Z. Xu, Wenhan Lu, Zebo Li, Pranav Khaitan, Valeriya Zaytseva",
    "Published": "2019-10-14T22:03:49Z",
    "Summary": "In all types of disasters, from earthquakes to armed conflicts, aid workers need accurate and timely data such as damage to buildings and population displacement to mount an effective response. Remote sensing provides this data at an unprecedented scale, but extracting operationalizable information from satellite images is slow and labor-intensive. In this work, we use machine learning to automate the detection of building damage in satellite imagery. We compare the performance of four different convolutional neural network models in detecting damaged buildings in the 2010 Haiti earthquake. We also quantify how well the models will generalize to future disasters by training and testing models on different disaster events.",
    "Link": "http://arxiv.org/abs/1910.06444v1",
    "PDF Link": "http://arxiv.org/pdf/1910.06444v1"
  },
  {
    "Title": "AI-based Drone Assisted Human Rescue in Disaster Environments:\n  Challenges and Opportunities",
    "Authors": "Narek Papyan, Michel Kulhandjian, Hovannes Kulhandjian, Levon Hakob Aslanyan",
    "Published": "2024-06-22T15:39:46Z",
    "Summary": "In this survey we are focusing on utilizing drone-based systems for the detection of individuals, particularly by identifying human screams and other distress signals. This study has significant relevance in post-disaster scenarios, including events such as earthquakes, hurricanes, military conflicts, wildfires, and more. These drones are capable of hovering over disaster-stricken areas that may be challenging for rescue teams to access directly. Unmanned aerial vehicles (UAVs), commonly referred to as drones, are frequently deployed for search-and-rescue missions during disaster situations. Typically, drones capture aerial images to assess structural damage and identify the extent of the disaster. They also employ thermal imaging technology to detect body heat signatures, which can help locate individuals. In some cases, larger drones are used to deliver essential supplies to people stranded in isolated disaster-stricken areas. In our discussions, we delve into the unique challenges associated with locating humans through aerial acoustics. The auditory system must distinguish between human cries and sounds that occur naturally, such as animal calls and wind. Additionally, it should be capable of recognizing distinct patterns related to signals like shouting, clapping, or other ways in which people attempt to signal rescue teams. To tackle this challenge, one solution involves harnessing artificial intelligence (AI) to analyze sound frequencies and identify common audio signatures. Deep learning-based networks, such as convolutional neural networks (CNNs), can be trained using these signatures to filter out noise generated by drone motors and other environmental factors. Furthermore, employing signal processing techniques like the direction of arrival (DOA) based on microphone array signals can enhance the precision of tracking the source of human noises.",
    "Link": "http://arxiv.org/abs/2406.15875v2",
    "PDF Link": "http://arxiv.org/pdf/2406.15875v2"
  },
  {
    "Title": "Generic Knowledge Boosted Pre-training For Remote Sensing Images",
    "Authors": "Ziyue Huang, Mingming Zhang, Yuan Gong, Qingjie Liu, Yunhong Wang",
    "Published": "2024-01-09T15:36:07Z",
    "Summary": "Deep learning models are essential for scene classification, change detection, land cover segmentation, and other remote sensing image understanding tasks. Most backbones of existing remote sensing deep learning models are typically initialized by pre-trained weights obtained from ImageNet pre-training (IMP). However, domain gaps exist between remote sensing images and natural images (e.g., ImageNet), making deep learning models initialized by pre-trained weights of IMP perform poorly for remote sensing image understanding. Although some pre-training methods are studied in the remote sensing community, current remote sensing pre-training methods face the problem of vague generalization by only using remote sensing images. In this paper, we propose a novel remote sensing pre-training framework, Generic Knowledge Boosted Remote Sensing Pre-training (GeRSP), to learn robust representations from remote sensing and natural images for remote sensing understanding tasks. GeRSP contains two pre-training branches: (1) A self-supervised pre-training branch is adopted to learn domain-related representations from unlabeled remote sensing images. (2) A supervised pre-training branch is integrated into GeRSP for general knowledge learning from labeled natural images. Moreover, GeRSP combines two pre-training branches using a teacher-student architecture to simultaneously learn representations with general and special knowledge, which generates a powerful pre-trained model for deep learning model initialization. Finally, we evaluate GeRSP and other remote sensing pre-training methods on three downstream tasks, i.e., object detection, semantic segmentation, and scene classification. The extensive experimental results consistently demonstrate that GeRSP can effectively learn robust representations in a unified manner, improving the performance of remote sensing downstream tasks.",
    "Link": "http://arxiv.org/abs/2401.04614v2",
    "PDF Link": "http://arxiv.org/pdf/2401.04614v2"
  },
  {
    "Title": "Mini-Unmanned Aerial Vehicle-Based Remote Sensing: Techniques,\n  Applications, and Prospects",
    "Authors": "Tian-Zhu Xiang, Gui-Song Xia, Liangpei Zhang",
    "Published": "2018-12-19T06:12:09Z",
    "Summary": "The past few decades have witnessed the great progress of unmanned aircraft vehicles (UAVs) in civilian fields, especially in photogrammetry and remote sensing. In contrast with the platforms of manned aircraft and satellite, the UAV platform holds many promising characteristics: flexibility, efficiency, high-spatial/temporal resolution, low cost, easy operation, etc., which make it an effective complement to other remote-sensing platforms and a cost-effective means for remote sensing. Considering the popularity and expansion of UAV-based remote sensing in recent years, this paper provides a systematic survey on the recent advances and future prospectives of UAVs in the remote-sensing community. Specifically, the main challenges and key technologies of remote-sensing data processing based on UAVs are discussed and summarized firstly. Then, we provide an overview of the widespread applications of UAVs in remote sensing. Finally, some prospects for future work are discussed. We hope this paper will provide remote-sensing researchers an overall picture of recent UAV-based remote sensing developments and help guide the further research on this topic.",
    "Link": "http://arxiv.org/abs/1812.07770v3",
    "PDF Link": "http://arxiv.org/pdf/1812.07770v3"
  },
  {
    "Title": "Image Fusion Technologies In Commercial Remote Sensing Packages",
    "Authors": "Firouz Abdullah Al-Wassai, N. V. Kalyankar",
    "Published": "2013-07-09T13:14:11Z",
    "Summary": "Several remote sensing software packages are used to the explicit purpose of analyzing and visualizing remotely sensed data, with the developing of remote sensing sensor technologies from last ten years. Accord-ing to literature, the remote sensing is still the lack of software tools for effective information extraction from remote sensing data. So, this paper provides a state-of-art of multi-sensor image fusion technologies as well as review on the quality evaluation of the single image or fused images in the commercial remote sensing pack-ages. It also introduces program (ALwassaiProcess) developed for image fusion and classification.",
    "Link": "http://arxiv.org/abs/1307.2440v1",
    "PDF Link": "http://arxiv.org/pdf/1307.2440v1"
  },
  {
    "Title": "Remote Sensing ChatGPT: Solving Remote Sensing Tasks with ChatGPT and\n  Visual Models",
    "Authors": "Haonan Guo, Xin Su, Chen Wu, Bo Du, Liangpei Zhang, Deren Li",
    "Published": "2024-01-17T09:44:07Z",
    "Summary": "Recently, the flourishing large language models(LLM), especially ChatGPT, have shown exceptional performance in language understanding, reasoning, and interaction, attracting users and researchers from multiple fields and domains. Although LLMs have shown great capacity to perform human-like task accomplishment in natural language and natural image, their potential in handling remote sensing interpretation tasks has not yet been fully explored. Moreover, the lack of automation in remote sensing task planning hinders the accessibility of remote sensing interpretation techniques, especially to non-remote sensing experts from multiple research fields. To this end, we present Remote Sensing ChatGPT, an LLM-powered agent that utilizes ChatGPT to connect various AI-based remote sensing models to solve complicated interpretation tasks. More specifically, given a user request and a remote sensing image, we utilized ChatGPT to understand user requests, perform task planning according to the tasks' functions, execute each subtask iteratively, and generate the final response according to the output of each subtask. Considering that LLM is trained with natural language and is not capable of directly perceiving visual concepts as contained in remote sensing images, we designed visual cues that inject visual information into ChatGPT. With Remote Sensing ChatGPT, users can simply send a remote sensing image with the corresponding request, and get the interpretation results as well as language feedback from Remote Sensing ChatGPT. Experiments and examples show that Remote Sensing ChatGPT can tackle a wide range of remote sensing tasks and can be extended to more tasks with more sophisticated models such as the remote sensing foundation model. The code and demo of Remote Sensing ChatGPT is publicly available at https://github.com/HaonanGuo/Remote-Sensing-ChatGPT .",
    "Link": "http://arxiv.org/abs/2401.09083v1",
    "PDF Link": "http://arxiv.org/pdf/2401.09083v1"
  },
  {
    "Title": "Exploring Models and Data for Remote Sensing Image Caption Generation",
    "Authors": "Xiaoqiang Lu, Binqiang Wang, Xiangtao Zheng, Xuelong Li",
    "Published": "2017-12-21T08:45:37Z",
    "Summary": "Inspired by recent development of artificial satellite, remote sensing images have attracted extensive attention. Recently, noticeable progress has been made in scene classification and target detection.However, it is still not clear how to describe the remote sensing image content with accurate and concise sentences. In this paper, we investigate to describe the remote sensing images with accurate and flexible sentences. First, some annotated instructions are presented to better describe the remote sensing images considering the special characteristics of remote sensing images. Second, in order to exhaustively exploit the contents of remote sensing images, a large-scale aerial image data set is constructed for remote sensing image caption. Finally, a comprehensive review is presented on the proposed data set to fully advance the task of remote sensing caption. Extensive experiments on the proposed data set demonstrate that the content of the remote sensing image can be completely described by generating language descriptions. The data set is available at https://github.com/201528014227051/RSICD_optimal",
    "Link": "http://arxiv.org/abs/1712.07835v1",
    "PDF Link": "http://arxiv.org/pdf/1712.07835v1"
  },
  {
    "Title": "Remote Sensing Image Scene Classification Meets Deep Learning:\n  Challenges, Methods, Benchmarks, and Opportunities",
    "Authors": "Gong Cheng, Xingxing Xie, Junwei Han, Lei Guo, Gui-Song Xia",
    "Published": "2020-05-03T14:18:00Z",
    "Summary": "Remote sensing image scene classification, which aims at labeling remote sensing images with a set of semantic categories based on their contents, has broad applications in a range of fields. Propelled by the powerful feature learning capabilities of deep neural networks, remote sensing image scene classification driven by deep learning has drawn remarkable attention and achieved significant breakthroughs. However, to the best of our knowledge, a comprehensive review of recent achievements regarding deep learning for scene classification of remote sensing images is still lacking. Considering the rapid evolution of this field, this paper provides a systematic survey of deep learning methods for remote sensing image scene classification by covering more than 160 papers. To be specific, we discuss the main challenges of remote sensing image scene classification and survey (1) Autoencoder-based remote sensing image scene classification methods, (2) Convolutional Neural Network-based remote sensing image scene classification methods, and (3) Generative Adversarial Network-based remote sensing image scene classification methods. In addition, we introduce the benchmarks used for remote sensing image scene classification and summarize the performance of more than two dozen of representative algorithms on three commonly-used benchmark data sets. Finally, we discuss the promising opportunities for further research.",
    "Link": "http://arxiv.org/abs/2005.01094v2",
    "PDF Link": "http://arxiv.org/pdf/2005.01094v2"
  },
  {
    "Title": "In-domain representation learning for remote sensing",
    "Authors": "Maxim Neumann, Andre Susano Pinto, Xiaohua Zhai, Neil Houlsby",
    "Published": "2019-11-15T16:09:38Z",
    "Summary": "Given the importance of remote sensing, surprisingly little attention has been paid to it by the representation learning community. To address it and to establish baselines and a common evaluation protocol in this domain, we provide simplified access to 5 diverse remote sensing datasets in a standardized form. Specifically, we investigate in-domain representation learning to develop generic remote sensing representations and explore which characteristics are important for a dataset to be a good source for remote sensing representation learning. The established baselines achieve state-of-the-art performance on these datasets.",
    "Link": "http://arxiv.org/abs/1911.06721v1",
    "PDF Link": "http://arxiv.org/pdf/1911.06721v1"
  },
  {
    "Title": "Bootstrapping Interactive Image-Text Alignment for Remote Sensing Image\n  Captioning",
    "Authors": "Cong Yang, Zuchao Li, Lefei Zhang",
    "Published": "2023-12-02T17:32:17Z",
    "Summary": "Recently, remote sensing image captioning has gained significant attention in the remote sensing community. Due to the significant differences in spatial resolution of remote sensing images, existing methods in this field have predominantly concentrated on the fine-grained extraction of remote sensing image features, but they cannot effectively handle the semantic consistency between visual features and textual features. To efficiently align the image-text, we propose a novel two-stage vision-language pre-training-based approach to bootstrap interactive image-text alignment for remote sensing image captioning, called BITA, which relies on the design of a lightweight interactive Fourier Transformer to better align remote sensing image-text features. The Fourier layer in the interactive Fourier Transformer is capable of extracting multi-scale features of remote sensing images in the frequency domain, thereby reducing the redundancy of remote sensing visual features. Specifically, the first stage involves preliminary alignment through image-text contrastive learning, which aligns the learned multi-scale remote sensing features from the interactive Fourier Transformer with textual features. In the second stage, the interactive Fourier Transformer connects the frozen image encoder with a large language model. Then, prefix causal language modeling is utilized to guide the text generation process using visual features. Ultimately, across the UCM-caption, RSICD, and NWPU-caption datasets, the experimental results clearly demonstrate that BITA outperforms other advanced comparative approaches. The code is available at https://github.com/yangcong356/BITA.",
    "Link": "http://arxiv.org/abs/2312.01191v1",
    "PDF Link": "http://arxiv.org/pdf/2312.01191v1"
  },
  {
    "Title": "RS-Mamba for Large Remote Sensing Image Dense Prediction",
    "Authors": "Sijie Zhao, Hao Chen, Xueliang Zhang, Pengfeng Xiao, Lei Bai, Wanli Ouyang",
    "Published": "2024-04-03T12:06:01Z",
    "Summary": "Context modeling is critical for remote sensing image dense prediction tasks. Nowadays, the growing size of very-high-resolution (VHR) remote sensing images poses challenges in effectively modeling context. While transformer-based models possess global modeling capabilities, they encounter computational challenges when applied to large VHR images due to their quadratic complexity. The conventional practice of cropping large images into smaller patches results in a notable loss of contextual information. To address these issues, we propose the Remote Sensing Mamba (RSM) for dense prediction tasks in large VHR remote sensing images. RSM is specifically designed to capture the global context of remote sensing images with linear complexity, facilitating the effective processing of large VHR images. Considering that the land covers in remote sensing images are distributed in arbitrary spatial directions due to characteristics of remote sensing over-head imaging, the RSM incorporates an omnidirectional selective scan module to globally model the context of images in multiple directions, capturing large spatial features from various directions. Extensive experiments on semantic segmentation and change detection tasks across various land covers demonstrate the effectiveness of the proposed RSM. We designed simple yet effective models based on RSM, achieving state-of-the-art performance on dense prediction tasks in VHR remote sensing images without fancy training strategies. Leveraging the linear complexity and global modeling capabilities, RSM achieves better efficiency and accuracy than transformer-based models on large remote sensing images. Interestingly, we also demonstrated that our model generally performs better with a larger image size on dense prediction tasks. Our code is available at https://github.com/walking-shadow/Official_Remote_Sensing_Mamba.",
    "Link": "http://arxiv.org/abs/2404.02668v2",
    "PDF Link": "http://arxiv.org/pdf/2404.02668v2"
  },
  {
    "Title": "Remote sensor response study in the regime of the microwave\n  radiation-induced magnetoresistance oscillations",
    "Authors": "Tianyu Ye, Ramesh Mani, Werner Wegscheider",
    "Published": "2013-11-13T20:33:59Z",
    "Summary": "A concurrent remote sensing and magneto-transport study of the microwave excited two dimensional electron system (2DES) at liquid Helium temperatures has been carried out using a carbon detector to remotely sense the microwave activity of the 2D electron system in the GaAs/AlGaAs heterostructure during conventional magnetotransport measurements. Various correlations are observed and reported between the oscillatory magnetotransport and the remotely sensed reflection. In addition, the oscillatory remotely sensed signal is shown to exhibit a power law type variation in its amplitude, similar to the radiation-induced magnetoresistance oscillations.",
    "Link": "http://arxiv.org/abs/1311.3283v1",
    "PDF Link": "http://arxiv.org/pdf/1311.3283v1"
  },
  {
    "Title": "Supervised Classification Performance of Multispectral Images",
    "Authors": "K. Perumal, R. Bhaskaran",
    "Published": "2010-02-22T03:12:14Z",
    "Summary": "Nowadays government and private agencies use remote sensing imagery for a wide range of applications from military applications to farm development. The images may be a panchromatic, multispectral, hyperspectral or even ultraspectral of terra bytes. Remote sensing image classification is one amongst the most significant application worlds for remote sensing. A few number of image classification algorithms have proved good precision in classifying remote sensing data. But, of late, due to the increasing spatiotemporal dimensions of the remote sensing data, traditional classification algorithms have exposed weaknesses necessitating further research in the field of remote sensing image classification. So an efficient classifier is needed to classify the remote sensing images to extract information. We are experimenting with both supervised and unsupervised classification. Here we compare the different classification methods and their performances. It is found that Mahalanobis classifier performed the best in our classification.",
    "Link": "http://arxiv.org/abs/1002.4046v1",
    "PDF Link": "http://arxiv.org/pdf/1002.4046v1"
  },
  {
    "Title": "What do We Learn by Semantic Scene Understanding for Remote Sensing\n  imagery in CNN framework?",
    "Authors": "Haifeng Li, Jian Peng, Chao Tao, Jie Chen, Min Deng",
    "Published": "2017-05-19T16:23:53Z",
    "Summary": "Recently, deep convolutional neural network (DCNN) achieved increasingly remarkable success and rapidly developed in the field of natural image recognition. Compared with the natural image, the scale of remote sensing image is larger and the scene and the object it represents are more macroscopic. This study inquires whether remote sensing scene and natural scene recognitions differ and raises the following questions: What are the key factors in remote sensing scene recognition? Is the DCNN recognition mechanism centered on object recognition still applicable to the scenarios of remote sensing scene understanding? We performed several experiments to explore the influence of the DCNN structure and the scale of remote sensing scene understanding from the perspective of scene complexity. Our experiment shows that understanding a complex scene depends on an in-depth network and multiple-scale perception. Using a visualization method, we qualitatively and quantitatively analyze the recognition mechanism in a complex remote sensing scene and demonstrate the importance of multi-objective joint semantic support.",
    "Link": "http://arxiv.org/abs/1705.07077v1",
    "PDF Link": "http://arxiv.org/pdf/1705.07077v1"
  },
  {
    "Title": "RS-YOLOX: A High Precision Detector for Object Detection in Satellite\n  Remote Sensing Images",
    "Authors": "Lei Yang, Guowu Yuan, Hao Zhou, Hongyu Liu, Jian Chen, Hao Wu",
    "Published": "2025-02-05T03:05:33Z",
    "Summary": "Automatic object detection by satellite remote sensing images is of great significance for resource exploration and natural disaster assessment. To solve existing problems in remote sensing image detection, this article proposes an improved YOLOX model for satellite remote sensing image automatic detection. This model is named RS-YOLOX. To strengthen the feature learning ability of the network, we used Efficient Channel Attention (ECA) in the backbone network of YOLOX and combined the Adaptively Spatial Feature Fusion (ASFF) with the neck network of YOLOX. To balance the numbers of positive and negative samples in training, we used the Varifocal Loss function. Finally, to obtain a high-performance remote sensing object detector, we combined the trained model with an open-source framework called Slicing Aided Hyper Inference (SAHI). This work evaluated models on three aerial remote sensing datasets (DOTA-v1.5, TGRS-HRRSD, and RSOD). Our comparative experiments demonstrate that our model has the highest accuracy in detecting objects in remote sensing image datasets.",
    "Link": "http://arxiv.org/abs/2502.02850v1",
    "PDF Link": "http://arxiv.org/pdf/2502.02850v1"
  },
  {
    "Title": "Galileo: Learning Global and Local Features in Pretrained Remote Sensing\n  Models",
    "Authors": "Gabriel Tseng, Anthony Fuller, Marlena Reil, Henry Herzog, Patrick Beukema, Favyen Bastani, James R. Green, Evan Shelhamer, Hannah Kerner, David Rolnick",
    "Published": "2025-02-13T14:21:03Z",
    "Summary": "From crop mapping to flood detection, machine learning in remote sensing has a wide range of societally beneficial applications. The commonalities between remote sensing data in these applications present an opportunity for pretrained machine learning models tailored to remote sensing to reduce the labeled data and effort required to solve individual tasks. However, such models must be: (i) flexible enough to ingest input data of varying sensor modalities and shapes (i.e., of varying spatial and temporal dimensions), and (ii) able to model Earth surface phenomena of varying scales and types. To solve this gap, we present Galileo, a family of pretrained remote sensing models designed to flexibly process multimodal remote sensing data. We also introduce a novel and highly effective self-supervised learning approach to learn both large- and small-scale features, a challenge not addressed by previous models. Our Galileo models obtain state-of-the-art results across diverse remote sensing tasks.",
    "Link": "http://arxiv.org/abs/2502.09356v1",
    "PDF Link": "http://arxiv.org/pdf/2502.09356v1"
  },
  {
    "Title": "Transformers in Remote Sensing: A Survey",
    "Authors": "Abdulaziz Amer Aleissaee, Amandeep Kumar, Rao Muhammad Anwer, Salman Khan, Hisham Cholakkal, Gui-Song Xia, Fahad Shahbaz khan",
    "Published": "2022-09-02T17:57:05Z",
    "Summary": "Deep learning-based algorithms have seen a massive popularity in different areas of remote sensing image analysis over the past decade. Recently, transformers-based architectures, originally introduced in natural language processing, have pervaded computer vision field where the self-attention mechanism has been utilized as a replacement to the popular convolution operator for capturing long-range dependencies. Inspired by recent advances in computer vision, remote sensing community has also witnessed an increased exploration of vision transformers for a diverse set of tasks. Although a number of surveys have focused on transformers in computer vision in general, to the best of our knowledge we are the first to present a systematic review of recent advances based on transformers in remote sensing. Our survey covers more than 60 recent transformers-based methods for different remote sensing problems in sub-areas of remote sensing: very high-resolution (VHR), hyperspectral (HSI) and synthetic aperture radar (SAR) imagery. We conclude the survey by discussing different challenges and open issues of transformers in remote sensing. Additionally, we intend to frequently update and maintain the latest transformers in remote sensing papers with their respective code at: https://github.com/VIROBO-15/Transformer-in-Remote-Sensing",
    "Link": "http://arxiv.org/abs/2209.01206v1",
    "PDF Link": "http://arxiv.org/pdf/2209.01206v1"
  },
  {
    "Title": "Effective Features of Remote Sensing Image Classification Using\n  Interactive Adaptive Thresholding Method",
    "Authors": "T. Balaji, Dr. M. Sumathi",
    "Published": "2014-01-30T05:33:27Z",
    "Summary": "Remote sensing image classification can be performed in many different ways to extract meaningful features. One common approach is to perform edge detection. A second approach is to try and detect whole shapes, given the fact that these shapes usually tend to have distinctive properties such as object foreground or background. To get optimal results, these two approaches can be combined. This paper adopts a combinatorial optimization method to adaptively select threshold based features to improve remote sensing image. Feature selection is an important combinatorial optimization problem in the remote sensing image classification. The feature selection method has to achieve three characteristics: first the performance issues by facilitating data collection and reducing storage space and classification time, second to perform semantics analysis helping to understand the problem, and third to improve prediction accuracy by avoiding the curse of dimensionality. The goal of this thresholding an image is to classify pixels as either dark or light and evaluation of classification results. Interactive adaptive thresholding is a form of thresholding that takes into account spatial variations in illumination of remote sensing image. We present a technique for remote sensing based adaptive thresholding using the interactive satellite image of the input. However, our solution is more robust to illumination changes in the remote sensing image. Additionally, our method is simple and easy to implement but it is effective algorithm to classify the image pixels. This technique is suitable for preprocessing the remote sensing image classification, making it a valuable tool for interactive remote based applications such as augmented reality of the classification procedure.",
    "Link": "http://arxiv.org/abs/1401.7743v1",
    "PDF Link": "http://arxiv.org/pdf/1401.7743v1"
  },
  {
    "Title": "Experimental demonstration of secure quantum remote sensing",
    "Authors": "Peng Yin, Yuki Takeuchi, Wen-Hao Zhang, Zhen-Qiang Yin, Yuichiro Matsuzaki, Xing-Xiang Peng, Xiao-Ye Xu, Jin-Shi Xu, Jian-Shun Tang, Zong-Quan Zhou, Geng Chen, Chuan-Feng Li, Guang-Can Guo",
    "Published": "2019-07-15T13:00:47Z",
    "Summary": "Quantum metrology aims to enhance the precision of various measurement tasks by taking advantages of quantum properties. In many scenarios, precision is not the sole target; the acquired information must be protected once it is generated in the sensing process. Considering a remote sensing scenario where a local site performs cooperative sensing with a remote site to collect private information at the remote site, the loss of sensing data inevitably causes private information to be revealed. Quantum key distribution is known to be a reliable solution for secure data transmission, however, it fails if an eavesdropper accesses the sensing data generated at a remote site. In this study, we demonstrate that by sharing entanglement between local and remote sites, secure quantum remote sensing can be realized, and the secure level is characterized by asymmetric Fisher information gain. Concretely, only the local site can acquire the estimated parameter accurately with Fisher information approaching 1. In contrast, the accessible Fisher information for an eavesdropper is nearly zero even if he/she obtains the raw sensing data at the remote site. This achievement is primarily due to the nonlocal calibration and steering of the probe state at the remote site. Our results explore one significant advantage of ``quantumness'' and extend the notion of quantum metrology to the security realm.",
    "Link": "http://arxiv.org/abs/1907.06480v1",
    "PDF Link": "http://arxiv.org/pdf/1907.06480v1"
  },
  {
    "Title": "ZoRI: Towards Discriminative Zero-Shot Remote Sensing Instance\n  Segmentation",
    "Authors": "Shiqi Huang, Shuting He, Bihan Wen",
    "Published": "2024-12-17T11:00:56Z",
    "Summary": "Instance segmentation algorithms in remote sensing are typically based on conventional methods, limiting their application to seen scenarios and closed-set predictions. In this work, we propose a novel task called zero-shot remote sensing instance segmentation, aimed at identifying aerial objects that are absent from training data. Challenges arise when classifying aerial categories with high inter-class similarity and intra-class variance. Besides, the domain gap between vision-language models' pretraining datasets and remote sensing datasets hinders the zero-shot capabilities of the pretrained model when it is directly applied to remote sensing images. To address these challenges, we propose a $\\textbf{Z}$ero-Sh$\\textbf{o}$t $\\textbf{R}$emote Sensing $\\textbf{I}$nstance Segmentation framework, dubbed $\\textbf{ZoRI}$. Our approach features a discrimination-enhanced classifier that uses refined textual embeddings to increase the awareness of class disparities. Instead of direct fine-tuning, we propose a knowledge-maintained adaptation strategy that decouples semantic-related information to preserve the pretrained vision-language alignment while adjusting features to capture remote sensing domain-specific visual cues. Additionally, we introduce a prior-injected prediction with cache bank of aerial visual prototypes to supplement the semantic richness of text embeddings and seamlessly integrate aerial representations, adapting to the remote sensing domain. We establish new experimental protocols and benchmarks, and extensive experiments convincingly demonstrate that ZoRI achieves the state-of-art performance on the zero-shot remote sensing instance segmentation task. Our code is available at https://github.com/HuangShiqi128/ZoRI.",
    "Link": "http://arxiv.org/abs/2412.12798v1",
    "PDF Link": "http://arxiv.org/pdf/2412.12798v1"
  },
  {
    "Title": "Multi-scale PIIFD for Registration of Multi-source Remote Sensing Images",
    "Authors": "Chenzhong Gao, Wei Li",
    "Published": "2021-04-26T13:44:48Z",
    "Summary": "This paper aims at providing multi-source remote sensing images registered in geometric space for image fusion. Focusing on the characteristics and differences of multi-source remote sensing images, a feature-based registration algorithm is implemented. The key technologies include image scale-space for implementing multi-scale properties, Harris corner detection for keypoints extraction, and partial intensity invariant feature descriptor (PIIFD) for keypoints description. Eventually, a multi-scale Harris-PIIFD image registration algorithm framework is proposed. The experimental results of four sets of representative real data show that the algorithm has excellent, stable performance in multi-source remote sensing image registration, and can achieve accurate spatial alignment, which has strong practical application value and certain generalization ability.",
    "Link": "http://arxiv.org/abs/2104.12572v1",
    "PDF Link": "http://arxiv.org/pdf/2104.12572v1"
  },
  {
    "Title": "Deep Attention Unet: A Network Model with Global Feature Perception\n  Ability",
    "Authors": "Jiacheng Li",
    "Published": "2023-04-21T09:12:29Z",
    "Summary": "Remote sensing image segmentation is a specific task of remote sensing image interpretation. A good remote sensing image segmentation algorithm can provide guidance for environmental protection, agricultural production, and urban construction. This paper proposes a new type of UNet image segmentation algorithm based on channel self attention mechanism and residual connection called . In my experiment, the new network model improved mIOU by 2.48% compared to traditional UNet on the FoodNet dataset. The image segmentation algorithm proposed in this article enhances the internal connections between different items in the image, thus achieving better segmentation results for remote sensing images with occlusion.",
    "Link": "http://arxiv.org/abs/2304.10829v2",
    "PDF Link": "http://arxiv.org/pdf/2304.10829v2"
  },
  {
    "Title": "Kronecker Product Feature Fusion for Convolutional Neural Network in\n  Remote Sensing Scene Classification",
    "Authors": "Yinzhu Cheng",
    "Published": "2024-01-08T19:01:01Z",
    "Summary": "Remote Sensing Scene Classification is a challenging and valuable research topic, in which Convolutional Neural Network (CNN) has played a crucial role. CNN can extract hierarchical convolutional features from remote sensing imagery, and Feature Fusion of different layers can enhance CNN's performance. Two successful Feature Fusion methods, Add and Concat, are employed in certain state-of-the-art CNN algorithms. In this paper, we propose a novel Feature Fusion algorithm, which unifies the aforementioned methods using the Kronecker Product (KPFF), and we discuss the Backpropagation procedure associated with this algorithm. To validate the efficacy of the proposed method, a series of experiments are designed and conducted. The results demonstrate its effectiveness of enhancing CNN's accuracy in Remote sensing scene classification.",
    "Link": "http://arxiv.org/abs/2402.00036v1",
    "PDF Link": "http://arxiv.org/pdf/2402.00036v1"
  },
  {
    "Title": "Seasonal Contrast: Unsupervised Pre-Training from Uncurated Remote\n  Sensing Data",
    "Authors": "Oscar Mañas, Alexandre Lacoste, Xavier Giro-i-Nieto, David Vazquez, Pau Rodriguez",
    "Published": "2021-03-30T18:26:39Z",
    "Summary": "Remote sensing and automatic earth monitoring are key to solve global-scale challenges such as disaster prevention, land use monitoring, or tackling climate change. Although there exist vast amounts of remote sensing data, most of it remains unlabeled and thus inaccessible for supervised learning algorithms. Transfer learning approaches can reduce the data requirements of deep learning algorithms. However, most of these methods are pre-trained on ImageNet and their generalization to remote sensing imagery is not guaranteed due to the domain gap. In this work, we propose Seasonal Contrast (SeCo), an effective pipeline to leverage unlabeled data for in-domain pre-training of remote sensing representations. The SeCo pipeline is composed of two parts. First, a principled procedure to gather large-scale, unlabeled and uncurated remote sensing datasets containing images from multiple Earth locations at different timestamps. Second, a self-supervised algorithm that takes advantage of time and position invariance to learn transferable representations for remote sensing applications. We empirically show that models trained with SeCo achieve better performance than their ImageNet pre-trained counterparts and state-of-the-art self-supervised learning methods on multiple downstream tasks. The datasets and models in SeCo will be made public to facilitate transfer learning and enable rapid progress in remote sensing applications.",
    "Link": "http://arxiv.org/abs/2103.16607v2",
    "PDF Link": "http://arxiv.org/pdf/2103.16607v2"
  },
  {
    "Title": "A Light-Weight Object Detection Framework with FPA Module for Optical\n  Remote Sensing Imagery",
    "Authors": "Xi Gu, Lingbin Kong, Zhicheng Wang, Jie Li, Zhaohui Yu, Gang Wei",
    "Published": "2020-09-07T12:41:17Z",
    "Summary": "With the development of remote sensing technology, the acquisition of remote sensing images is easier and easier, which provides sufficient data resources for the task of detecting remote sensing objects. However, how to detect objects quickly and accurately from many complex optical remote sensing images is a challenging hot issue. In this paper, we propose an efficient anchor free object detector, CenterFPANet. To pursue speed, we use a lightweight backbone and introduce the asymmetric revolution block. To improve the accuracy, we designed the FPA module, which links the feature maps of different levels, and introduces the attention mechanism to dynamically adjust the weights of each level of feature maps, which solves the problem of detection difficulty caused by large size range of remote sensing objects. This strategy can improve the accuracy of remote sensing image object detection without reducing the detection speed. On the DOTA dataset, CenterFPANet mAP is 64.00%, and FPS is 22.2, which is close to the accuracy of the anchor-based methods currently used and much faster than them. Compared with Faster RCNN, mAP is 6.76% lower but 60.87% faster. All in all, CenterFPANet achieves a balance between speed and accuracy in large-scale optical remote sensing object detection.",
    "Link": "http://arxiv.org/abs/2009.03063v1",
    "PDF Link": "http://arxiv.org/pdf/2009.03063v1"
  },
  {
    "Title": "Geographical Knowledge-driven Representation Learning for Remote Sensing\n  Images",
    "Authors": "Wenyuan Li, Keyan Chen, Hao Chen, Zhenwei Shi",
    "Published": "2021-07-12T09:23:15Z",
    "Summary": "The proliferation of remote sensing satellites has resulted in a massive amount of remote sensing images. However, due to human and material resource constraints, the vast majority of remote sensing images remain unlabeled. As a result, it cannot be applied to currently available deep learning methods. To fully utilize the remaining unlabeled images, we propose a Geographical Knowledge-driven Representation learning method for remote sensing images (GeoKR), improving network performance and reduce the demand for annotated data. The global land cover products and geographical location associated with each remote sensing image are regarded as geographical knowledge to provide supervision for representation learning and network pre-training. An efficient pre-training framework is proposed to eliminate the supervision noises caused by imaging times and resolutions difference between remote sensing images and geographical knowledge. A large scale pre-training dataset Levir-KR is proposed to support network pre-training. It contains 1,431,950 remote sensing images from Gaofen series satellites with various resolutions. Experimental results demonstrate that our proposed method outperforms ImageNet pre-training and self-supervised representation learning methods and significantly reduces the burden of data annotation on downstream tasks such as scene classification, semantic segmentation, object detection, and cloud / snow detection. It demonstrates that our proposed method can be used as a novel paradigm for pre-training neural networks. Codes will be available on https://github.com/flyakon/Geographical-Knowledge-driven-Representaion-Learning.",
    "Link": "http://arxiv.org/abs/2107.05276v1",
    "PDF Link": "http://arxiv.org/pdf/2107.05276v1"
  },
  {
    "Title": "Optical Remote Sensing Image Understanding with Weak Supervision:\n  Concepts, Methods, and Perspectives",
    "Authors": "Jun Yue, Leyuan Fang, Pedram Ghamisi, Weiying Xie, Jun Li, Jocelyn Chanussot, Antonio J Plaza",
    "Published": "2022-04-18T09:23:22Z",
    "Summary": "In recent years, supervised learning has been widely used in various tasks of optical remote sensing image understanding, including remote sensing image classification, pixel-wise segmentation, change detection, and object detection. The methods based on supervised learning need a large amount of high-quality training data and their performance highly depends on the quality of the labels. However, in practical remote sensing applications, it is often expensive and time-consuming to obtain large-scale data sets with high-quality labels, which leads to a lack of sufficient supervised information. In some cases, only coarse-grained labels can be obtained, resulting in the lack of exact supervision. In addition, the supervised information obtained manually may be wrong, resulting in a lack of accurate supervision. Therefore, remote sensing image understanding often faces the problems of incomplete, inexact, and inaccurate supervised information, which will affect the breadth and depth of remote sensing applications. In order to solve the above-mentioned problems, researchers have explored various tasks in remote sensing image understanding under weak supervision. This paper summarizes the research progress of weakly supervised learning in the field of remote sensing, including three typical weakly supervised paradigms: 1) Incomplete supervision, where only a subset of training data is labeled; 2) Inexact supervision, where only coarse-grained labels of training data are given; 3) Inaccurate supervision, where the labels given are not always true on the ground.",
    "Link": "http://arxiv.org/abs/2204.09120v1",
    "PDF Link": "http://arxiv.org/pdf/2204.09120v1"
  },
  {
    "Title": "Supervised and Contrastive Self-Supervised In-Domain Representation\n  Learning for Dense Prediction Problems in Remote Sensing",
    "Authors": "Ali Ghanbarzade, Hossein Soleimani",
    "Published": "2023-01-29T20:56:51Z",
    "Summary": "In recent years Convolutional neural networks (CNN) have made significant progress in computer vision. These advancements have been applied to other areas, such as remote sensing and have shown satisfactory results. However, the lack of large labeled datasets and the inherent complexity of remote sensing problems have made it difficult to train deep CNNs for dense prediction problems. To solve this issue, ImageNet pretrained weights have been used as a starting point in various dense predictions tasks. Although this type of transfer learning has led to improvements, the domain difference between natural and remote sensing images has also limited the performance of deep CNNs. On the other hand, self-supervised learning methods for learning visual representations from large unlabeled images have grown substantially over the past two years. Accordingly, in this paper we have explored the effectiveness of in-domain representations in both supervised and self-supervised forms to solve the domain difference between remote sensing and the ImageNet dataset. The obtained weights from remote sensing images are utilized as initial weights for solving semantic segmentation and object detection tasks and state-of-the-art results are obtained. For self-supervised pre-training, we have utilized the SimSiam algorithm as it is simple and does not need huge computational resources. One of the most influential factors in acquiring general visual representations from remote sensing images is the pre-training dataset. To examine the effect of the pre-training dataset, equal-sized remote sensing datasets are used for pre-training. Our results have demonstrated that using datasets with a high spatial resolution for self-supervised representation learning leads to high performance in downstream tasks.",
    "Link": "http://arxiv.org/abs/2301.12541v1",
    "PDF Link": "http://arxiv.org/pdf/2301.12541v1"
  },
  {
    "Title": "FireRisk: A Remote Sensing Dataset for Fire Risk Assessment with\n  Benchmarks Using Supervised and Self-supervised Learning",
    "Authors": "Shuchang Shen, Sachith Seneviratne, Xinye Wanyan, Michael Kirley",
    "Published": "2023-03-13T11:54:16Z",
    "Summary": "In recent decades, wildfires, as widespread and extremely destructive natural disasters, have caused tremendous property losses and fatalities, as well as extensive damage to forest ecosystems. Many fire risk assessment projects have been proposed to prevent wildfires, but GIS-based methods are inherently challenging to scale to different geographic areas due to variations in data collection and local conditions. Inspired by the abundance of publicly available remote sensing projects and the burgeoning development of deep learning in computer vision, our research focuses on assessing fire risk using remote sensing imagery.   In this work, we propose a novel remote sensing dataset, FireRisk, consisting of 7 fire risk classes with a total of 91872 labelled images for fire risk assessment. This remote sensing dataset is labelled with the fire risk classes supplied by the Wildfire Hazard Potential (WHP) raster dataset, and remote sensing images are collected using the National Agriculture Imagery Program (NAIP), a high-resolution remote sensing imagery program. On FireRisk, we present benchmark performance for supervised and self-supervised representations, with Masked Autoencoders (MAE) pre-trained on ImageNet1k achieving the highest classification accuracy, 65.29%.   This remote sensing dataset, FireRisk, provides a new direction for fire risk assessment, and we make it publicly available on https://github.com/CharmonyShen/FireRisk.",
    "Link": "http://arxiv.org/abs/2303.07035v2",
    "PDF Link": "http://arxiv.org/pdf/2303.07035v2"
  },
  {
    "Title": "Changes to Captions: An Attentive Network for Remote Sensing Change\n  Captioning",
    "Authors": "Shizhen Chang, Pedram Ghamisi",
    "Published": "2023-04-03T15:51:42Z",
    "Summary": "In recent years, advanced research has focused on the direct learning and analysis of remote sensing images using natural language processing (NLP) techniques. The ability to accurately describe changes occurring in multi-temporal remote sensing images is becoming increasingly important for geospatial understanding and land planning. Unlike natural image change captioning tasks, remote sensing change captioning aims to capture the most significant changes, irrespective of various influential factors such as illumination, seasonal effects, and complex land covers. In this study, we highlight the significance of accurately describing changes in remote sensing images and present a comparison of the change captioning task for natural and synthetic images and remote sensing images. To address the challenge of generating accurate captions, we propose an attentive changes-to-captions network, called Chg2Cap for short, for bi-temporal remote sensing images. The network comprises three main components: 1) a Siamese CNN-based feature extractor to collect high-level representations for each image pair; 2) an attentive decoder that includes a hierarchical self-attention block to locate change-related features and a residual block to generate the image embedding; and 3) a transformer-based caption generator to decode the relationship between the image embedding and the word embedding into a description. The proposed Chg2Cap network is evaluated on two representative remote sensing datasets, and a comprehensive experimental analysis is provided. The code and pre-trained models will be available online at https://github.com/ShizhenChang/Chg2Cap.",
    "Link": "http://arxiv.org/abs/2304.01091v2",
    "PDF Link": "http://arxiv.org/pdf/2304.01091v2"
  },
  {
    "Title": "Defense against Adversarial Cloud Attack on Remote Sensing Salient\n  Object Detection",
    "Authors": "Huiming Sun, Lan Fu, Jinlong Li, Qing Guo, Zibo Meng, Tianyun Zhang, Yuewei Lin, Hongkai Yu",
    "Published": "2023-06-30T07:06:13Z",
    "Summary": "Detecting the salient objects in a remote sensing image has wide applications for the interdisciplinary research. Many existing deep learning methods have been proposed for Salient Object Detection (SOD) in remote sensing images and get remarkable results. However, the recent adversarial attack examples, generated by changing a few pixel values on the original remote sensing image, could result in a collapse for the well-trained deep learning based SOD model. Different with existing methods adding perturbation to original images, we propose to jointly tune adversarial exposure and additive perturbation for attack and constrain image close to cloudy image as Adversarial Cloud. Cloud is natural and common in remote sensing images, however, camouflaging cloud based adversarial attack and defense for remote sensing images are not well studied before. Furthermore, we design DefenseNet as a learn-able pre-processing to the adversarial cloudy images so as to preserve the performance of the deep learning based remote sensing SOD model, without tuning the already deployed deep SOD model. By considering both regular and generalized adversarial examples, the proposed DefenseNet can defend the proposed Adversarial Cloud in white-box setting and other attack methods in black-box setting. Experimental results on a synthesized benchmark from the public remote sensing SOD dataset (EORSSD) show the promising defense against adversarial cloud attacks.",
    "Link": "http://arxiv.org/abs/2306.17431v2",
    "PDF Link": "http://arxiv.org/pdf/2306.17431v2"
  },
  {
    "Title": "Large Language Models for Captioning and Retrieving Remote Sensing\n  Images",
    "Authors": "João Daniel Silva, João Magalhães, Devis Tuia, Bruno Martins",
    "Published": "2024-02-09T15:31:01Z",
    "Summary": "Image captioning and cross-modal retrieval are examples of tasks that involve the joint analysis of visual and linguistic information. In connection to remote sensing imagery, these tasks can help non-expert users in extracting relevant Earth observation information for a variety of applications. Still, despite some previous efforts, the development and application of vision and language models to the remote sensing domain have been hindered by the relatively small size of the available datasets and models used in previous studies. In this work, we propose RS-CapRet, a Vision and Language method for remote sensing tasks, in particular image captioning and text-image retrieval. We specifically propose to use a highly capable large decoder language model together with image encoders adapted to remote sensing imagery through contrastive language-image pre-training. To bridge together the image encoder and language decoder, we propose training simple linear layers with examples from combining different remote sensing image captioning datasets, keeping the other parameters frozen. RS-CapRet can then generate descriptions for remote sensing images and retrieve images from textual descriptions, achieving SOTA or competitive performance with existing methods. Qualitative results illustrate that RS-CapRet can effectively leverage the pre-trained large language model to describe remote sensing images, retrieve them based on different types of queries, and also show the ability to process interleaved sequences of images and text in a dialogue manner.",
    "Link": "http://arxiv.org/abs/2402.06475v1",
    "PDF Link": "http://arxiv.org/pdf/2402.06475v1"
  },
  {
    "Title": "LSKNet: A Foundation Lightweight Backbone for Remote Sensing",
    "Authors": "Yuxuan Li, Xiang Li, Yimian Dai, Qibin Hou, Li Liu, Yongxiang Liu, Ming-Ming Cheng, Jian Yang",
    "Published": "2024-03-18T12:43:38Z",
    "Summary": "Remote sensing images pose distinct challenges for downstream tasks due to their inherent complexity. While a considerable amount of research has been dedicated to remote sensing classification, object detection and semantic segmentation, most of these studies have overlooked the valuable prior knowledge embedded within remote sensing scenarios. Such prior knowledge can be useful because remote sensing objects may be mistakenly recognized without referencing a sufficiently long-range context, which can vary for different objects. This paper considers these priors and proposes a lightweight Large Selective Kernel Network (LSKNet) backbone. LSKNet can dynamically adjust its large spatial receptive field to better model the ranging context of various objects in remote sensing scenarios. To our knowledge, large and selective kernel mechanisms have not been previously explored in remote sensing images. Without bells and whistles, our lightweight LSKNet sets new state-of-the-art scores on standard remote sensing classification, object detection and semantic segmentation benchmarks. Our comprehensive analysis further validated the significance of the identified priors and the effectiveness of LSKNet. The code is available at https://github.com/zcablii/LSKNet.",
    "Link": "http://arxiv.org/abs/2403.11735v5",
    "PDF Link": "http://arxiv.org/pdf/2403.11735v5"
  },
  {
    "Title": "PIR: Remote Sensing Image-Text Retrieval with Prior Instruction\n  Representation Learning",
    "Authors": "Jiancheng Pan, Muyuan Ma, Qing Ma, Cong Bai, Shengyong Chen",
    "Published": "2024-05-16T14:53:45Z",
    "Summary": "Remote sensing image-text retrieval constitutes a foundational aspect of remote sensing interpretation tasks, facilitating the alignment of vision and language representations. This paper introduces a prior instruction representation (PIR) learning paradigm that draws on prior knowledge to instruct adaptive learning of vision and text representations. Based on PIR, a domain-adapted remote sensing image-text retrieval framework PIR-ITR is designed to address semantic noise issues in vision-language understanding tasks. However, with massive additional data for pre-training the vision-language foundation model, remote sensing image-text retrieval is further developed into an open-domain retrieval task. Continuing with the above, we propose PIR-CLIP, a domain-specific CLIP-based framework for remote sensing image-text retrieval, to address semantic noise in remote sensing vision-language representations and further improve open-domain retrieval performance. In vision representation, we utilize the prior-guided knowledge of the remote sensing scene recognition by building a belief matrix to select key features for reducing the impact of semantic noise. In text representation, we use the previous time step to cyclically activate the current time step to enhance text representation capability. A cluster-wise Affiliation Loss (AL) is proposed to constrain the inter-classes and to reduce the semantic confusion zones in the common subspace. Comprehensive experiments demonstrate that PIR could enhance vision and text representations and outperform the state-of-the-art methods of closed-domain and open-domain retrieval on two benchmark datasets, RSICD and RSITMD.",
    "Link": "http://arxiv.org/abs/2405.10160v2",
    "PDF Link": "http://arxiv.org/pdf/2405.10160v2"
  },
  {
    "Title": "ERVD: An Efficient and Robust ViT-Based Distillation Framework for\n  Remote Sensing Image Retrieval",
    "Authors": "Le Dong, Qixuan Cao, Lei Pu, Fangfang Wu, Weisheng Dong, Xin Li, Guangming Shi",
    "Published": "2024-12-24T03:44:26Z",
    "Summary": "ERVD: An Efficient and Robust ViT-Based Distillation Framework for Remote Sensing Image Retrieval",
    "Link": "http://arxiv.org/abs/2412.18136v1",
    "PDF Link": "http://arxiv.org/pdf/2412.18136v1"
  },
  {
    "Title": "RS-RAG: Bridging Remote Sensing Imagery and Comprehensive Knowledge with\n  a Multi-Modal Dataset and Retrieval-Augmented Generation Model",
    "Authors": "Congcong Wen, Yiting Lin, Xiaokang Qu, Nan Li, Yong Liao, Hui Lin, Xiang Li",
    "Published": "2025-04-07T12:13:43Z",
    "Summary": "Recent progress in VLMs has demonstrated impressive capabilities across a variety of tasks in the natural image domain. Motivated by these advancements, the remote sensing community has begun to adopt VLMs for remote sensing vision-language tasks, including scene understanding, image captioning, and visual question answering. However, existing remote sensing VLMs typically rely on closed-set scene understanding and focus on generic scene descriptions, yet lack the ability to incorporate external knowledge. This limitation hinders their capacity for semantic reasoning over complex or context-dependent queries that involve domain-specific or world knowledge. To address these challenges, we first introduced a multimodal Remote Sensing World Knowledge (RSWK) dataset, which comprises high-resolution satellite imagery and detailed textual descriptions for 14,141 well-known landmarks from 175 countries, integrating both remote sensing domain knowledge and broader world knowledge. Building upon this dataset, we proposed a novel Remote Sensing Retrieval-Augmented Generation (RS-RAG) framework, which consists of two key components. The Multi-Modal Knowledge Vector Database Construction module encodes remote sensing imagery and associated textual knowledge into a unified vector space. The Knowledge Retrieval and Response Generation module retrieves and re-ranks relevant knowledge based on image and/or text queries, and incorporates the retrieved content into a knowledge-augmented prompt to guide the VLM in producing contextually grounded responses. We validated the effectiveness of our approach on three representative vision-language tasks, including image captioning, image classification, and visual question answering, where RS-RAG significantly outperformed state-of-the-art baselines.",
    "Link": "http://arxiv.org/abs/2504.04988v1",
    "PDF Link": "http://arxiv.org/pdf/2504.04988v1"
  },
  {
    "Title": "Adaptive Deep Pyramid Matching for Remote Sensing Scene Classification",
    "Authors": "Qingshan Liu, Renlong Hang, Huihui Song, Fuping Zhu, Javier Plaza, Antonio Plaza",
    "Published": "2016-11-11T05:17:56Z",
    "Summary": "Convolutional neural networks (CNNs) have attracted increasing attention in the remote sensing community. Most CNNs only take the last fully-connected layers as features for the classification of remotely sensed images, discarding the other convolutional layer features which may also be helpful for classification purposes. In this paper, we propose a new adaptive deep pyramid matching (ADPM) model that takes advantage of the features from all of the convolutional layers for remote sensing image classification. To this end, the optimal fusing weights for different convolutional layers are learned from the data itself. In remotely sensed scenes, the objects of interest exhibit different scales in distinct scenes, and even a single scene may contain objects with different sizes. To address this issue, we select the CNN with spatial pyramid pooling (SPP-net) as the basic deep network, and further construct a multi-scale ADPM model to learn complementary information from multi-scale images. Our experiments have been conducted using two widely used remote sensing image databases, and the results show that the proposed method significantly improves the performance when compared to other state-of-the-art methods.",
    "Link": "http://arxiv.org/abs/1611.03589v1",
    "PDF Link": "http://arxiv.org/pdf/1611.03589v1"
  },
  {
    "Title": "MARTA GANs: Unsupervised Representation Learning for Remote Sensing\n  Image Classification",
    "Authors": "Daoyu Lin, Kun Fu, Yang Wang, Guangluan Xu, Xian Sun",
    "Published": "2016-12-28T13:24:10Z",
    "Summary": "With the development of deep learning, supervised learning has frequently been adopted to classify remotely sensed images using convolutional networks (CNNs). However, due to the limited amount of labeled data available, supervised learning is often difficult to carry out. Therefore, we proposed an unsupervised model called multiple-layer feature-matching generative adversarial networks (MARTA GANs) to learn a representation using only unlabeled data. MARTA GANs consists of both a generative model $G$ and a discriminative model $D$. We treat $D$ as a feature extractor. To fit the complex properties of remote sensing data, we use a fusion layer to merge the mid-level and global features. $G$ can produce numerous images that are similar to the training data; therefore, $D$ can learn better representations of remotely sensed images using the training data provided by $G$. The classification results on two widely used remote sensing image databases show that the proposed method significantly improves the classification performance compared with other state-of-the-art methods.",
    "Link": "http://arxiv.org/abs/1612.08879v3",
    "PDF Link": "http://arxiv.org/pdf/1612.08879v3"
  },
  {
    "Title": "A flux calibration method for remote sensing satellites using stars",
    "Authors": "Chun Xu",
    "Published": "2017-07-07T03:04:22Z",
    "Summary": "Star surveys and model analyses show that many stars have absolute stable fluxes as good as 3% in 0.3-35{\\mu}m wavebands and about 1% in the visible wavebands. The relative flux calibrations between stars are better than 0.2%. Some stars have extremely stable fluxes and can be used as long term flux calibration sources. Stellar brightness is several orders of magnitude lower than most ground objects while the stars do not usually appear in remote sensing cameras, which makes the stars inappropriate for being calibration sources. The calibration method using stars discussed in this paper is through a mini-camera attached to remote sensing satellite. The mini-camera works at similar wavebands as the remote sensing cameras and it can observe the stars and the ground objects alternatively. High signal-to-noise ratio is achieved for the relatively faint stars through longer exposure time. Simultaneous precise cross-calibration is obtained as the mini-camera and remote sensing cameras look at the ground objects at the same time. The fluxes from the stars used as calibration standards are transferred to the remote sensing cameras through this procedure. Analysis shows that a 2% accurate calibration is possible.",
    "Link": "http://arxiv.org/abs/1707.02023v1",
    "PDF Link": "http://arxiv.org/pdf/1707.02023v1"
  },
  {
    "Title": "Quorum sensing and remote synchronization in networks of Kuramoto\n  oscillators: a biological interpretation",
    "Authors": "Vincenzo Fioriti",
    "Published": "2016-04-01T12:17:20Z",
    "Summary": "Non-linear oscillator networks have revealed properties as the remote synchronization and the quorum sensing. The remote synchronization, defined as the synchronization of nodes not directly connected by any sequence of synchronized nodes, was found firstly in networks of amplitude oscillators and recently in bipartite delayed networks of phase oscillators. The quorum sensing, a biological information scheme discovered in cell aggregates, has been investigated in amplitude oscillators coupled by a common medium. Implications of such findings are important in technology and biology. We show both of them in non-bipartite, biologically plausible networks of Kuramoto oscillators. The quorum sensing emerges using the graph edge density, while the remote synchronization is obtained by means of an oscillator acting as a pacemaker. In the remote synchronization two distinct groups of well inter and infra-synchronized nodes, separated by non-synchronized paths, appear clearly. Our biological interpretation is that the remote synchronization, bypassing the normal quorum sensing mechanism, is responsible of the pathological cell proliferation. This approach seems suitable to study the quorum sensing alterations due to genetic mutation or to the environmental action before the actual mass replication begins.",
    "Link": "http://arxiv.org/abs/1604.00994v1",
    "PDF Link": "http://arxiv.org/pdf/1604.00994v1"
  },
  {
    "Title": "Integrating global spatial features in CNN based Hyperspectral/SAR\n  imagery classification",
    "Authors": "Fan Zhang, MinChao Yan, Chen Hu, Jun Ni, Fei Ma",
    "Published": "2020-05-30T10:00:10Z",
    "Summary": "The land cover classification has played an important role in remote sensing because it can intelligently identify things in one huge remote sensing image to reduce the work of humans. However, a lot of classification methods are designed based on the pixel feature or limited spatial feature of the remote sensing image, which limits the classification accuracy and universality of their methods. This paper proposed a novel method to take into the information of remote sensing image, i.e., geographic latitude-longitude information. In addition, a dual-branch convolutional neural network (CNN) classification method is designed in combination with the global information to mine the pixel features of the image. Then, the features of the two neural networks are fused with another fully neural network to realize the classification of remote sensing images. Finally, two remote sensing images are used to verify the effectiveness of our method, including hyperspectral imaging (HSI) and polarimetric synthetic aperture radar (PolSAR) imagery. The result of the proposed method is superior to the traditional single-channel convolutional neural network.",
    "Link": "http://arxiv.org/abs/2006.00234v2",
    "PDF Link": "http://arxiv.org/pdf/2006.00234v2"
  },
  {
    "Title": "Deep learning in remote sensing: a review",
    "Authors": "Xiao Xiang Zhu, Devis Tuia, Lichao Mou, Gui-Song Xia, Liangpei Zhang, Feng Xu, Friedrich Fraundorfer",
    "Published": "2017-10-11T08:35:05Z",
    "Summary": "Standing at the paradigm shift towards data-intensive science, machine learning techniques are becoming increasingly important. In particular, as a major breakthrough in the field, deep learning has proven as an extremely powerful tool in many fields. Shall we embrace deep learning as the key to all? Or, should we resist a 'black-box' solution? There are controversial opinions in the remote sensing community. In this article, we analyze the challenges of using deep learning for remote sensing data analysis, review the recent advances, and provide resources to make deep learning in remote sensing ridiculously simple to start with. More importantly, we advocate remote sensing scientists to bring their expertise into deep learning, and use it as an implicit general model to tackle unprecedented large-scale influential challenges, such as climate change and urbanization.",
    "Link": "http://arxiv.org/abs/1710.03959v1",
    "PDF Link": "http://arxiv.org/pdf/1710.03959v1"
  },
  {
    "Title": "A Remote Sensing Image Dataset for Cloud Removal",
    "Authors": "Daoyu Lin, Guangluan Xu, Xiaoke Wang, Yang Wang, Xian Sun, Kun Fu",
    "Published": "2019-01-03T03:43:38Z",
    "Summary": "Cloud-based overlays are often present in optical remote sensing images, thus limiting the application of acquired data. Removing clouds is an indispensable pre-processing step in remote sensing image analysis. Deep learning has achieved great success in the field of remote sensing in recent years, including scene classification and change detection. However, deep learning is rarely applied in remote sensing image removal clouds. The reason is the lack of data sets for training neural networks. In order to solve this problem, this paper first proposed the Remote sensing Image Cloud rEmoving dataset (RICE). The proposed dataset consists of two parts: RICE1 contains 500 pairs of images, each pair has images with cloud and cloudless size of 512*512; RICE2 contains 450 sets of images, each set contains three 512*512 size images. , respectively, the reference picture without clouds, the picture of the cloud and the mask of its cloud. The dataset is freely available at \\url{https://github.com/BUPTLdy/RICE_DATASET}.",
    "Link": "http://arxiv.org/abs/1901.00600v1",
    "PDF Link": "http://arxiv.org/pdf/1901.00600v1"
  },
  {
    "Title": "Cloud Removal for Remote Sensing Imagery via Spatial Attention\n  Generative Adversarial Network",
    "Authors": "Heng Pan",
    "Published": "2020-09-28T02:13:23Z",
    "Summary": "Optical remote sensing imagery has been widely used in many fields due to its high resolution and stable geometric properties. However, remote sensing imagery is inevitably affected by climate, especially clouds. Removing the cloud in the high-resolution remote sensing satellite image is an indispensable pre-processing step before analyzing it. For the sake of large-scale training data, neural networks have been successful in many image processing tasks, but the use of neural networks to remove cloud in remote sensing imagery is still relatively small. We adopt generative adversarial network to solve this task and introduce the spatial attention mechanism into the remote sensing imagery cloud removal task, proposes a model named spatial attention generative adversarial network (SpA GAN), which imitates the human visual mechanism, and recognizes and focuses the cloud area with local-to-global spatial attention, thereby enhancing the information recovery of these areas and generating cloudless images with better quality...",
    "Link": "http://arxiv.org/abs/2009.13015v2",
    "PDF Link": "http://arxiv.org/pdf/2009.13015v2"
  },
  {
    "Title": "Self-supervised Learning in Remote Sensing: A Review",
    "Authors": "Yi Wang, Conrad M Albrecht, Nassim Ait Ali Braham, Lichao Mou, Xiao Xiang Zhu",
    "Published": "2022-06-27T11:04:47Z",
    "Summary": "In deep learning research, self-supervised learning (SSL) has received great attention triggering interest within both the computer vision and remote sensing communities. While there has been a big success in computer vision, most of the potential of SSL in the domain of earth observation remains locked. In this paper, we provide an introduction to, and a review of the concepts and latest developments in SSL for computer vision in the context of remote sensing. Further, we provide a preliminary benchmark of modern SSL algorithms on popular remote sensing datasets, verifying the potential of SSL in remote sensing and providing an extended study on data augmentations. Finally, we identify a list of promising directions of future research in SSL for earth observation (SSL4EO) to pave the way for fruitful interaction of both domains.",
    "Link": "http://arxiv.org/abs/2206.13188v2",
    "PDF Link": "http://arxiv.org/pdf/2206.13188v2"
  },
  {
    "Title": "DDIPNet and DDIPNet+: Discriminant Deep Image Prior Networks for Remote\n  Sensing Image Classification",
    "Authors": "Daniel F. S. Santos, Rafael G. Pires, Leandro A. Passos, João P. Papa",
    "Published": "2022-12-20T16:39:04Z",
    "Summary": "Research on remote sensing image classification significantly impacts essential human routine tasks such as urban planning and agriculture. Nowadays, the rapid advance in technology and the availability of many high-quality remote sensing images create a demand for reliable automation methods. The current paper proposes two novel deep learning-based architectures for image classification purposes, i.e., the Discriminant Deep Image Prior Network and the Discriminant Deep Image Prior Network+, which combine Deep Image Prior and Triplet Networks learning strategies. Experiments conducted over three well-known public remote sensing image datasets achieved state-of-the-art results, evidencing the effectiveness of using deep image priors for remote sensing image classification.",
    "Link": "http://arxiv.org/abs/2212.10411v1",
    "PDF Link": "http://arxiv.org/pdf/2212.10411v1"
  },
  {
    "Title": "Reef-insight: A framework for reef habitat mapping with clustering\n  methods via remote sensing",
    "Authors": "Saharsh Barve, Jody M. Webster, Rohitash Chandra",
    "Published": "2023-01-26T00:03:09Z",
    "Summary": "Environmental damage has been of much concern, particularly in coastal areas and the oceans, given climate change and the drastic effects of pollution and extreme climate events. Our present-day analytical capabilities, along with advancements in information acquisition techniques such as remote sensing, can be utilised for the management and study of coral reef ecosystems. In this paper, we present Reef-Insight, an unsupervised machine learning framework that features advanced clustering methods and remote sensing for reef habitat mapping. Our framework compares different clustering methods for reef habitat mapping using remote sensing data. We evaluate four major clustering approaches based on qualitative and visual assessments which include k-means, hierarchical clustering, Gaussian mixture model, and density-based clustering. We utilise remote sensing data featuring the One Tree Island reef in Australia's Southern Great Barrier Reef. Our results indicate that clustering methods using remote sensing data can well identify benthic and geomorphic clusters in reefs when compared with other studies. Our results indicate that Reef-Insight can generate detailed reef habitat maps outlining distinct reef habitats and has the potential to enable further insights for reef restoration projects.",
    "Link": "http://arxiv.org/abs/2301.10876v2",
    "PDF Link": "http://arxiv.org/pdf/2301.10876v2"
  },
  {
    "Title": "VRSBench: A Versatile Vision-Language Benchmark Dataset for Remote\n  Sensing Image Understanding",
    "Authors": "Xiang Li, Jian Ding, Mohamed Elhoseiny",
    "Published": "2024-06-18T08:15:21Z",
    "Summary": "We introduce a new benchmark designed to advance the development of general-purpose, large-scale vision-language models for remote sensing images. Although several vision-language datasets in remote sensing have been proposed to pursue this goal, existing datasets are typically tailored to single tasks, lack detailed object information, or suffer from inadequate quality control. Exploring these improvement opportunities, we present a Versatile vision-language Benchmark for Remote Sensing image understanding, termed VRSBench. This benchmark comprises 29,614 images, with 29,614 human-verified detailed captions, 52,472 object references, and 123,221 question-answer pairs. It facilitates the training and evaluation of vision-language models across a broad spectrum of remote sensing image understanding tasks. We further evaluated state-of-the-art models on this benchmark for three vision-language tasks: image captioning, visual grounding, and visual question answering. Our work aims to significantly contribute to the development of advanced vision-language models in the field of remote sensing. The data and code can be accessed at https://github.com/lx709/VRSBench.",
    "Link": "http://arxiv.org/abs/2406.12384v2",
    "PDF Link": "http://arxiv.org/pdf/2406.12384v2"
  },
  {
    "Title": "Single-Temporal Supervised Learning for Universal Remote Sensing Change\n  Detection",
    "Authors": "Zhuo Zheng, Yanfei Zhong, Ailong Ma, Liangpei Zhang",
    "Published": "2024-06-22T00:03:21Z",
    "Summary": "Bitemporal supervised learning paradigm always dominates remote sensing change detection using numerous labeled bitemporal image pairs, especially for high spatial resolution (HSR) remote sensing imagery. However, it is very expensive and labor-intensive to label change regions in large-scale bitemporal HSR remote sensing image pairs. In this paper, we propose single-temporal supervised learning (STAR) for universal remote sensing change detection from a new perspective of exploiting changes between unpaired images as supervisory signals. STAR enables us to train a high-accuracy change detector only using unpaired labeled images and can generalize to real-world bitemporal image pairs. To demonstrate the flexibility and scalability of STAR, we design a simple yet unified change detector, termed ChangeStar2, capable of addressing binary change detection, object change detection, and semantic change detection in one architecture. ChangeStar2 achieves state-of-the-art performances on eight public remote sensing change detection datasets, covering above two supervised settings, multiple change types, multiple scenarios. The code is available at https://github.com/Z-Zheng/pytorch-change-models.",
    "Link": "http://arxiv.org/abs/2406.15694v1",
    "PDF Link": "http://arxiv.org/pdf/2406.15694v1"
  },
  {
    "Title": "DDFAV: Remote Sensing Large Vision Language Models Dataset and\n  Evaluation Benchmark",
    "Authors": "Haodong Li, Haicheng Qu, Xiaofeng Zhang",
    "Published": "2024-11-05T02:03:12Z",
    "Summary": "With the rapid development of large vision language models (LVLMs), these models have shown excellent results in various multimodal tasks. Since LVLMs are prone to hallucinations and there are currently few datasets and evaluation methods specifically designed for remote sensing, their performance is typically poor when applied to remote sensing tasks. To address these issues, this paper introduces a high quality remote sensing LVLMs dataset, DDFAV, created using data augmentation and data mixing strategies. Next, a training instruction set is produced based on some high-quality remote sensing images selected from the proposed dataset. Finally, we develop a remote sensing LVLMs hallucination evaluation method RSPOPE based on the proposed dataset and evaluate the zero-shot capabilities of different LVLMs. Our proposed dataset, instruction set, and evaluation method files are available at https://github.com/HaodongLi2024/rspope.",
    "Link": "http://arxiv.org/abs/2411.02733v1",
    "PDF Link": "http://arxiv.org/pdf/2411.02733v1"
  },
  {
    "Title": "RemoteTrimmer: Adaptive Structural Pruning for Remote Sensing Image\n  Classification",
    "Authors": "Guangwenjie Zou, Liang Yao, Fan Liu, Chuanyi Zhang, Xin Li, Ning Chen, Shengxiang Xu, Jun Zhou",
    "Published": "2024-12-17T07:00:07Z",
    "Summary": "Since high resolution remote sensing image classification often requires a relatively high computation complexity, lightweight models tend to be practical and efficient. Model pruning is an effective method for model compression. However, existing methods rarely take into account the specificity of remote sensing images, resulting in significant accuracy loss after pruning. To this end, we propose an effective structural pruning approach for remote sensing image classification. Specifically, a pruning strategy that amplifies the differences in channel importance of the model is introduced. Then an adaptive mining loss function is designed for the fine-tuning process of the pruned model. Finally, we conducted experiments on two remote sensing classification datasets. The experimental results demonstrate that our method achieves minimal accuracy loss after compressing remote sensing classification models, achieving state-of-the-art (SoTA) performance.",
    "Link": "http://arxiv.org/abs/2412.12603v2",
    "PDF Link": "http://arxiv.org/pdf/2412.12603v2"
  },
  {
    "Title": "A Decade of Deep Learning for Remote Sensing Spatiotemporal Fusion:\n  Advances, Challenges, and Opportunities",
    "Authors": "Enzhe Sun, Yongchuan Cui, Peng Liu, Jining Yan",
    "Published": "2025-04-01T15:30:48Z",
    "Summary": "Hardware limitations and satellite launch costs make direct acquisition of high temporal-spatial resolution remote sensing imagery challenging. Remote sensing spatiotemporal fusion (STF) technology addresses this problem by merging high temporal but low spatial resolution imagery with high spatial but low temporal resolution imagery to efficiently generate high spatiotemporal resolution satellite images. STF provides unprecedented observational capabilities for land surface change monitoring, agricultural management, and environmental research. Deep learning (DL) methods have revolutionized the remote sensing spatiotemporal fusion field over the past decade through powerful automatic feature extraction and nonlinear modeling capabilities, significantly outperforming traditional methods in handling complex spatiotemporal data. Despite the rapid development of DL-based remote sensing STF, the community lacks a systematic review of this quickly evolving field. This paper comprehensively reviews DL developments in remote sensing STF over the last decade, analyzing key research trends, method classifications, commonly used datasets, and evaluation metrics. It discusses major challenges in existing research and identifies promising future research directions as references for researchers in this field to inspire new ideas. The specific models, datasets, and other information mentioned in this article have been collected in: https://github.com/yc-cui/Deep-Learning-Spatiotemporal-Fusion-Survey.",
    "Link": "http://arxiv.org/abs/2504.00901v1",
    "PDF Link": "http://arxiv.org/pdf/2504.00901v1"
  },
  {
    "Title": "A Review of Geospatial Content in IEEE Visualization Publications",
    "Authors": "Alexander Yoshizumi, Megan M. Coffer, Elyssa L. Collins, Mollie D. Gaines, Xiaojie Gao, Kate Jones, Ian R. McGregor, Katie A. McQuillan, Vinicius Perin, Laura M. Tomkins, Thom Worm, Laura Tateosian",
    "Published": "2020-09-07T19:42:56Z",
    "Summary": "Geospatial analysis is crucial for addressing many of the world's most pressing challenges. Given this, there is immense value in improving and expanding the visualization techniques used to communicate geospatial data. In this work, we explore this important intersection -- between geospatial analytics and visualization -- by examining a set of recent IEEE VIS Conference papers (a selection from 2017-2019) to assess the inclusion of geospatial data and geospatial analyses within these papers. After removing the papers with no geospatial data, we organize the remaining literature into geospatial data domain categories and provide insight into how these categories relate to VIS Conference paper types. We also contextualize our results by investigating the use of geospatial terms in IEEE Visualization publications over the last 30 years. Our work provides an understanding of the quantity and role of geospatial subject matter in recent IEEE VIS publications and supplies a foundation for future meta-analytical work around geospatial analytics and geovisualization that may shed light on opportunities for innovation.",
    "Link": "http://arxiv.org/abs/2009.03390v1",
    "PDF Link": "http://arxiv.org/pdf/2009.03390v1"
  },
  {
    "Title": "An LLM Agent for Automatic Geospatial Data Analysis",
    "Authors": "Yuxing Chen, Weijie Wang, Sylvain Lobry, Camille Kurtz",
    "Published": "2024-10-24T14:47:25Z",
    "Summary": "Large language models (LLMs) are being used in data science code generation tasks, but they often struggle with complex sequential tasks, leading to logical errors. Their application to geospatial data processing is particularly challenging due to difficulties in incorporating complex data structures and spatial constraints, effectively utilizing diverse function calls, and the tendency to hallucinate less-used geospatial libraries. To tackle these problems, we introduce GeoAgent, a new interactive framework designed to help LLMs handle geospatial data processing more effectively. GeoAgent pioneers the integration of a code interpreter, static analysis, and Retrieval-Augmented Generation (RAG) techniques within a Monte Carlo Tree Search (MCTS) algorithm, offering a novel approach to geospatial data processing. In addition, we contribute a new benchmark specifically designed to evaluate the LLM-based approach in geospatial tasks. This benchmark leverages a variety of Python libraries and includes both single-turn and multi-turn tasks such as data acquisition, data analysis, and visualization. By offering a comprehensive evaluation among diverse geospatial contexts, this benchmark sets a new standard for developing LLM-based approaches in geospatial data analysis tasks. Our findings suggest that relying solely on knowledge of LLM is insufficient for accurate geospatial task programming, which requires coherent multi-step processes and multiple function calls. Compared to the baseline LLMs, the proposed GeoAgent has demonstrated superior performance, yielding notable improvements in function calls and task completion. In addition, these results offer valuable insights for the future development of LLM agents in automatic geospatial data analysis task programming.",
    "Link": "http://arxiv.org/abs/2410.18792v2",
    "PDF Link": "http://arxiv.org/pdf/2410.18792v2"
  },
  {
    "Title": "Geospatial Knowledge Graphs",
    "Authors": "Rui Zhu",
    "Published": "2024-05-13T11:45:22Z",
    "Summary": "Geospatial knowledge graphs have emerged as a novel paradigm for representing and reasoning over geospatial information. In this framework, entities such as places, people, events, and observations are depicted as nodes, while their relationships are represented as edges. This graph-based data format lays the foundation for creating a \"FAIR\" (Findable, Accessible, Interoperable, and Reusable) environment, facilitating the management and analysis of geographic information. This entry first introduces key concepts in knowledge graphs along with their associated standardization and tools. It then delves into the application of knowledge graphs in geography and environmental sciences, emphasizing their role in bridging symbolic and subsymbolic GeoAI to address cross-disciplinary geospatial challenges. At the end, new research directions related to geospatial knowledge graphs are outlined.",
    "Link": "http://arxiv.org/abs/2405.07664v1",
    "PDF Link": "http://arxiv.org/pdf/2405.07664v1"
  },
  {
    "Title": "Self-supervised Learning for Geospatial AI: A Survey",
    "Authors": "Yile Chen, Weiming Huang, Kaiqi Zhao, Yue Jiang, Gao Cong",
    "Published": "2024-08-22T05:28:22Z",
    "Summary": "The proliferation of geospatial data in urban and territorial environments has significantly facilitated the development of geospatial artificial intelligence (GeoAI) across various urban applications. Given the vast yet inherently sparse labeled nature of geospatial data, there is a critical need for techniques that can effectively leverage such data without heavy reliance on labeled datasets. This requirement aligns with the principles of self-supervised learning (SSL), which has attracted increasing attention for its adoption in geospatial data. This paper conducts a comprehensive and up-to-date survey of SSL techniques applied to or developed for three primary data (geometric) types prevalent in geospatial vector data: points, polylines, and polygons. We systematically categorize various SSL techniques into predictive and contrastive methods, discussing their application with respect to each data type in enhancing generalization across various downstream tasks. Furthermore, we review the emerging trends of SSL for GeoAI, and several task-specific SSL techniques. Finally, we discuss several key challenges in the current research and outline promising directions for future investigation. By presenting a structured analysis of relevant studies, this paper aims to inspire continued advancements in the integration of SSL with GeoAI, encouraging innovative methods to harnessing the power of geospatial data.",
    "Link": "http://arxiv.org/abs/2408.12133v1",
    "PDF Link": "http://arxiv.org/pdf/2408.12133v1"
  },
  {
    "Title": "Geo-OLM: Enabling Sustainable Earth Observation Studies with\n  Cost-Efficient Open Language Models & State-Driven Workflows",
    "Authors": "Dimitrios Stamoulis, Diana Marculescu",
    "Published": "2025-04-06T01:31:04Z",
    "Summary": "Geospatial Copilots hold immense potential for automating Earth observation (EO) and climate monitoring workflows, yet their reliance on large-scale models such as GPT-4o introduces a paradox: tools intended for sustainability studies often incur unsustainable costs. Using agentic AI frameworks in geospatial applications can amass thousands of dollars in API charges or requires expensive, power-intensive GPUs for deployment, creating barriers for researchers, policymakers, and NGOs. Unfortunately, when geospatial Copilots are deployed with open language models (OLMs), performance often degrades due to their dependence on GPT-optimized logic. In this paper, we present Geo-OLM, a tool-augmented geospatial agent that leverages the novel paradigm of state-driven LLM reasoning to decouple task progression from tool calling. By alleviating the workflow reasoning burden, our approach enables low-resource OLMs to complete geospatial tasks more effectively. When downsizing to small models below 7B parameters, Geo-OLM outperforms the strongest prior geospatial baselines by 32.8% in successful query completion rates. Our method performs comparably to proprietary models achieving results within 10% of GPT-4o, while reducing inference costs by two orders of magnitude from \\$500-\\$1000 to under \\$10. We present an in-depth analysis with geospatial downstream benchmarks, providing key insights to help practitioners effectively deploy OLMs for EO applications.",
    "Link": "http://arxiv.org/abs/2504.04319v1",
    "PDF Link": "http://arxiv.org/pdf/2504.04319v1"
  },
  {
    "Title": "DeepSPACE: Approximate Geospatial Query Processing with Deep Learning",
    "Authors": "Dimitri Vorona, Andreas Kipf, Thomas Neumann, Alfons Kemper",
    "Published": "2019-06-14T09:16:16Z",
    "Summary": "The amount of the available geospatial data grows at an ever faster pace. This leads to the constantly increasing demand for processing power and storage in order to provide data analysis in a timely manner. At the same time, a lot of geospatial processing is visual and exploratory in nature, thus having bounded precision requirements. We present DeepSPACE, a deep learning-based approximate geospatial query processing engine which combines modest hardware requirements with the ability to answer flexible aggregation queries while keeping the required state to a few hundred KiBs.",
    "Link": "http://arxiv.org/abs/1906.06085v1",
    "PDF Link": "http://arxiv.org/pdf/1906.06085v1"
  },
  {
    "Title": "Chain-of-Programming (CoP) : Empowering Large Language Models for\n  Geospatial Code Generation",
    "Authors": "Shuyang Hou, Haoyue Jiao, Zhangxiao Shen, Jianyuan Liang, Anqi Zhao, Xiaopu Zhang, Jianxun Wang, Huayi Wu",
    "Published": "2024-11-16T09:20:35Z",
    "Summary": "With the rapid growth of interdisciplinary demands for geospatial modeling and the rise of large language models (LLMs), geospatial code generation technology has seen significant advancements. However, existing LLMs often face challenges in the geospatial code generation process due to incomplete or unclear user requirements and insufficient knowledge of specific platform syntax rules, leading to the generation of non-executable code, a phenomenon known as \"code hallucination.\" To address this issue, this paper proposes a Chain of Programming (CoP) framework, which decomposes the code generation process into five steps: requirement analysis, algorithm design, code implementation, code debugging, and code annotation. The framework incorporates a shared information pool, knowledge base retrieval, and user feedback mechanisms, forming an end-to-end code generation flow from requirements to code without the need for model fine-tuning. Based on a geospatial problem classification framework and evaluation benchmarks, the CoP strategy significantly improves the logical clarity, syntactical correctness, and executability of the generated code, with improvements ranging from 3.0% to 48.8%. Comparative and ablation experiments further validate the superiority of the CoP strategy over other optimization approaches and confirm the rationality and necessity of its key components. Through case studies on building data visualization and fire data analysis, this paper demonstrates the application and effectiveness of CoP in various geospatial scenarios. The CoP framework offers a systematic, step-by-step approach to LLM-based geospatial code generation tasks, significantly enhancing code generation performance in geospatial tasks and providing valuable insights for code generation in other vertical domains.",
    "Link": "http://arxiv.org/abs/2411.10753v1",
    "PDF Link": "http://arxiv.org/pdf/2411.10753v1"
  },
  {
    "Title": "Geospatial Big Data Handling Theory and Methods: A Review and Research\n  Challenges",
    "Authors": "S. Li, S. Dragicevic, F. Anton, M. Sester, S. Winter, A. Coltekin, C. Pettit, B. Jiang, J. Haworth, A. Stein, T. Cheng",
    "Published": "2015-11-10T08:10:50Z",
    "Summary": "Big data has now become a strong focus of global interest that is increasingly attracting the attention of academia, industry, government and other organizations. Big data can be situated in the disciplinary area of traditional geospatial data handling theory and methods. The increasing volume and varying format of collected geospatial big data presents challenges in storing, managing, processing, analyzing, visualizing and verifying the quality of data. This has implications for the quality of decisions made with big data. Consequently, this position paper of the International Society for Photogrammetry and Remote Sensing (ISPRS) Technical Commission II (TC II) revisits the existing geospatial data handling methods and theories to determine if they are still capable of handling emerging geospatial big data. Further, the paper synthesises problems, major issues and challenges with current developments as well as recommending what needs to be developed further in the near future.   Keywords: Big data, Geospatial, Data handling, Analytics, Spatial Modeling, Review",
    "Link": "http://arxiv.org/abs/1511.03010v1",
    "PDF Link": "http://arxiv.org/pdf/1511.03010v1"
  },
  {
    "Title": "High Performance Computing for Geospatial Applications: A Retrospective\n  View",
    "Authors": "Marc P. Armstrong",
    "Published": "2019-12-11T19:28:17Z",
    "Summary": "Many types of geospatial analyses are computationally complex, involving, for example, solution processes that require numerous iterations or combinatorial comparisons. This complexity has motivated the application of high performance computing (HPC) to a variety of geospatial problems. In many instances, HPC assumes even greater importance because complexity interacts with rapidly growing volumes of geospatial information to further impede analysis and display. This chapter briefly reviews the underlying need for HPC in geospatial applications and describes different approaches to past implementations. Many of these applications were developed using hardware systems that had a relatively short life-span and were implemented in software that was not easily portable. More promising recent approaches have turned to the use of distributed resources that includes cyberinfrastructure as well as cloud and fog computing.",
    "Link": "http://arxiv.org/abs/1912.06548v1",
    "PDF Link": "http://arxiv.org/pdf/1912.06548v1"
  },
  {
    "Title": "Bridging Remote Sensors with Multisensor Geospatial Foundation Models",
    "Authors": "Boran Han, Shuai Zhang, Xingjian Shi, Markus Reichstein",
    "Published": "2024-04-01T17:30:56Z",
    "Summary": "In the realm of geospatial analysis, the diversity of remote sensors, encompassing both optical and microwave technologies, offers a wealth of distinct observational capabilities. Recognizing this, we present msGFM, a multisensor geospatial foundation model that effectively unifies data from four key sensor modalities. This integration spans an expansive dataset of two million multisensor images. msGFM is uniquely adept at handling both paired and unpaired sensor data. For data originating from identical geolocations, our model employs an innovative cross-sensor pretraining approach in masked image modeling, enabling the synthesis of joint representations from diverse sensors. msGFM, incorporating four remote sensors, upholds strong performance, forming a comprehensive model adaptable to various sensor types. msGFM has demonstrated enhanced proficiency in a range of both single-sensor and multisensor downstream tasks. These include scene classification, segmentation, cloud removal, and pan-sharpening. A key discovery of our research is that representations derived from natural images are not always compatible with the distinct characteristics of geospatial remote sensors, underscoring the limitations of existing representations in this field. Our work can serve as a guide for developing multisensor geospatial pretraining models, paving the way for more advanced geospatial capabilities.",
    "Link": "http://arxiv.org/abs/2404.01260v1",
    "PDF Link": "http://arxiv.org/pdf/2404.01260v1"
  },
  {
    "Title": "Geospatial Analysis and Internet of Things in Environmental Informatics",
    "Authors": "Andreas Kamilaris, Frank Ostermann",
    "Published": "2018-08-01T10:46:24Z",
    "Summary": "Geospatial analysis offers large potential for better understanding, modelling and visualizing our natural and artificial ecosystems, using Internet of Things as a pervasive sensing infrastructure. This paper performs a review of research work based on the IoT, in which geospatial analysis has been employed in environmental informatics. Six different geospatial analysis methods have been identified, presented together with 26 relevant IoT initiatives adopting some of these techniques. Analysis is performed in relation to the type of IoT devices used, their deployment status and data transmission standards, data types employed, and reliability of measurements. This paper scratches the surface of this combination of technologies and techniques, providing indications of how IoT, together with geospatial analysis, are currently being used in the domain of environmental research.",
    "Link": "http://arxiv.org/abs/1808.01895v1",
    "PDF Link": "http://arxiv.org/pdf/1808.01895v1"
  },
  {
    "Title": "Geospatial foundation models for image analysis: evaluating and\n  enhancing NASA-IBM Prithvi's domain adaptability",
    "Authors": "Chia-Yu Hsu, Wenwen Li, Sizhe Wang",
    "Published": "2024-08-31T15:51:23Z",
    "Summary": "Research on geospatial foundation models (GFMs) has become a trending topic in geospatial artificial intelligence (AI) research due to their potential for achieving high generalizability and domain adaptability, reducing model training costs for individual researchers. Unlike large language models, such as ChatGPT, constructing visual foundation models for image analysis, particularly in remote sensing, encountered significant challenges such as formulating diverse vision tasks into a general problem framework. This paper evaluates the recently released NASA-IBM GFM Prithvi for its predictive performance on high-level image analysis tasks across multiple benchmark datasets. Prithvi was selected because it is one of the first open-source GFMs trained on time-series of high-resolution remote sensing imagery. A series of experiments were designed to assess Prithvi's performance as compared to other pre-trained task-specific AI models in geospatial image analysis. New strategies, including band adaptation, multi-scale feature generation, and fine-tuning techniques, are introduced and integrated into an image analysis pipeline to enhance Prithvi's domain adaptation capability and improve model performance. In-depth analyses reveal Prithvi's strengths and weaknesses, offering insights for both improving Prithvi and developing future visual foundation models for geospatial tasks.",
    "Link": "http://arxiv.org/abs/2409.00489v1",
    "PDF Link": "http://arxiv.org/pdf/2409.00489v1"
  },
  {
    "Title": "Geospatial Narratives and their Spatio-Temporal Dynamics: Commonsense\n  Reasoning for High-level Analyses in Geographic Information Systems",
    "Authors": "Mehul Bhatt, Jan Oliver Wallgruen",
    "Published": "2013-07-09T18:54:29Z",
    "Summary": "The modelling, analysis, and visualisation of dynamic geospatial phenomena has been identified as a key developmental challenge for next-generation Geographic Information Systems (GIS). In this context, the envisaged paradigmatic extensions to contemporary foundational GIS technology raises fundamental questions concerning the ontological, formal representational, and (analytical) computational methods that would underlie their spatial information theoretic underpinnings.   We present the conceptual overview and architecture for the development of high-level semantic and qualitative analytical capabilities for dynamic geospatial domains. Building on formal methods in the areas of commonsense reasoning, qualitative reasoning, spatial and temporal representation and reasoning, reasoning about actions and change, and computational models of narrative, we identify concrete theoretical and practical challenges that accrue in the context of formal reasoning about `space, events, actions, and change'. With this as a basis, and within the backdrop of an illustrated scenario involving the spatio-temporal dynamics of urban narratives, we address specific problems and solutions techniques chiefly involving `qualitative abstraction', `data integration and spatial consistency', and `practical geospatial abduction'. From a broad topical viewpoint, we propose that next-generation dynamic GIS technology demands a transdisciplinary scientific perspective that brings together Geography, Artificial Intelligence, and Cognitive Science.   Keywords: artificial intelligence; cognitive systems; human-computer interaction; geographic information systems; spatio-temporal dynamics; computational models of narrative; geospatial analysis; geospatial modelling; ontology; qualitative spatial modelling and reasoning; spatial assistance systems",
    "Link": "http://arxiv.org/abs/1307.2541v2",
    "PDF Link": "http://arxiv.org/pdf/1307.2541v2"
  },
  {
    "Title": "GEOBench-VLM: Benchmarking Vision-Language Models for Geospatial Tasks",
    "Authors": "Muhammad Sohail Danish, Muhammad Akhtar Munir, Syed Roshaan Ali Shah, Kartik Kuckreja, Fahad Shahbaz Khan, Paolo Fraccaro, Alexandre Lacoste, Salman Khan",
    "Published": "2024-11-28T18:59:56Z",
    "Summary": "While numerous recent benchmarks focus on evaluating generic Vision-Language Models (VLMs), they do not effectively address the specific challenges of geospatial applications. Generic VLM benchmarks are not designed to handle the complexities of geospatial data, an essential component for applications such as environmental monitoring, urban planning, and disaster management. Key challenges in the geospatial domain include temporal change detection, large-scale object counting, tiny object detection, and understanding relationships between entities in remote sensing imagery. To bridge this gap, we present GEOBench-VLM, a comprehensive benchmark specifically designed to evaluate VLMs on geospatial tasks, including scene understanding, object counting, localization, fine-grained categorization, segmentation, and temporal analysis. Our benchmark features over 10,000 manually verified instructions and spanning diverse visual conditions, object types, and scales. We evaluate several state-of-the-art VLMs to assess performance on geospatial-specific challenges. The results indicate that although existing VLMs demonstrate potential, they face challenges when dealing with geospatial-specific tasks, highlighting the room for further improvements. Notably, the best-performing LLaVa-OneVision achieves only 41.7% accuracy on MCQs, slightly more than GPT-4o, which is approximately double the random guess performance. Our benchmark is publicly available at https://github.com/The-AI-Alliance/GEO-Bench-VLM .",
    "Link": "http://arxiv.org/abs/2411.19325v2",
    "PDF Link": "http://arxiv.org/pdf/2411.19325v2"
  },
  {
    "Title": "Deep Learning Techniques for Geospatial Data Analysis",
    "Authors": "Arvind W. Kiwelekar, Geetanjali S. Mahamunkar, Laxman D. Netak, Valmik B Nikam",
    "Published": "2020-08-30T11:51:10Z",
    "Summary": "Consumer electronic devices such as mobile handsets, goods tagged with RFID labels, location and position sensors are continuously generating a vast amount of location enriched data called geospatial data. Conventionally such geospatial data is used for military applications. In recent times, many useful civilian applications have been designed and deployed around such geospatial data. For example, a recommendation system to suggest restaurants or places of attraction to a tourist visiting a particular locality. At the same time, civic bodies are harnessing geospatial data generated through remote sensing devices to provide better services to citizens such as traffic monitoring, pothole identification, and weather reporting. Typically such applications are leveraged upon non-hierarchical machine learning techniques such as Naive-Bayes Classifiers, Support Vector Machines, and decision trees. Recent advances in the field of deep-learning showed that Neural Network-based techniques outperform conventional techniques and provide effective solutions for many geospatial data analysis tasks such as object recognition, image classification, and scene understanding. The chapter presents a survey on the current state of the applications of deep learning techniques for analyzing geospatial data.   The chapter is organized as below: (i) A brief overview of deep learning algorithms. (ii)Geospatial Analysis: a Data Science Perspective (iii) Deep-learning techniques for Remote Sensing data analytics tasks (iv) Deep-learning techniques for GPS data analytics(iv) Deep-learning techniques for RFID data analytics.",
    "Link": "http://arxiv.org/abs/2008.13146v1",
    "PDF Link": "http://arxiv.org/pdf/2008.13146v1"
  },
  {
    "Title": "Comparative Analysis of SpatialHadoop and GeoSpark for Geospatial Big\n  Data Analytics",
    "Authors": "Rakesh K. Lenka, Rabindra K. Barik, Noopur Gupta, Syed Mohd Ali, Amiya Rath, Harishchandra Dubey",
    "Published": "2016-12-22T03:46:11Z",
    "Summary": "In this digitalised world where every information is stored, the data a are growing exponentially. It is estimated that data are doubles itself every two years. Geospatial data are one of the prime contributors to the big data scenario. There are numerous tools of the big data analytics. But not all the big data analytics tools are capabilities to handle geospatial big data. In the present paper, it has been discussed about the recent two popular open source geospatial big data analytical tools i.e. Spatial- Hadoop and GeoSpark which can be used for analysis and process the geospatial big data in efficient manner. It has compared the architectural view of SpatialHadoop and GeoSpark. Through the architectural comparison, it has also summarised the merits and demerits of these tools according the execution times and volume of the data which has been used.",
    "Link": "http://arxiv.org/abs/1612.07433v2",
    "PDF Link": "http://arxiv.org/pdf/1612.07433v2"
  },
  {
    "Title": "Graph Theory Applications in Advanced Geospatial Research",
    "Authors": "Surajit Ghosh, Archita Mallick, Anuva Chowdhury, Kounik De Sarkar",
    "Published": "2023-09-06T15:47:18Z",
    "Summary": "Geospatial sciences include a wide range of applications, from environmental monitoring transportation to infrastructure planning, as well as location-based analysis and services. Graph theory algorithms in mathematics have emerged as indispensable tools in these domains due to their capability to model and analyse spatial relationships efficiently. This article explores the applications of graph theory algorithms in geospatial sciences, highlighting their role in network analysis, spatial connectivity, geographic information systems, and various other spatial problem-solving scenarios like digital twin. The article provides a comprehensive idea about graph theory's key concepts and algorithms that assist the geospatial modelling processes and insights into real-world geospatial challenges and opportunities. It lists the extensive research, innovative technologies and methodologies implemented in this domain.",
    "Link": "http://arxiv.org/abs/2309.03249v2",
    "PDF Link": "http://arxiv.org/pdf/2309.03249v2"
  },
  {
    "Title": "Identification of Anomalous Geospatial Trajectories via Persistent\n  Homology",
    "Authors": "Kyle Evans-Lee, Kevin Lamb",
    "Published": "2024-10-04T19:46:08Z",
    "Summary": "We present a novel method for analyzing geospatial trajectory data using topological data analysis (TDA) to identify a specific class of anomalies, commonly referred to as crop circles, in AIS data. This approach is the first of its kind to be applied to spatiotemporal data. By embedding $2+1$-dimensional spatiotemporal data into $\\mathbb{R}^3$, we utilize persistent homology to detect loops within the trajectories in $\\mathbb{R}^2$. Our research reveals that, under normal conditions, trajectory data embedded in $\\mathbb{R}^3$ over time do not form loops. Consequently, we can effectively identify anomalies characterized by the presence of loops within the trajectories. This method is robust and capable of detecting loops that are invariant to small perturbations, variations in geometric shape, and local coordinate projections. Additionally, our approach provides a novel perspective on anomaly detection, offering enhanced sensitivity and specificity in identifying atypical patterns in geospatial data. This approach has significant implications for various applications, including maritime navigation, environmental monitoring, and surveillance.",
    "Link": "http://arxiv.org/abs/2410.03889v1",
    "PDF Link": "http://arxiv.org/pdf/2410.03889v1"
  },
  {
    "Title": "Interoperability in Planetary Research for Geospatial Data Analysis",
    "Authors": "Trent M. Hare, Angelo P. Rossi, Alessandro Frigeri, Chiara Marmo",
    "Published": "2017-06-08T17:00:03Z",
    "Summary": "For more than a decade there has been a push in the planetary science community to support interoperable methods for accessing and working with geospatial data. Common geospatial data products for planetary research include image mosaics, digital elevation or terrain models, geologic maps, geographic location databases (e.g., craters, volcanoes) or any data that can be tied to the surface of a planetary body (including moons, comets or asteroids). Several U.S. and international cartographic research institutions have converged on mapping standards that embrace standardized geospatial image formats, geologic mapping conventions, U.S. Federal Geographic Data Committee (FGDC) cartographic and metadata standards, and notably on-line mapping services as defined by the Open Geospatial Consortium (OGC).",
    "Link": "http://arxiv.org/abs/1706.02683v1",
    "PDF Link": "http://arxiv.org/pdf/1706.02683v1"
  },
  {
    "Title": "Performance Evaluation of Geospatial Images based on Zarr and Tiff",
    "Authors": "Jaheer Khan, Swarup E, Rakshit Ramesh",
    "Published": "2024-11-18T05:34:31Z",
    "Summary": "This evaluate the performance of geospatial image processing using two distinct data storage formats: Zarr and TIFF. Geospatial images, converted to numerous applications like environmental monitoring, urban planning, and disaster management. Traditional Tagged Image File Format is mostly used because it is simple and compatible but may lack by performance limitations while working on large datasets. Zarr is a new format designed for the cloud systems,that offers scalability and efficient storage with data chunking and compression techniques. This study compares the two formats in terms of storage efficiency, access speed, and computational performance during typical geospatial processing tasks. Through analysis on a range of geospatial datasets, this provides details about the practical advantages and limitations of each format,helping users to select the appropriate format based on their specific needs and constraints.",
    "Link": "http://arxiv.org/abs/2411.11291v1",
    "PDF Link": "http://arxiv.org/pdf/2411.11291v1"
  },
  {
    "Title": "Geospatial and Symbolic Hypothesis for the Foundation of Tenochtitlan\n  Based on Digital Elevation Analysis of the Valley of Mexico",
    "Authors": "Jose Alberto Baeza Guerra",
    "Published": "2025-04-03T18:11:55Z",
    "Summary": "This paper proposes a novel hypothesis about the foundation of Tenochtitlan by combining digital elevation modeling with historical and symbolic analysis. Using geospatial data from EarthExplorer, we simulate various historical water levels in the Valley of Mexico. The resulting lake configurations reveal possible locations for ancient settlements near now-vanished shorelines, suggesting a dynamic transformation of sacred geography that aligns with key Mexica myths. We identify Santa Mar\\'ia Aztahuacan as a strong candidate for the historical Aztlan and propose a reinterpretation of foundational codices in light of geomythical correlations.",
    "Link": "http://arxiv.org/abs/2504.03787v1",
    "PDF Link": "http://arxiv.org/pdf/2504.03787v1"
  },
  {
    "Title": "GeoAI in Social Science",
    "Authors": "Wenwen Li",
    "Published": "2023-12-19T20:23:18Z",
    "Summary": "GeoAI, or geospatial artificial intelligence, is an exciting new area that leverages artificial intelligence (AI), geospatial big data, and massive computing power to solve problems with high automation and intelligence. This paper reviews the progress of AI in social science research, highlighting important advancements in using GeoAI to fill critical data and knowledge gaps. It also discusses the importance of breaking down data silos, accelerating convergence among GeoAI research methods, as well as moving GeoAI beyond geospatial benefits.",
    "Link": "http://arxiv.org/abs/2401.05398v1",
    "PDF Link": "http://arxiv.org/pdf/2401.05398v1"
  },
  {
    "Title": "FogGIS: Fog Computing for Geospatial Big Data Analytics",
    "Authors": "Rabindra K. Barik, Harishchandra Dubey, Arun B. Samaddar, Rajan D. Gupta, Prakash K. Ray",
    "Published": "2016-12-10T12:59:54Z",
    "Summary": "Cloud Geographic Information Systems (GIS) has emerged as a tool for analysis, processing and transmission of geospatial data. The Fog computing is a paradigm where Fog devices help to increase throughput and reduce latency at the edge of the client. This paper developed a Fog-based framework named Fog GIS for mining analytics from geospatial data. We built a prototype using Intel Edison, an embedded microprocessor. We validated the FogGIS by doing preliminary analysis. including compression, and overlay analysis. Results showed that Fog computing hold a great promise for analysis of geospatial data. We used several open source compression techniques for reducing the transmission to the cloud.",
    "Link": "http://arxiv.org/abs/1701.02601v1",
    "PDF Link": "http://arxiv.org/pdf/1701.02601v1"
  },
  {
    "Title": "Evaluating Effectiveness of Interactivity in Contour-based Geospatial\n  Visualizations",
    "Authors": "Abdullah-Al-Raihan Nayeem, Dongyun Han, William J. Tolone, Isaac Cho",
    "Published": "2024-10-13T22:25:39Z",
    "Summary": "Contour maps are an essential tool for exploring spatial features of the terrain, such as distance, directions, and surface gradient among the contour areas. User interactions in contour-based visualizations create approaches to visual analysis that are noticeably different from the perspective of human cognition. As such, various interactive approaches have been introduced to improve system usability and enhance human cognition for complex and large-scale spatial data exploration. However, what user interaction means for contour maps, its purpose, when to leverage, and design primitives have yet to be investigated in the context of analysis tasks. Therefore, further research is needed to better understand and quantify the potentials and benefits offered by user interactions in contour-based geospatial visualizations designed to support analytical tasks. In this paper, we present a contour-based interactive geospatial visualization designed for analytical tasks. We conducted a crowd-sourced user study (N=62) to examine the impact of interactive features on analysis using contour-based geospatial visualizations. Our results show that the interactive features aid in their data analysis and understanding in terms of spatial data extent, map layout, task complexity, and user expertise. Finally, we discuss our findings in-depth, which will serve as guidelines for future design and implementation of interactive features in support of case-specific analytical tasks on contour-based geospatial views.",
    "Link": "http://arxiv.org/abs/2410.10032v1",
    "PDF Link": "http://arxiv.org/pdf/2410.10032v1"
  },
  {
    "Title": "Assessment of a new GeoAI foundation model for flood inundation mapping",
    "Authors": "Wenwen Li, Hyunho Lee, Sizhe Wang, Chia-Yu Hsu, Samantha T. Arundel",
    "Published": "2023-09-25T19:50:47Z",
    "Summary": "Vision foundation models are a new frontier in Geospatial Artificial Intelligence (GeoAI), an interdisciplinary research area that applies and extends AI for geospatial problem solving and geographic knowledge discovery, because of their potential to enable powerful image analysis by learning and extracting important image features from vast amounts of geospatial data. This paper evaluates the performance of the first-of-its-kind geospatial foundation model, IBM-NASA's Prithvi, to support a crucial geospatial analysis task: flood inundation mapping. This model is compared with convolutional neural network and vision transformer-based architectures in terms of mapping accuracy for flooded areas. A benchmark dataset, Sen1Floods11, is used in the experiments, and the models' predictability, generalizability, and transferability are evaluated based on both a test dataset and a dataset that is completely unseen by the model. Results show the good transferability of the Prithvi model, highlighting its performance advantages in segmenting flooded areas in previously unseen regions. The findings also indicate areas for improvement for the Prithvi model in terms of adopting multi-scale representation learning, developing more end-to-end pipelines for high-level image analysis tasks, and offering more flexibility in terms of input data bands.",
    "Link": "http://arxiv.org/abs/2309.14500v4",
    "PDF Link": "http://arxiv.org/pdf/2309.14500v4"
  },
  {
    "Title": "Detecting Spatial Patterns of Disease in Large Collections of Electronic\n  Medical Records Using Neighbor-Based Bootstrapping (NB2)",
    "Authors": "Maria T Patterson, Robert L Grossman",
    "Published": "2017-03-06T00:14:36Z",
    "Summary": "We introduce a method called neighbor-based bootstrapping (NB2) that can be used to quantify the geospatial variation of a variable. We applied this method to an analysis of the incidence rates of disease from electronic medical record data (ICD-9 codes) for approximately 100 million individuals in the US over a period of 8 years. We considered the incidence rate of disease in each county and its geospatially contiguous neighbors and rank ordered diseases in terms of their degree of geospatial variation as quantified by the NB2 method.   We show that this method yields results in good agreement with established methods for detecting spatial autocorrelation (Moran's I method and kriging). Moreover, the NB2 method can be tuned to identify both large area and small area geospatial variations. This method also applies more generally in any parameter space that can be partitioned to consist of regions and their neighbors.",
    "Link": "http://arxiv.org/abs/1703.01692v1",
    "PDF Link": "http://arxiv.org/pdf/1703.01692v1"
  },
  {
    "Title": "GFM4MPM: Towards Geospatial Foundation Models for Mineral Prospectivity\n  Mapping",
    "Authors": "Angel Daruna, Vasily Zadorozhnyy, Georgina Lukoczki, Han-Pang Chiu",
    "Published": "2024-06-18T16:24:28Z",
    "Summary": "Machine Learning (ML) for Mineral Prospectivity Mapping (MPM) remains a challenging problem as it requires the analysis of associations between large-scale multi-modal geospatial data and few historical mineral commodity observations (positive labels). Recent MPM works have explored Deep Learning (DL) as a modeling tool with more representation capacity. However, these overparameterized methods may be more prone to overfitting due to their reliance on scarce labeled data. While a large quantity of unlabeled geospatial data exists, no prior MPM works have considered using such information in a self-supervised manner. Our MPM approach uses a masked image modeling framework to pretrain a backbone neural network in a self-supervised manner using unlabeled geospatial data alone. After pretraining, the backbone network provides feature extraction for downstream MPM tasks. We evaluated our approach alongside existing methods to assess mineral prospectivity of Mississippi Valley Type (MVT) and Clastic-Dominated (CD) Lead-Zinc deposits in North America and Australia. Our results demonstrate that self-supervision promotes robustness in learned features, improving prospectivity predictions. Additionally, we leverage explainable artificial intelligence techniques to demonstrate that individual predictions can be interpreted from a geological perspective.",
    "Link": "http://arxiv.org/abs/2406.12756v1",
    "PDF Link": "http://arxiv.org/pdf/2406.12756v1"
  },
  {
    "Title": "Application of Disentanglement to Map Registration Problem",
    "Authors": "Hae Jin Song, Patrycja Krawczuk, Po-Hsuan Huang",
    "Published": "2024-08-26T09:55:32Z",
    "Summary": "Geospatial data come from various sources, such as satellites, aircraft, and LiDAR. The variability of the source is not limited to the types of data acquisition techniques, as we have maps from different time periods. To incorporate these data for a coherent analysis, it is essential to first align different \"styles\" of geospatial data to its matching images that point to the same location on the surface of the Earth. In this paper, we approach the image registration as a two-step process of (1) extracting geospatial contents invariant to visual (and any other non-content-related) information, and (2) matching the data based on such (purely) geospatial contents. We hypothesize that a combination of $\\beta$-VAE-like architecture [2] and adversarial training will achieve both the disentanglement of the geographic information and artistic styles and generation of new map tiles by composing the encoded geographic information with any artistic style.",
    "Link": "http://arxiv.org/abs/2408.14152v1",
    "PDF Link": "http://arxiv.org/pdf/2408.14152v1"
  },
  {
    "Title": "A systematic review of geospatial location embedding approaches in large\n  language models: A path to spatial AI systems",
    "Authors": "Sean Tucker",
    "Published": "2024-01-12T12:43:33Z",
    "Summary": "Geospatial Location Embedding (GLE) helps a Large Language Model (LLM) assimilate and analyze spatial data. GLE emergence in Geospatial Artificial Intelligence (GeoAI) is precipitated by the need for deeper geospatial awareness in our complex contemporary spaces and the success of LLMs in extracting deep meaning in Generative AI. We searched Google Scholar, Science Direct, and arXiv for papers on geospatial location embedding and LLM and reviewed articles focused on gaining deeper spatial \"knowing\" through LLMs. We screened 304 titles, 30 abstracts, and 18 full-text papers that reveal four GLE themes - Entity Location Embedding (ELE), Document Location Embedding (DLE), Sequence Location Embedding (SLE), and Token Location Embedding (TLE). Synthesis is tabular and narrative, including a dialogic conversation between \"Space\" and \"LLM.\" Though GLEs aid spatial understanding by superimposing spatial data, they emphasize the need to advance in the intricacies of spatial modalities and generalized reasoning. GLEs signal the need for a Spatial Foundation/Language Model (SLM) that embeds spatial knowing within the model architecture. The SLM framework advances Spatial Artificial Intelligence Systems (SPAIS), establishing a Spatial Vector Space (SVS) that maps to physical space. The resulting spatially imbued Language Model is unique. It simultaneously represents actual space and an AI-capable space, paving the way for AI native geo storage, analysis, and multi-modality as the basis for Spatial Artificial Intelligence Systems (SPAIS).",
    "Link": "http://arxiv.org/abs/2401.10279v1",
    "PDF Link": "http://arxiv.org/pdf/2401.10279v1"
  },
  {
    "Title": "idwMapper: An interactive and data-driven web mapping framework for\n  visualizing and sensing high-dimensional geospatial (big) data",
    "Authors": "Sarigai Sarigai, Liping Yang, Katie Slack, K. Maria D. Lane, Michaela Buenemann, Qiusheng Wu, Gordon Woodhull, Joshua Driscol",
    "Published": "2024-02-16T17:18:57Z",
    "Summary": "We are surrounded by overwhelming big data, which brings substantial advances but meanwhile poses many challenges. Geospatial big data comprises a big portion of big data, and is essential and powerful for decision-making if being utilized strategically. Volumes in size and high dimensions are two of the major challenges that prevent strategic decision-making from (geospatial) big data. Interactive map-based and geovisualization enabled web applications are intuitive and useful to construct knowledge and reveal insights from high-dimensional (geospatial) big data for actionable decision-making. We propose an interactive and data-driven web mapping framework, named idwMapper, for visualizing and sensing high dimensional geospatial (big) data in an interactive and scalable manner. To demonstrate the wide applicability and usefulness of our framework, we have applied our idwMapper framework to three real-world case studies and implemented three corresponding web map applications: iLit4GEE-AI, iWURanking, and iTRELISmap. We expect and hope the three web maps demonstrated in different domains, from literature big data analysis through world university ranking to scholar mapping, will provide a good start and inspire researchers and practitioners in various domains to apply our idwMapper to solve (or at least aid them in solving) their impactful problems.",
    "Link": "http://arxiv.org/abs/2402.11001v1",
    "PDF Link": "http://arxiv.org/pdf/2402.11001v1"
  },
  {
    "Title": "Pretraining Billion-scale Geospatial Foundational Models on Frontier",
    "Authors": "Aristeidis Tsaris, Philipe Ambrozio Dias, Abhishek Potnis, Junqi Yin, Feiyi Wang, Dalton Lunga",
    "Published": "2024-04-17T19:16:32Z",
    "Summary": "As AI workloads increase in scope, generalization capability becomes challenging for small task-specific models and their demand for large amounts of labeled training samples increases. On the contrary, Foundation Models (FMs) are trained with internet-scale unlabeled data via self-supervised learning and have been shown to adapt to various tasks with minimal fine-tuning. Although large FMs have demonstrated significant impact in natural language processing and computer vision, efforts toward FMs for geospatial applications have been restricted to smaller size models, as pretraining larger models requires very large computing resources equipped with state-of-the-art hardware accelerators. Current satellite constellations collect 100+TBs of data a day, resulting in images that are billions of pixels and multimodal in nature. Such geospatial data poses unique challenges opening up new opportunities to develop FMs. We investigate billion scale FMs and HPC training profiles for geospatial applications by pretraining on publicly available data. We studied from end-to-end the performance and impact in the solution by scaling the model size. Our larger 3B parameter size model achieves up to 30% improvement in top1 scene classification accuracy when comparing a 100M parameter model. Moreover, we detail performance experiments on the Frontier supercomputer, America's first exascale system, where we study different model and data parallel approaches using PyTorch's Fully Sharded Data Parallel library. Specifically, we study variants of the Vision Transformer architecture (ViT), conducting performance analysis for ViT models with size up to 15B parameters. By discussing throughput and performance bottlenecks under different parallelism configurations, we offer insights on how to leverage such leadership-class HPC resources when developing large models for geospatial imagery applications.",
    "Link": "http://arxiv.org/abs/2404.11706v1",
    "PDF Link": "http://arxiv.org/pdf/2404.11706v1"
  },
  {
    "Title": "A comprehensive GeoAI review: Progress, Challenges and Outlooks",
    "Authors": "Anasse Boutayeb, Iyad Lahsen-cherif, Ahmed El Khadimi",
    "Published": "2024-12-16T10:41:02Z",
    "Summary": "In recent years, Geospatial Artificial Intelligence (GeoAI) has gained traction in the most relevant research works and industrial applications, while also becoming involved in various fields of use. This paper offers a comprehensive review of GeoAI as a synergistic concept applying Artificial Intelligence (AI) methods and models to geospatial data. A preliminary study is carried out, identifying the methodology of the work, the research motivations, the issues and the directions to be tracked, followed by exploring how GeoAI can be used in various interesting fields of application, such as precision agriculture, environmental monitoring, disaster management and urban planning. Next, a statistical and semantic analysis is carried out, followed by a clear and precise presentation of the challenges facing GeoAI. Then, a concrete exploration of the future prospects is provided, based on several informations gathered during the census. To sum up, this paper provides a complete overview of the correlation between AI and the geospatial domain, while mentioning the researches conducted in this context, and emphasizing the close relationship linking GeoAI with other advanced concepts such as geographic information systems (GIS) and large-scale geospatial data, known as big geodata. This will enable researchers and scientific community to assess the state of progress in this promising field, and will help other interested parties to gain a better understanding of the issues involved.",
    "Link": "http://arxiv.org/abs/2412.11643v1",
    "PDF Link": "http://arxiv.org/pdf/2412.11643v1"
  },
  {
    "Title": "Composite Kernel Local Angular Discriminant Analysis for Multi-Sensor\n  Geospatial Image Analysis",
    "Authors": "Saurabh Prasad, Minshan Cui, Lifeng Yan",
    "Published": "2016-07-18T02:50:40Z",
    "Summary": "With the emergence of passive and active optical sensors available for geospatial imaging, information fusion across sensors is becoming ever more important. An important aspect of single (or multiple) sensor geospatial image analysis is feature extraction - the process of finding \"optimal\" lower dimensional subspaces that adequately characterize class-specific information for subsequent analysis tasks, such as classification, change and anomaly detection etc. In recent work, we proposed and developed an angle-based discriminant analysis approach that projected data onto subspaces with maximal \"angular\" separability in the input (raw) feature space and Reproducible Kernel Hilbert Space (RKHS). We also developed an angular locality preserving variant of this algorithm. In this letter, we advance this work and make it suitable for information fusion - we propose and validate a composite kernel local angular discriminant analysis projection, that can operate on an ensemble of feature sources (e.g. from different sources), and project the data onto a unified space through composite kernels where the data are maximally separated in an angular sense. We validate this method with the multi-sensor University of Houston hyperspectral and LiDAR dataset, and demonstrate that the proposed method significantly outperforms other composite kernel approaches to sensor (information) fusion.",
    "Link": "http://arxiv.org/abs/1607.04939v1",
    "PDF Link": "http://arxiv.org/pdf/1607.04939v1"
  },
  {
    "Title": "Spherical k-Nearest Neighbors Interpolation",
    "Authors": "Philippe Trempe",
    "Published": "2019-10-01T22:55:07Z",
    "Summary": "Geospatial interpolation is a challenging task due to real world data often being sparse, heterogeneous and inconsistent. For that matter, this work presents SkNNI, a spherical interpolation algorithm capable of working with such challenging geospatial data. This work also presents NDDNISD an accurate and efficient interpolation function for SkNNI which shines due to its spatial awareness in terms of proximity and distribution of observation neighbors. SkNNI's open source implementation is also discussed and illustrated with a simple usage example.",
    "Link": "http://arxiv.org/abs/1910.00704v1",
    "PDF Link": "http://arxiv.org/pdf/1910.00704v1"
  },
  {
    "Title": "Geospatial Analysis Requires a Different Way of Thinking: The Problem of\n  Spatial Heterogeneity",
    "Authors": "Bin Jiang",
    "Published": "2014-01-23T07:53:25Z",
    "Summary": "Geospatial analysis is very much dominated by a Gaussian way of thinking, which assumes that things in the world can be characterized by a well-defined mean, i.e., things are more or less similar in size. However, this assumption is not always valid. In fact, many things in the world lack a well-defined mean, and therefore there are far more small things than large ones. This paper attempts to argue that geospatial analysis requires a different way of thinking - a Paretian way of thinking that underlies skewed distribution such as power laws, Pareto and lognormal distributions. I review two properties of spatial dependence and spatial heterogeneity, and point out that the notion of spatial heterogeneity in current spatial statistics is only used to characterize local variance of spatial dependence. I subsequently argue for a broad perspective on spatial heterogeneity, and suggest it be formulated as a scaling law. I further discuss the implications of Paretian thinking and the scaling law for better understanding of geographic forms and processes, in particular while facing massive amounts of social media data. In the spirit of Paretian thinking, geospatial analysis should seek to simulate geographic events and phenomena from the bottom up rather than correlations as guided by Gaussian thinking.",
    "Link": "http://arxiv.org/abs/1401.5889v2",
    "PDF Link": "http://arxiv.org/pdf/1401.5889v2"
  },
  {
    "Title": "Scalable Analysis of Urban Scaling Laws: Leveraging Cloud Computing to\n  Analyze 21,280 Global Cities",
    "Authors": "Zhenhui Li, Hongwei Zhang, Kan Wu",
    "Published": "2024-12-03T09:13:37Z",
    "Summary": "Cities play a pivotal role in human development and sustainability, yet studying them presents significant challenges due to the vast scale and complexity of spatial-temporal data. One such challenge is the need to uncover universal urban patterns, such as the urban scaling law, across thousands of cities worldwide. In this study, we propose a novel large-scale geospatial data processing system that enables city analysis on an unprecedented scale. We demonstrate the system's capabilities by revisiting the urban scaling law across 21,280 cities globally, using a range of open-source datasets including road networks, nighttime light intensity, built-up areas, and population statistics. Analyzing the characteristics of 21,280 cities involves querying over half a billion geospatial data points, a task that traditional Geographic Information Systems (GIS) would take several days to process. In contrast, our cloud-based system accelerates the analysis, reducing processing time to just minutes while significantly lowering resource consumption. Our findings reveal that the urban scaling law varies across cities in under-developed, developing, and developed regions, extending the insights gained from previous studies focused on hundreds of cities. This underscores the critical importance of cloud-based big data processing for efficient, large-scale geospatial analysis. As the availability of satellite imagery and other global datasets continues to grow, the potential for scientific discovery expands exponentially. Our approach not only demonstrates how such large-scale tasks can be executed efficiently but also offers a powerful solution for data scientists and researchers working in the fields of city and geospatial science.",
    "Link": "http://arxiv.org/abs/2412.02299v1",
    "PDF Link": "http://arxiv.org/pdf/2412.02299v1"
  },
  {
    "Title": "GeoGPT: Understanding and Processing Geospatial Tasks through An\n  Autonomous GPT",
    "Authors": "Yifan Zhang, Cheng Wei, Shangyou Wu, Zhengting He, Wenhao Yu",
    "Published": "2023-07-16T03:03:59Z",
    "Summary": "Decision-makers in GIS need to combine a series of spatial algorithms and operations to solve geospatial tasks. For example, in the task of facility siting, the Buffer tool is usually first used to locate areas close or away from some specific entities; then, the Intersect or Erase tool is used to select candidate areas satisfied multiple requirements. Though professionals can easily understand and solve these geospatial tasks by sequentially utilizing relevant tools, it is difficult for non-professionals to handle these problems. Recently, Generative Pre-trained Transformer (e.g., ChatGPT) presents strong performance in semantic understanding and reasoning. Especially, AutoGPT can further extend the capabilities of large language models (LLMs) by automatically reasoning and calling externally defined tools. Inspired by these studies, we attempt to lower the threshold of non-professional users to solve geospatial tasks by integrating the semantic understanding ability inherent in LLMs with mature tools within the GIS community. Specifically, we develop a new framework called GeoGPT that can conduct geospatial data collection, processing, and analysis in an autonomous manner with the instruction of only natural language. In other words, GeoGPT is used to understand the demands of non-professional users merely based on input natural language descriptions, and then think, plan, and execute defined GIS tools to output final effective results. Several cases including geospatial data crawling, spatial query, facility siting, and mapping validate the effectiveness of our framework. Though limited cases are presented in this paper, GeoGPT can be further extended to various tasks by equipping with more GIS tools, and we think the paradigm of \"foundational plus professional\" implied in GeoGPT provides an effective way to develop next-generation GIS in this era of large foundation models.",
    "Link": "http://arxiv.org/abs/2307.07930v1",
    "PDF Link": "http://arxiv.org/pdf/2307.07930v1"
  },
  {
    "Title": "Towards Scalable Foundation Model for Multi-modal and Hyperspectral\n  Geospatial Data",
    "Authors": "Haozhe Si, Yuxuan Wan, Minh Do, Deepak Vasisht, Han Zhao, Hendrik F. Hamann",
    "Published": "2025-03-17T05:42:19Z",
    "Summary": "Geospatial raster data, such as that collected by satellite-based imaging systems at different times and spectral bands, hold immense potential for enabling a wide range of high-impact applications. This potential stems from the rich information that is spatially and temporally contextualized across multiple channels and sensing modalities. Recent work has adapted existing self-supervised learning approaches for such geospatial data. However, they fall short of scalable model architectures, leading to inflexibility and computational inefficiencies when faced with an increasing number of channels and modalities. To address these limitations, we introduce Low-rank Efficient Spatial-Spectral Vision Transformer with three key innovations: i) the LESS Attention Block that approximates high-dimensional spatial-spectral attention through Kronecker's product of the low-dimensional spatial and spectral attention components; ii) the Continuous Positional-Channel Embedding Layer that preserves both the continuity and physical characteristics of each spatial-spectral patch; and iii) the Perception Field Mask that exploits local spatial dependencies by constraining attention to neighboring patches. To evaluate the proposed innovations, we construct GFM-Bench, which serves as a comprehensive benchmark for such geospatial raster data. We pretrain LESS ViT using a Hyperspectral Masked Autoencoder framework with integrated positional and channel masking strategies. Experimental results demonstrate that our proposed method achieves competitive performance against state-of-the-art multi-modal geospatial foundation models while outperforming them on cross-satellite generalization tasks with higher computational efficiency. The flexibility and extensibility of our framework make it a promising direction for future geospatial data analysis tasks that involve a wide range of modalities and channels.",
    "Link": "http://arxiv.org/abs/2503.12843v3",
    "PDF Link": "http://arxiv.org/pdf/2503.12843v3"
  },
  {
    "Title": "Comparative Performance of Advanced NLP Models and LLMs in Multilingual\n  Geo-Entity Detection",
    "Authors": "Kalin Kopanov",
    "Published": "2024-12-29T09:47:14Z",
    "Summary": "The integration of advanced Natural Language Processing (NLP) methodologies and Large Language Models (LLMs) has significantly enhanced the extraction and analysis of geospatial data from multilingual texts, impacting sectors such as national and international security. This paper presents a comprehensive evaluation of leading NLP models -- SpaCy, XLM-RoBERTa, mLUKE, GeoLM -- and LLMs, specifically OpenAI's GPT 3.5 and GPT 4, within the context of multilingual geo-entity detection. Utilizing datasets from Telegram channels in English, Russian, and Arabic, we examine the performance of these models through metrics such as accuracy, precision, recall, and F1 scores, to assess their effectiveness in accurately identifying geospatial references. The analysis exposes each model's distinct advantages and challenges, underscoring the complexities involved in achieving precise geo-entity identification across varied linguistic landscapes. The conclusions drawn from this experiment aim to direct the enhancement and creation of more advanced and inclusive NLP tools, thus advancing the field of geospatial analysis and its application to global security.",
    "Link": "http://arxiv.org/abs/2412.20414v1",
    "PDF Link": "http://arxiv.org/pdf/2412.20414v1"
  },
  {
    "Title": "Surface temperatures in New York City: Geospatial data enables the\n  accurate prediction of radiative heat transfer",
    "Authors": "Masoud Ghandehari, Thorsten Emig, Milad Aghamohamadnia",
    "Published": "2017-08-27T13:49:08Z",
    "Summary": "Three decades into the research seeking to derive the urban energy budget, the dynamics of the thermal exchange between the densely built infrastructure and the environment are still not well understood. We present a novel hybrid experimental-numerical approach for the analysis of the radiative heat transfer in New York City. The aim of this work is to contribute to the calculation of the urban energy budget, in particular the stored energy. Improved understanding of urban thermodynamics incorporating the interaction of the various bodies will have implications on energy conservation at the building scale, as well as human health and comfort at the urban scale. The platform presented is based on longwave hyperspectral imaging of nearly 100 blocks of Manhattan, and a geospatial radiosity model that describes the collective radiative heat exchange between multiple buildings. The close comparison of temperature values derived from measurements and the computed surface temperatures (including streets and roads) implies that this geospatial, thermodynamic numerical model applied to urban structures, is promising for accurate and high resolution analysis of urban surface temperatures.",
    "Link": "http://arxiv.org/abs/1708.08089v1",
    "PDF Link": "http://arxiv.org/pdf/1708.08089v1"
  },
  {
    "Title": "Geospatial distributions reflect rates of evolution of features of\n  language",
    "Authors": "Henri Kauhanen, Deepthi Gopal, Tobias Galla, Ricardo Bermúdez-Otero",
    "Published": "2018-01-29T17:24:27Z",
    "Summary": "Quantifying the speed of linguistic change is challenging due to the fact that the historical evolution of languages is sparsely documented. Consequently, traditional methods rely on phylogenetic reconstruction. In this paper, we propose a model-based approach to the problem through the analysis of language change as a stochastic process combining vertical descent, spatial interactions, and mutations in both dimensions. A notion of linguistic temperature emerges naturally from this analysis as a dimensionless measure of the propensity of a linguistic feature to undergo change. We demonstrate how temperatures of linguistic features can be inferred from their present-day geospatial distributions, without recourse to information about their phylogenies. Thus the evolutionary dynamics of language, operating across thousands of years, leaves a measurable geospatial signature. This signature licenses inferences about the historical evolution of languages even in the absence of longitudinal data.",
    "Link": "http://arxiv.org/abs/1801.09637v2",
    "PDF Link": "http://arxiv.org/pdf/1801.09637v2"
  },
  {
    "Title": "Leveraging Geospatial Information to address Space Epidemiology through\n  Multi$\\unicode{x2013}$omics $\\unicode{x2013}$ Report of an Interdisciplinary\n  Workshop",
    "Authors": "Annette L. Sobel, Kenneth Yeh, Elaine Bradford, Colin Price, Joseph Russell, Gene Olinger, Sheila Grant, Chi-Ren Shyu",
    "Published": "2023-08-12T02:41:30Z",
    "Summary": "This article will summarize the workshop proceedings of a workshop conducted at the University of Missouri that addressed the use of multi-omics fused with geospatial information to assess and improve the precision and environmental analysis of indicators of crew space health. The workshop addressed the state of the art of multi-omics research and practice and the potential future use of multi-omics platforms in extreme environments. The workshop also focused on potential new strategies for data collection, analysis, and fusion with crosstalk with the field of environmental health, biosecurity, and radiation safety, addressing gaps and shortfalls and potential new approaches to enhancing astronaut health safety and security. Ultimately, the panel proceedings resulted in a synthesis of new research and translational opportunities to improve space and terrestrial epidemiology. In the future, early disease prevention that employs new and expanded data sources enhanced by the analytic precision of geospatial information and artificial intelligence algorithms.",
    "Link": "http://arxiv.org/abs/2308.07339v1",
    "PDF Link": "http://arxiv.org/pdf/2308.07339v1"
  },
  {
    "Title": "Analysing complexity of XML Schemas in geospatial web services",
    "Authors": "Alain Tamayo, Carlos Granell, Joaquín Huerta",
    "Published": "2011-10-02T17:49:47Z",
    "Summary": "XML Schema is the language used to define the structure of messages exchanged between OGC-based web service clients and providers. The size of these schemas has been growing with time, reaching a state that makes its understanding and effective application a hard task. A first step to cope with this situation is to provide different ways to measure the complexity of the schemas. In this regard, we present in this paper an analysis of the complexity of XML schemas in OGC web services. We use a group of metrics found in the literature and introduce new metrics to measure size and/or complexity of these schemas. The use of adequate metrics allows us to quantify the complexity, quality and other properties of the schemas, which can be very useful in different scenarios.",
    "Link": "http://arxiv.org/abs/1110.0207v1",
    "PDF Link": "http://arxiv.org/pdf/1110.0207v1"
  },
  {
    "Title": "Context Trees: Augmenting Geospatial Trajectories with Context",
    "Authors": "Alasdair Thomason, Nathan Griffiths, Victor Sanchez",
    "Published": "2016-06-14T09:27:29Z",
    "Summary": "Exposing latent knowledge in geospatial trajectories has the potential to provide a better understanding of the movements of individuals and groups. Motivated by such a desire, this work presents the context tree, a new hierarchical data structure that summarises the context behind user actions in a single model. We propose a method for context tree construction that augments geospatial trajectories with land usage data to identify such contexts. Through evaluation of the construction method and analysis of the properties of generated context trees, we demonstrate the foundation for understanding and modelling behaviour afforded. Summarising user contexts into a single data structure gives easy access to information that would otherwise remain latent, providing the basis for better understanding and predicting the actions and behaviours of individuals and groups. Finally, we also present a method for pruning context trees, for use in applications where it is desirable to reduce the size of the tree while retaining useful information.",
    "Link": "http://arxiv.org/abs/1606.04269v1",
    "PDF Link": "http://arxiv.org/pdf/1606.04269v1"
  },
  {
    "Title": "Aggregation and visualization of spatial data with application to\n  classification of land use and land cover",
    "Authors": "Mihal Miu, Xiaokun Zhang, M. Ali Akber Dewan, Junye Wang",
    "Published": "2017-04-19T18:01:29Z",
    "Summary": "Aggregation and visualization of geographical data are an important part of environmental data mining, environmental modelling, and agricultural management. However, it is difficult to aggregate geospatial data of the various formats, such as maps, census and survey data. This paper presents a framework named PlaniSphere, which can aggregate the various geospatial datasets, and synthesizes raw data. We developed an algorithm in PlaniSphere to aggregate remote sensing images with census data for classification and visualization of land use and land cover (LULC). The results show that the framework is able to classify geospatial data sets of LULC from multiple formats. National census data sets can be used for calibration of remote sensing LULC classifications. This provides a new approach for the classification of remote sensing data. This approach proposed in this paper should be useful for LULC classification in environmental spatial analysis.",
    "Link": "http://arxiv.org/abs/1704.05860v1",
    "PDF Link": "http://arxiv.org/pdf/1704.05860v1"
  },
  {
    "Title": "Persistent Homology of Geospatial Data: A Case Study with Voting",
    "Authors": "Michelle Feng, Mason A. Porter",
    "Published": "2019-01-29T05:39:40Z",
    "Summary": "A crucial step in the analysis of persistent homology is the transformation of data into an appropriate topological object (in our case, a simplicial complex). Modern packages for persistent homology often construct Vietoris--Rips or other distance-based simplicial complexes on point clouds because they are relatively easy to compute. We investigate alternative methods of constructing these complexes and the effects of making associated choices during simplicial-complex construction on the output of persistent-homology algorithms. We present two new methods for constructing simplicial complexes from two-dimensional geospatial data (such as maps). We apply these methods to a California precinct-level voting data set, demonstrating that our new constructions can capture geometric characteristics that are missed by distance-based constructions. Our new constructions can thus yield more interpretable persistence modules and barcodes for geospatial data. In particular, they are able to distinguish short-persistence features that occur only for a narrow range of distance scales (e.g., voting behaviors in densely populated cities) from short-persistence noise by incorporating information about other spatial relationships between precincts.",
    "Link": "http://arxiv.org/abs/1902.05911v2",
    "PDF Link": "http://arxiv.org/pdf/1902.05911v2"
  },
  {
    "Title": "HeGeL: A Novel Dataset for Geo-Location from Hebrew Text",
    "Authors": "Tzuf Paz-Argaman, Tal Bauman, Itai Mondshine, Itzhak Omer, Sagi Dalyot, Reut Tsarfaty",
    "Published": "2023-07-02T08:09:10Z",
    "Summary": "The task of textual geolocation - retrieving the coordinates of a place based on a free-form language description - calls for not only grounding but also natural language understanding and geospatial reasoning. Even though there are quite a few datasets in English used for geolocation, they are currently based on open-source data (Wikipedia and Twitter), where the location of the described place is mostly implicit, such that the location retrieval resolution is limited. Furthermore, there are no datasets available for addressing the problem of textual geolocation in morphologically rich and resource-poor languages, such as Hebrew. In this paper, we present the Hebrew Geo-Location (HeGeL) corpus, designed to collect literal place descriptions and analyze lingual geospatial reasoning. We crowdsourced 5,649 literal Hebrew place descriptions of various place types in three cities in Israel. Qualitative and empirical analysis show that the data exhibits abundant use of geospatial reasoning and requires a novel environmental representation.",
    "Link": "http://arxiv.org/abs/2307.00509v1",
    "PDF Link": "http://arxiv.org/pdf/2307.00509v1"
  },
  {
    "Title": "More than Correlation: Do Large Language Models Learn Causal\n  Representations of Space?",
    "Authors": "Yida Chen, Yixian Gan, Sijia Li, Li Yao, Xiaohan Zhao",
    "Published": "2023-12-26T01:27:29Z",
    "Summary": "Recent work found high mutual information between the learned representations of large language models (LLMs) and the geospatial property of its input, hinting an emergent internal model of space. However, whether this internal space model has any causal effects on the LLMs' behaviors was not answered by that work, led to criticism of these findings as mere statistical correlation. Our study focused on uncovering the causality of the spatial representations in LLMs. In particular, we discovered the potential spatial representations in DeBERTa, GPT-Neo using representational similarity analysis and linear and non-linear probing. Our casual intervention experiments showed that the spatial representations influenced the model's performance on next word prediction and a downstream task that relies on geospatial information. Our experiments suggested that the LLMs learn and use an internal model of space in solving geospatial related tasks.",
    "Link": "http://arxiv.org/abs/2312.16257v1",
    "PDF Link": "http://arxiv.org/pdf/2312.16257v1"
  },
  {
    "Title": "An Ensemble Framework for Explainable Geospatial Machine Learning Models",
    "Authors": "Lingbo Liu",
    "Published": "2024-03-05T21:12:10Z",
    "Summary": "Analyzing spatially varying effects is pivotal in geographic analysis. However, accurately capturing and interpreting this variability is challenging due to the increasing complexity and non-linearity of geospatial data. Recent advancements in integrating Geographically Weighted (GW) models with artificial intelligence (AI) methodologies offer novel approaches. However, these methods often focus on single algorithms and emphasize prediction over interpretability. The recent GeoShapley method integrates machine learning (ML) with Shapley values to explain the contribution of geographical features, advancing the combination of geospatial ML and explainable AI (XAI). Yet, it lacks exploration of the nonlinear interactions between geographical features and explanatory variables. Herein, an ensemble framework is proposed to merge local spatial weighting scheme with XAI and ML technologies to bridge this gap. Through tests on synthetic datasets and comparisons with GWR, MGWR, and GeoShapley, this framework is verified to enhance interpretability and predictive accuracy by elucidating spatial variability. Reproducibility is explored through the comparison of spatial weighting schemes and various ML models, emphasizing the necessity of model reproducibility to address model and parameter uncertainty. This framework works in both geographic regression and classification, offering a novel approach to understanding complex spatial phenomena.",
    "Link": "http://arxiv.org/abs/2403.03328v2",
    "PDF Link": "http://arxiv.org/pdf/2403.03328v2"
  },
  {
    "Title": "Multi-Agent Geospatial Copilots for Remote Sensing Workflows",
    "Authors": "Chaehong Lee, Varatheepan Paramanayakam, Andreas Karatzas, Yanan Jian, Michael Fore, Heming Liao, Fuxun Yu, Ruopu Li, Iraklis Anagnostopoulos, Dimitrios Stamoulis",
    "Published": "2025-01-27T17:54:31Z",
    "Summary": "We present GeoLLM-Squad, a geospatial Copilot that introduces the novel multi-agent paradigm to remote sensing (RS) workflows. Unlike existing single-agent approaches that rely on monolithic large language models (LLM), GeoLLM-Squad separates agentic orchestration from geospatial task-solving, by delegating RS tasks to specialized sub-agents. Built on the open-source AutoGen and GeoLLM-Engine frameworks, our work enables the modular integration of diverse applications, spanning urban monitoring, forestry protection, climate analysis, and agriculture studies. Our results demonstrate that while single-agent systems struggle to scale with increasing RS task complexity, GeoLLM-Squad maintains robust performance, achieving a 17% improvement in agentic correctness over state-of-the-art baselines. Our findings highlight the potential of multi-agent AI in advancing RS workflows.",
    "Link": "http://arxiv.org/abs/2501.16254v1",
    "PDF Link": "http://arxiv.org/pdf/2501.16254v1"
  }
]